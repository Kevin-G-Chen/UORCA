{
 "cells": [
  {
   "cell_type": "raw",
   "id": "232f6a47-2bc6-4384-8de9-14140fc9d4f8",
   "metadata": {},
   "source": [
    "This notebook will contain attempts to analyse Kallisto data.\n",
    "\n",
    "While Kallisto is downstream of downloading FASTQ files (and subsequently quantification via Kallisto), I will still just be starting here. \n",
    "\n",
    "My goals here are as follows:\n",
    "- Process metadata (including interpreting the different research groups, identifying what comparisons might be interesting, fixing any errors in the metadata. I will be providing a metadata sheet with mistakes to see how this will work.). Note that the prompt I end up needing here might have to be revised based on how I go with the dataset download.\n",
    "- Create (what I assume is) the DGEList object in Python, i.e. via PyDESeq2. If I can't get this to work, I will create an R script and call this script in Python. This is true for future steps too. Note that part of this entails being able to match samples to the correct files\n",
    "- Perform the filtering and normalisation. Should be straightfoward (I think...) if I can get the above to work\n",
    "- Perform the DEG analysis. Note that this relies on correctly identifying the groups. \n",
    "- Perform an ORA/GSEA. If I can get the above to work, this should be straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d5ce11-4322-466f-8ec6-397ca9f78750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2546b752-753c-4247-aa60-37fd3c702b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('../.env')\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d981877-4137-4ef5-b1e7-4478705bd96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red, Orange, Yellow, Green, Blue, Indigo, Violet.\n"
     ]
    }
   ],
   "source": [
    "# Test OpenAI API...\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=openai_api_key,  # this is also the default, it can be omitted\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the colours of the rainbow? Only respond with the names of the colours.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "result = chat_completion.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98796af4-7e42-4728-8bbf-84fbd8b487d2",
   "metadata": {},
   "source": [
    "I think my goal will be to integrate R, OpenAI, and Python. I would feel more confident working with an edgeR/limma based pipeline. \n",
    "\n",
    "Therefore, I will begin with just a simple implementation, testing SETBP1 data.\n",
    "\n",
    "Note that while sample data is provided with pydeseq2, this is completely synthetic - the genes are not real, and therefore this ends up not being very valuable.\n",
    "\n",
    "With R integration, the specific steps are as follows:\n",
    "- Identify the Kallisto files (likely no LLM needed)\n",
    "- Identify other files (i.e. tx2gene) (likely no LLM needed)\n",
    "- Process metadata, i.e. correct errors, match to Kallisto files (LLM needed)\n",
    "- Import Kallisto counts (no LLM needed)\n",
    "- Add metadata (and gene data) to the Kallisto counts, i.e. create DGEList object (no LLM needed)\n",
    "- Perform filtering and normalisation (no LLM needed, I can use default settings)\n",
    "- Define contrasts (probably the main hard part - from a value standpoint, there are considerations around which contrasts are interesting and valid, from the technical standpoint it's probably ok? Unsure how easily the LLM will interpret the metadata...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ccf30-35f8-4a34-aa73-24503445a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing correction of metadata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d88fd0ff-c0d5-4b51-9f91-e5d241b4020c",
   "metadata": {},
   "source": [
    "I am going to begin with testing to see if I can get the LLM to analyse metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1bab847-c833-43f6-8115-5435954848e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test matching of Kallisto samples to metadata\n",
    "\n",
    "# I will begin by testing to see if 1) Kallisto samples can be matched to metadata based on the names of the samples in the metadata, and 2) if the two can be merged together in a single data frame\n",
    "\n",
    "class ColumnIdentification(BaseModel):\n",
    "    likely_sample_column: str = Field(..., description=\"The column name most likely to contain sample identifiers\")\n",
    "    confidence: float = Field(..., description=\"Confidence score for the column identification (0-1)\")\n",
    "    reasoning: str = Field(..., description=\"Explanation for why this column was chosen\")\n",
    "\n",
    "class SampleMatch(BaseModel):\n",
    "    metadata_sample: str = Field(..., description=\"The sample name from the metadata\")\n",
    "    file_name: str = Field(..., description=\"The matched file name\")\n",
    "    confidence: float = Field(..., description=\"Confidence score of the match (0-1)\")\n",
    "\n",
    "class MatchResult(BaseModel):\n",
    "    column_identification: ColumnIdentification = Field(..., description=\"Identification of the sample name column\")\n",
    "    matches: List[SampleMatch] = Field(..., description=\"List of matched samples and file names\")\n",
    "    matching_logic: str = Field(..., description=\"Explanation of the logic used to match samples to file names\")\n",
    "\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def create_prompt(metadata_df, file_names):\n",
    "    prompt = f\"\"\"Analyze the following metadata and list of file names:\n",
    "\n",
    "Metadata columns:\n",
    "{metadata_df.columns.tolist()}\n",
    "\n",
    "Metadata:\n",
    "{metadata_df.to_string()}\n",
    "\n",
    "File names:\n",
    "{file_names}\n",
    "\n",
    "Tasks:\n",
    "1. Identify the column most likely to contain sample identifiers. Provide the column name, a confidence score, and your reasoning.\n",
    "2. Match each sample from the identified column to the most likely corresponding file name. Consider variations in capitalization, spaces, dashes, and potential typos.\n",
    "3. Explain the logic you used to match samples to file names.\n",
    "\n",
    "Provide your analysis in a structured format.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_openai_response(prompt, openai_api_key):\n",
    "    client = instructor.patch(OpenAI(\n",
    "    api_key=openai_api_key))\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_model=MatchResult\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def update_metadata(metadata_df, match_result):\n",
    "    sample_column = match_result.column_identification.likely_sample_column\n",
    "    file_name_dict = {match.metadata_sample: match.file_name for match in match_result.matches}\n",
    "    metadata_df['file_name'] = metadata_df[sample_column].map(file_name_dict)\n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ff40434-4b29-4d85-9749-f25219082f3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likely sample name column: Sample Name\n",
      "Confidence: 1.0\n",
      "Reasoning: The 'Sample Name' column contains names that are structured similarly to the file names, with consistent patterns allowing for clear correspondence.\n",
      "\n",
      "Matching logic:\n",
      "The matching was performed by normalizing both sample identifiers and file names to a consistent format: converting underscores to dashes, stripping out spaces, and ensuring uniform casing. The resulting normalized names were then compared for exact matches.\n",
      "\n",
      "Updated Metadata:\n",
      "                          Sample Name Transfection      Genotype Cell type   \\\n",
      "0      KOLF2_SETBP1_VUS2_A21_10_day 0            A   VUS2 HDR/WT       iPSC   \n",
      "1       KOLF2_SETBP1_VUS2_A21_7_day 0            A   VUS2 HDR/WT       iPSC   \n",
      "2     KOLF2_SETBP1_VUS2_A21_10_day 24            A   VUS2 HDR/WT        NPC   \n",
      "3      KOLF2_SETBP1_VUS2_A21_7_day 24            A   VUS2 HDR/WT        NPC   \n",
      "4      KOLF2_SETBP1_VUS2_B1_4.1_day 0            B   VUS2 HDR/WT       iPSC   \n",
      "5      KOLF2_SETBP1_VUS2_B1_4.2_day 0            B   VUS2 HDR/WT       iPSC   \n",
      "6     KOLF2_SETBP1_VUS2_B1_4.1_day 24            B   VUS2 HDR/WT        NPC   \n",
      "7     KOLF2_SETBP1_VUS2_B1_4.2_day 24            B   VUS2 HDR/WT        NPC   \n",
      "8    KOLF2_SETBP1_VUS2_C12.1_WT_day 0            C         WT/WT       iPSC   \n",
      "9    KOLF2_SETBP1_VUS2_C12.2_WT_day 0            C         WT/WT       iPSC   \n",
      "10   KOLF2_SETBP1_VUS2_C12.3_WT_day 0            C         WT/WT       iPSC   \n",
      "11  KOLF2_SETBP1_VUS2_C12.1_WT_day 24            C         WT/WT        NPC   \n",
      "12  KOLF2_SETBP1_VUS2_C12.2_WT_day 24            C         WT/WT        NPC   \n",
      "13  KOLF2_SETBP1_VUS2_C12.3_WT_day 24            C         WT/WT        NPC   \n",
      "14    KOLF2_SETBP1_PATH1_4.6_WT_day 0            D         WT/WT       iPSC   \n",
      "15    KOLF2_SETBP1_PATH1_4.7_WT_day 0            D         WT/WT       iPSC   \n",
      "16   KOLF2_SETBP1_PATH1_4.6_WT_day 24            D         WT/WT        NPC   \n",
      "17   KOLF2_SETBP1_PATH1_4.7_WT_day 24            D         WT/WT        NPC   \n",
      "18       KOLF2_SETBP1_PATH2_3.2_day 0            E  PATH2 HDR/WT       iPSC   \n",
      "19    KOLF2_SETBP1_PATH2_3.3_WT_day 0            E         WT/WT       iPSC   \n",
      "20       KOLF2_SETBP1_PATH2_3.4_day 0            E  PATH2 HDR/WT       iPSC   \n",
      "21       KOLF2_SETBP1_PATH2_3.6_day 0            E  PATH2 HDR/WT       iPSC   \n",
      "22      KOLF2_SETBP1_PATH2_3.2_day 24            E  PATH2 HDR/WT        NPC   \n",
      "23   KOLF2_SETBP1_PATH2_3.3_WT_day 24            E         WT/WT        NPC   \n",
      "24      KOLF2_SETBP1_PATH2_3.4_day 24            E  PATH2 HDR/WT        NPC   \n",
      "25      KOLF2_SETBP1_PATH2_3.6_day 24            E  PATH2 HDR/WT        NPC   \n",
      "26     KOLF2_SETBP1_PATH3_15.10_day 0            F  PATH3 HDR/WT       iPSC   \n",
      "27    KOLF2_SETBP1_PATH3_15.7.1_day 0            F  PATH3 HDR/WT       iPSC   \n",
      "28    KOLF2_SETBP1_PATH3_15.7.2_day 0            F  PATH3 HDR/WT       iPSC   \n",
      "29    KOLF2_SETBP1_PATH3_15.10_day 24            F  PATH3 HDR/WT        NPC   \n",
      "30   KOLF2_SETBP1_PATH3_15.7.1_day 24            F  PATH3 HDR/WT        NPC   \n",
      "31   KOLF2_SETBP1_PATH3_15.7.2_day 24            F  PATH3 HDR/WT        NPC   \n",
      "\n",
      "    Neural differentiation set                                       file_name  \n",
      "0                            2     KOLF2-SETBP1-VUS2-A21-10-day-0/abundance.h5  \n",
      "1                            1      KOLF2-SETBP1-VUS2-A21-7-day-0/abundance.h5  \n",
      "2                            2    KOLF2-SETBP1-VUS2-A21-10-day-24/abundance.h5  \n",
      "3                            1     KOLF2-SETBP1-VUS2-A21-7-day-24/abundance.h5  \n",
      "4                            1     KOLF2-SETBP1-VUS2-B1-4-1-day-0/abundance.h5  \n",
      "5                            2     KOLF2-SETBP1-VUS2-B1-4-2-day-0/abundance.h5  \n",
      "6                            1    KOLF2-SETBP1-VUS2-B1-4-1-day-24/abundance.h5  \n",
      "7                            2    KOLF2-SETBP1-VUS2-B1-4-2-day-24/abundance.h5  \n",
      "8                            1   KOLF2-SETBP1-VUS2-C12-1-WT-day-0/abundance.h5  \n",
      "9                            1   KOLF2-SETBP1-VUS2-C12-2-WT-day-0/abundance.h5  \n",
      "10                           2   KOLF2-SETBP1-VUS2-C12-3-WT-day-0/abundance.h5  \n",
      "11                           1  KOLF2-SETBP1-VUS2-C12-1-WT-day-24/abundance.h5  \n",
      "12                           1  KOLF2-SETBP1-VUS2-C12-2-WT-day-24/abundance.h5  \n",
      "13                           2  KOLF2-SETBP1-VUS2-C12-3-WT-day-24/abundance.h5  \n",
      "14                           1    KOLF2-SETBP1-PATH1-4-6-WT-day-0/abundance.h5  \n",
      "15                           2    KOLF2-SETBP1-PATH1-4-7-WT-day-0/abundance.h5  \n",
      "16                           1   KOLF2-SETBP1-PATH1-4-6-WT-day-24/abundance.h5  \n",
      "17                           2   KOLF2-SETBP1-PATH1-4-7-WT-day-24/abundance.h5  \n",
      "18                           1       KOLF2-SETBP1-PATH2-3-2-day-0/abundance.h5  \n",
      "19                           2    KOLF2-SETBP1-PATH2-3-3-WT-day-0/abundance.h5  \n",
      "20                           1       KOLF2-SETBP1-PATH2-3-4-day-0/abundance.h5  \n",
      "21                           2       KOLF2-SETBP1-PATH2-3-6-day-0/abundance.h5  \n",
      "22                           1      KOLF2-SETBP1-PATH2-3-2-day-24/abundance.h5  \n",
      "23                           2   KOLF2-SETBP1-PATH2-3-3-WT-day-24/abundance.h5  \n",
      "24                           1      KOLF2-SETBP1-PATH2-3-4-day-24/abundance.h5  \n",
      "25                           2      KOLF2-SETBP1-PATH2-3-6-day-24/abundance.h5  \n",
      "26                           2     KOLF2-SETBP1-PATH3-15-10-day-0/abundance.h5  \n",
      "27                           1    KOLF2-SETBP1-PATH3-15-7-1-day-0/abundance.h5  \n",
      "28                           2    KOLF2-SETBP1-PATH3-15-7-2-day-0/abundance.h5  \n",
      "29                           2    KOLF2-SETBP1-PATH3-15-10-day-24/abundance.h5  \n",
      "30                           1   KOLF2-SETBP1-PATH3-15-7-1-day-24/abundance.h5  \n",
      "31                           2   KOLF2-SETBP1-PATH3-15-7-2-day-24/abundance.h5  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    api_key = openai_api_key\n",
    "\n",
    "    # Path to your metadata CSV file\n",
    "    metadata_file_path = \"../InputData/SETBP1_Tests/SETBP1_RNAseq_samples.csv\"\n",
    "    \n",
    "    # Read the metadata CSV file\n",
    "    metadata_df = read_csv(metadata_file_path)\n",
    "    \n",
    "    # List of sample file names (you would typically read this from a directory or another source)\n",
    "    with open(\"../InputData/SETBP1_Tests/abundance_files.txt\", 'r') as file:\n",
    "        file_names = file.read()\n",
    "    \n",
    "    # Create a prompt with the metadata and file names\n",
    "    prompt = create_prompt(metadata_df, file_names)\n",
    "    \n",
    "    # Get the response from OpenAI\n",
    "    response = get_openai_response(prompt, openai_api_key)\n",
    "    # Print the identified sample name column and reasoning\n",
    "    print(f\"Likely sample name column: {response.column_identification.likely_sample_column}\")\n",
    "    print(f\"Confidence: {response.column_identification.confidence}\")\n",
    "    print(f\"Reasoning: {response.column_identification.reasoning}\\n\")\n",
    "    \n",
    "    # Print the matching logic\n",
    "    print(\"Matching logic:\")\n",
    "    print(response.matching_logic)\n",
    "    print()\n",
    "    \n",
    "    # Update the metadata DataFrame with matched file names\n",
    "    updated_metadata = update_metadata(metadata_df, response)\n",
    "    \n",
    "    # Print the updated metadata\n",
    "    print(\"Updated Metadata:\")\n",
    "    print(updated_metadata)\n",
    "    \n",
    "    # Optionally, save the updated metadata to a new CSV file\n",
    "    updated_metadata.to_csv(\"../results/2024_07_31_AutomatedDataProcessing/Clean_MetadataSampleMatching.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6f8b5a8b-37e0-4dd3-ac7b-0536f5ca6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat above but with a more error-filled CSV. This will also be where I refine my prompt.\n",
    "\n",
    "class ColumnIdentification(BaseModel):\n",
    "    likely_sample_column: str = Field(..., description=\"The column name most likely to contain sample identifiers\")\n",
    "    confidence: float = Field(..., description=\"Confidence score for the column identification (0-1)\")\n",
    "    reasoning: str = Field(..., description=\"Explanation for why this column was chosen\")\n",
    "\n",
    "class SampleMatch(BaseModel):\n",
    "    metadata_sample: str = Field(..., description=\"The sample name from the metadata\")\n",
    "    file_name: str = Field(..., description=\"The matched file name\")\n",
    "    confidence: float = Field(..., description=\"Confidence score of the match (0-1)\")\n",
    "\n",
    "class MatchResult(BaseModel):\n",
    "    column_identification: ColumnIdentification = Field(..., description=\"Identification of the sample name column\")\n",
    "    matches: List[SampleMatch] = Field(..., description=\"List of matched samples and file names\")\n",
    "    matching_logic: str = Field(..., description=\"Explanation of the logic used to match samples to file names\")\n",
    "\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def create_prompt(metadata_df, file_names):\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "## IDENTITY AND PURPOSE\n",
    "\n",
    "You are an expert in bioinformatic analyses. You will be provided with a list of files, and a metadata data frame. You are tasked with matching the file names to the existing metadata data frame. \n",
    "\n",
    "Take a deep breath, and carefully follow the steps outlined below to achieve the intended task.\n",
    "\n",
    "## STEPS\n",
    "\n",
    "1. First, identify the column name that is most likely to correspond to sample names. Focus on words such as \"sample name\" or \"ID\" to make this judgement.\n",
    "2. Match file names to rows in the metadata. Keep the following in mind:\n",
    "- Every file name should match to a row in the metadata\n",
    "- There may be errors in the metadata\n",
    "- You should make efforts to identify likely errors, and consider the possibility of a match if the error was corrected\n",
    "- Examples of errors include random spaces, incorrect spellings, inconsistent cases, and random punctuations\n",
    "- Note that you may need to systematically apply some rules to better identify matches\n",
    "- Do not prescriptively follow a rule to find matches; be flexible instead.\n",
    "\n",
    "## OUTPUT\n",
    "\n",
    "Provided your analysis in a structured format:\n",
    "1. Provide the column name most likely to correspond to sample names, with a confidence score and reasoning.\n",
    "2. List of each sample from the identified column and its likely corresponding file name.\n",
    "3. Explain the logic used to match samples to file names.\n",
    "\n",
    "## INPUT\n",
    "Metadata columns:\n",
    "{metadata_df.columns.tolist()}\n",
    "\n",
    "Metadata:\n",
    "{metadata_df.to_string()}\n",
    "\n",
    "File names:\n",
    "{file_names}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_openai_response(prompt, openai_api_key):\n",
    "    client = instructor.patch(OpenAI(\n",
    "    api_key=openai_api_key))\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_model=MatchResult\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def update_metadata(metadata_df, match_result):\n",
    "    sample_column = match_result.column_identification.likely_sample_column\n",
    "    file_name_dict = {match.metadata_sample: match.file_name for match in match_result.matches}\n",
    "    file_match_score = {match.metadata_sample: match.confidence for match in match_result.matches}\n",
    "    metadata_df['file_name'] = metadata_df[sample_column].map(file_name_dict)\n",
    "    metadata_df['match_confidence'] = metadata_df[sample_column].map(file_match_score)\n",
    "    return metadata_df\n",
    "\n",
    "def process_metadata_and_files(metadata_file_path, file_names_path, openai_api_key):\n",
    "    # Read the metadata CSV file\n",
    "    metadata_df = read_csv(metadata_file_path)\n",
    "    \n",
    "    # Read the list of sample file names. For the moment I just manually specified the input text files, but this will need to change later.\n",
    "    with open(file_names_path, 'r') as file:\n",
    "        file_names = file.read().splitlines()\n",
    "    \n",
    "    # Create a prompt with the metadata and file names\n",
    "    prompt = create_prompt(metadata_df, file_names)\n",
    "    \n",
    "    # Get the response from OpenAI\n",
    "    response = get_openai_response(prompt, openai_api_key)\n",
    "    \n",
    "    if isinstance(response, str):\n",
    "        print(response)\n",
    "        return\n",
    "    \n",
    "    # Print the identified sample name column and reasoning\n",
    "    print(f\"Likely sample name column: {response.column_identification.likely_sample_column}\")\n",
    "    print(f\"Confidence: {response.column_identification.confidence}\")\n",
    "    print(f\"Reasoning: {response.column_identification.reasoning}\\n\")\n",
    "    \n",
    "    # Print the matching logic\n",
    "    print(\"Matching logic:\")\n",
    "    print(response.matching_logic)\n",
    "    print()\n",
    "    \n",
    "    # Update the metadata DataFrame with matched file names\n",
    "    updated_metadata = update_metadata(metadata_df, response)\n",
    "    \n",
    "    # Print the updated metadata\n",
    "    return(updated_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ef456-657b-476f-b86c-965b69c301d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    api_key = openai_api_key\n",
    "\n",
    "    # Path to your metadata CSV file\n",
    "    metadata_file_path = \"../InputData/SETBP1_Tests/SETBP1_RNAseq_samples_errors.csv\"\n",
    "    \n",
    "    # Path to your file names list\n",
    "    file_names_path = \"../InputData/SETBP1_Tests/abundance_files.txt\"\n",
    "    \n",
    "    # Process the metadata and files\n",
    "    updated_metadata = process_metadata_and_files(metadata_file_path, file_names_path, api_key)\n",
    "\n",
    "updated_metadata.to_csv(\"../results/2024_07_31_AutomatedDataProcessing/Errors_MetadataSampleMatching.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3d0dba03-3a9a-4c18-bcfd-1a741692391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.to_string of                           Sample Name Transfection      Genotype Cell type   \\\n",
      "0      KOLF2_SETBP1_VUS2_A21_10_day 0            A   VUS2 HDR/WT       iPSC   \n",
      "1      KOLF2_SETBP1_ VUS2_A21_7_day 0            A   VUS2 HDR/WT       iPSC   \n",
      "2   KOLF2_SETBP1_VUS2_A21_10_day 24.             A          VUS@        NPC   \n",
      "3      KOLF2_SETBP1_VUS2_A21_7_day 24            A   VUS2 HDR/WT        NPC   \n",
      "4      KOLF2_SETBP1_VUS2_B1_4.1_day 0            B   VUS2 HDR/WT       iPSC   \n",
      "5      KOLF2_SETBP1_VUS2_B1_4.2_day 0            B   VUS2 HDR/WT       iPSC   \n",
      "6     KOLF2_SETBP1_VUS2_B1_4.1_day 24            B   vus2 HDR/WT        NPC   \n",
      "7     KOLF2_SETBP1_VUS2_B1_4.2_day 24            B   VUS2 HDR/WT        NPC   \n",
      "8    KOLF2_SETBP1_VUS2_C12.1_WT_day 0            C         WT/WT       iPSC   \n",
      "9    KOLF2_SETBP1_VUS2_C12.2_WT_day 0            C         WT/WT       iPSC   \n",
      "10   KOLF2_SETBP1_VUS2_C12-3_WT_day 0            C         WT/WT       iPSC   \n",
      "11  KOLF2_SETBP1_VUS2_C12.1_WT_day 24            C         WT/WT        NPC   \n",
      "12  KOLF2_SETBP1_VUS2_C12.2_WT_day 24            C         WT/WT        NPC   \n",
      "13  KOLF2_SETBP1_VUS2_C12.3_WT_day 24            C         WT/WT        NPC   \n",
      "14    KOLF2_SETBP1_PATH1_4.6_WT_day 0            D           NaN       iPSC   \n",
      "15    KOLF2_SETBP1_PATH1_4.7_WT_day 0            D         WT/WT       iPSC   \n",
      "16   KOLF2_SETBP1_PATH1_4.6_WT_day 24            D         WT/wt        NaN   \n",
      "17   KOLF2_SETBP1_PATH1_4.7_WT_day 24            D         WT/WT        NaN   \n",
      "18        KOLF2_SETBP1_PATH2_3.2_day0            E  PATH2 HDR/WT       iPSC   \n",
      "19    KOLF2_SETBP1_PATH2_3.3_WT_day 0            E         WT/WT       iPSC   \n",
      "20       KOLF2_SETBP1_PATH2_3.4_day 0            E  PATH2 HDR/WT       iPSC   \n",
      "21       KOLF2_SETBP1_PATH2_3.6_day 0            E  PATH2 HDR/WT       iPSC   \n",
      "22      KOLF2_SETBP1_PATH2_3.2_day 24            E  PATH2 HDR/WT        NPC   \n",
      "23   kolf2_SETBP1_PATH2_3.3_WT_day 24            E         WT/WT        NPC   \n",
      "24      KOLF2_SETBP1_PATH2_3.4_day 24            E  PATH2 HDR/WT        NPC   \n",
      "25      KLOF2_SETBP1_PATH2_3.6_day 24            E  PATH2 HDR/WT        NPC   \n",
      "26     KOLF2_SETBP1_PATH3_15.10_day 0            F  PATH3 HDR/WT       iPSC   \n",
      "27    KOLF2_SETBP1_PATH3_15.7.1_day 0            F         PATH3       iPSC   \n",
      "28    KOLF2_SETPB1_PATH3_15.7.2_day 0            F  PATH3 HDR/WT       iPSC   \n",
      "29    KOLF2_SETBP1_PATH3_15.10_day 24            F  PATH3 HDR/WT        NPC   \n",
      "30   KOLF2_SETBP1_PATH3_15.7.1_day 24            F  PATH3 HDR/WT        NPC   \n",
      "31   KOLF2_SETBP1_PATH3_15.7.2_day 24            F  PATH3 HDR/WT        NPC   \n",
      "\n",
      "    Neural differentiation set  \n",
      "0                            2  \n",
      "1                            1  \n",
      "2                            2  \n",
      "3                            1  \n",
      "4                            1  \n",
      "5                            2  \n",
      "6                            1  \n",
      "7                            2  \n",
      "8                            1  \n",
      "9                            1  \n",
      "10                           2  \n",
      "11                           1  \n",
      "12                           1  \n",
      "13                           2  \n",
      "14                           1  \n",
      "15                           2  \n",
      "16                           1  \n",
      "17                           2  \n",
      "18                           1  \n",
      "19                           2  \n",
      "20                           1  \n",
      "21                           2  \n",
      "22                           1  \n",
      "23                           2  \n",
      "24                           1  \n",
      "25                           2  \n",
      "26                           2  \n",
      "27                           1  \n",
      "28                           2  \n",
      "29                           2  \n",
      "30                           1  \n",
      "31                           2  >\n"
     ]
    }
   ],
   "source": [
    "metadata_df = read_csv(\"../InputData/SETBP1_Tests/SETBP1_RNAseq_samples_errors.csv\")\n",
    "    \n",
    "    # Read the list of sample file names. For the moment I just manually specified the input text files, but this will need to change later.\n",
    "with open(file_names_path, 'r') as file:\n",
    "    file_names = file.read().splitlines()\n",
    "\n",
    "class ColumnIdentification(BaseModel):\n",
    "    likely_sample_column: str = Field(..., description=\"The column name most likely to contain sample identifiers\")\n",
    "    confidence: float = Field(..., description=\"Confidence score for the column identification (0-1)\")\n",
    "    reasoning: str = Field(..., description=\"Explanation for why this column was chosen\")\n",
    "\n",
    "class SampleMatch(BaseModel):\n",
    "    metadata_sample: str = Field(..., description=\"The matched sample name as reported in the metadata\")\n",
    "    file_name: str = Field(..., description=\"The file name\")\n",
    "    confidence: float = Field(..., description=\"Confidence score of the match (0-1)\")\n",
    "\n",
    "class MatchResult(BaseModel):\n",
    "    column_identification: ColumnIdentification = Field(..., description=\"Identification of the sample name column\")\n",
    "    matches: List[SampleMatch] = Field(..., description=\"List of matched samples and file names\")\n",
    "\n",
    "def create_prompt(metadata_df, file_names):\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "## IDENTITY AND PURPOSE\n",
    "\n",
    "You are an expert in bioinformatic analyses. You will be provided with a list of files, and a metadata data frame. You are tasked with matching the file names to the existing metadata data frame. \n",
    "\n",
    "Take a deep breath, and carefully follow the steps outlined below to achieve the intended task.\n",
    "\n",
    "## STEPS\n",
    "\n",
    "1. First, identify the column name that is most likely to correspond to sample names. Focus on words such as \"sample name\" or \"ID\" to make this judgement.\n",
    "2. Match ALL file names to rows in the metadata. Keep the following in mind:\n",
    "- Every file name should match to a row in the metadata\n",
    "- There may be errors in the metadata\n",
    "- You should make efforts to identify likely errors, and consider the possibility of a match if the error was corrected\n",
    "- Examples of errors include random spaces, incorrect spellings, inconsistent cases, and random punctuations\n",
    "- Note that you may need to systematically apply some rules to better identify matches\n",
    "- Do not prescriptively follow a rule to find matches; be flexible instead\n",
    "- If a file name does not have a match in the sample metadata, report this as \"No Match\"\n",
    "\n",
    "## OUTPUT\n",
    "\n",
    "Provided your analysis in a structured format:\n",
    "1. Provide the column name most likely to correspond to sample names, with a confidence score and reasoning.\n",
    "2. List of EACH file, and the most likely corresponding sample name. Report the sample name EXACTLY as it is in the metadata - do not attempt to correct the metadata value. Similarly, report the file name EXACTLY as given.\n",
    "\n",
    "## INPUT\n",
    "Metadata columns:\n",
    "{metadata_df.columns.tolist()}\n",
    "\n",
    "Metadata:\n",
    "{metadata_df.to_string()}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "    \n",
    "prompt = create_prompt(metadata_df, file_names)\n",
    "\n",
    "def get_openai_response(prompt, openai_api_key):\n",
    "    client = instructor.patch(OpenAI(\n",
    "    api_key=openai_api_key))\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            max_tokens = 10000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_model=MatchResult\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "response = get_openai_response(prompt, openai_api_key)\n",
    "print(metadata_df.to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7c5d82e3-22be-431f-86f3-3f4de0dfba1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_A21_10_day 0', file_name='KOLF2_SETBP1_VUS2_A21_10_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_ VUS2_A21_7_day 0', file_name='KOLF2_SETBP1_ VUS2_A21_7_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_A21_10_day 24.', file_name='KOLF2_SETBP1_VUS2_A21_10_day 24.', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_A21_7_day 24', file_name='KOLF2_SETBP1_VUS2_A21_7_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_B1_4.1_day 0', file_name='KOLF2_SETBP1_VUS2_B1_4.1_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_B1_4.2_day 0', file_name='KOLF2_SETBP1_VUS2_B1_4.2_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_B1_4.1_day 24', file_name='KOLF2_SETBP1_VUS2_B1_4.1_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_B1_4.2_day 24', file_name='KOLF2_SETBP1_VUS2_B1_4.2_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_C12.1_WT_day 0', file_name='KOLF2_SETBP1_VUS2_C12.1_WT_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_C12.2_WT_day 0', file_name='KOLF2_SETBP1_VUS2_C12.2_WT_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_C12-3_WT_day 0', file_name='KOLF2_SETBP1_VUS2_C12-3_WT_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_C12.1_WT_day 24', file_name='KOLF2_SETBP1_VUS2_C12.1_WT_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_C12.2_WT_day 24', file_name='KOLF2_SETBP1_VUS2_C12.2_WT_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_VUS2_C12.3_WT_day 24', file_name='KOLF2_SETBP1_VUS2_C12.3_WT_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH1_4.6_WT_day 0', file_name='KOLF2_SETBP1_PATH1_4.6_WT_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH1_4.7_WT_day 0', file_name='KOLF2_SETBP1_PATH1_4.7_WT_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH1_4.6_WT_day 24', file_name='KOLF2_SETBP1_PATH1_4.6_WT_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH1_4.7_WT_day 24', file_name='KOLF2_SETBP1_PATH1_4.7_WT_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH2_3.2_day0', file_name='KOLF2_SETBP1_PATH2_3.2_day0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH2_3.3_WT_day 0', file_name='KOLF2_SETBP1_PATH2_3.3_WT_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH2_3.4_day 0', file_name='KOLF2_SETBP1_PATH2_3.4_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH2_3.6_day 0', file_name='KOLF2_SETBP1_PATH2_3.6_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH2_3.2_day 24', file_name='KOLF2_SETBP1_PATH2_3.2_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH2_3.3_WT_day 24', file_name='KOLF2_SETBP1_PATH2_3.3_WT_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH2_3.4_day 24', file_name='KOLF2_SETBP1_PATH2_3.4_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KLOF2_SETBP1_PATH2_3.6_day 24', file_name='KLOF2_SETBP1_PATH2_3.6_day 24', confidence=0.9),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH3_15.10_day 0', file_name='KOLF2_SETBP1_PATH3_15.10_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH3_15.7.1_day 0', file_name='KOLF2_SETBP1_PATH3_15.7.1_day 0', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETPB1_PATH3_15.7.2_day 0', file_name='No Match', confidence=0.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH3_15.10_day 24', file_name='KOLF2_SETBP1_PATH3_15.10_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH3_15.7.1_day 24', file_name='KOLF2_SETBP1_PATH3_15.7.1_day 24', confidence=1.0),\n",
       " SampleMatch(metadata_sample='KOLF2_SETBP1_PATH3_15.7.2_day 24', file_name='KOLF2_SETBP1_PATH3_15.7.2_day 24', confidence=1.0)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fbffc-9354-4b67-9bc8-ce8a128d0337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry)",
   "language": "python",
   "name": "poetry-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
