{
 "cells": [
  {
   "cell_type": "raw",
   "id": "232f6a47-2bc6-4384-8de9-14140fc9d4f8",
   "metadata": {},
   "source": [
    "This notebook will contain attempts to analyse Kallisto data.\n",
    "\n",
    "While Kallisto is downstream of downloading FASTQ files (and subsequently quantification via Kallisto), I will still just be starting here. \n",
    "\n",
    "My goals here are as follows:\n",
    "- Process metadata (including interpreting the different research groups, identifying what comparisons might be interesting, fixing any errors in the metadata. I will be providing a metadata sheet with mistakes to see how this will work.). Note that the prompt I end up needing here might have to be revised based on how I go with the dataset download.\n",
    "- Create (what I assume is) the DGEList object in Python, i.e. via PyDESeq2. If I can't get this to work, I will create an R script and call this script in Python. This is true for future steps too. Note that part of this entails being able to match samples to the correct files\n",
    "- Perform the filtering and normalisation. Should be straightfoward (I think...) if I can get the above to work\n",
    "- Perform the DEG analysis. Note that this relies on correctly identifying the groups. \n",
    "- Perform an ORA/GSEA. If I can get the above to work, this should be straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d5ce11-4322-466f-8ec6-397ca9f78750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2546b752-753c-4247-aa60-37fd3c702b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('../.env')\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d981877-4137-4ef5-b1e7-4478705bd96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red, Orange, Yellow, Green, Blue, Indigo, Violet.\n"
     ]
    }
   ],
   "source": [
    "# Test OpenAI API...\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=openai_api_key,  # this is also the default, it can be omitted\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the colours of the rainbow? Only respond with the names of the colours.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "result = chat_completion.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98796af4-7e42-4728-8bbf-84fbd8b487d2",
   "metadata": {},
   "source": [
    "I think my goal will be to integrate R, OpenAI, and Python. I would feel more confident working with an edgeR/limma based pipeline. \n",
    "\n",
    "Therefore, I will begin with just a simple implementation, testing SETBP1 data.\n",
    "\n",
    "Note that while sample data is provided with pydeseq2, this is completely synthetic - the genes are not real, and therefore this ends up not being very valuable.\n",
    "\n",
    "With R integration, the specific steps are as follows:\n",
    "- Identify the Kallisto files (likely no LLM needed)\n",
    "- Identify other files (i.e. tx2gene) (likely no LLM needed)\n",
    "- Process metadata, i.e. correct errors, match to Kallisto files (LLM needed)\n",
    "- Import Kallisto counts (no LLM needed)\n",
    "- Add metadata (and gene data) to the Kallisto counts, i.e. create DGEList object (no LLM needed)\n",
    "- Perform filtering and normalisation (no LLM needed, I can use default settings)\n",
    "- Define contrasts (probably the main hard part - from a value standpoint, there are considerations around which contrasts are interesting and valid, from the technical standpoint it's probably ok? Unsure how easily the LLM will interpret the metadata...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ccf30-35f8-4a34-aa73-24503445a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing correction of metadata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d88fd0ff-c0d5-4b51-9f91-e5d241b4020c",
   "metadata": {},
   "source": [
    "I am going to begin with testing to see if I can get the LLM to analyse metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1bab847-c833-43f6-8115-5435954848e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test matching of Kallisto samples to metadata\n",
    "\n",
    "# I will begin by testing to see if 1) Kallisto samples can be matched to metadata based on the names of the samples in the metadata, and 2) if the two can be merged together in a single data frame\n",
    "\n",
    "class ColumnIdentification(BaseModel):\n",
    "    likely_sample_column: str = Field(..., description=\"The column name most likely to contain sample identifiers\")\n",
    "    confidence: float = Field(..., description=\"Confidence score for the column identification (0-1)\")\n",
    "    reasoning: str = Field(..., description=\"Explanation for why this column was chosen\")\n",
    "\n",
    "class SampleMatch(BaseModel):\n",
    "    metadata_sample: str = Field(..., description=\"The sample name from the metadata\")\n",
    "    file_name: str = Field(..., description=\"The matched file name\")\n",
    "    confidence: float = Field(..., description=\"Confidence score of the match (0-1)\")\n",
    "\n",
    "class MatchResult(BaseModel):\n",
    "    column_identification: ColumnIdentification = Field(..., description=\"Identification of the sample name column\")\n",
    "    matches: List[SampleMatch] = Field(..., description=\"List of matched samples and file names\")\n",
    "    matching_logic: str = Field(..., description=\"Explanation of the logic used to match samples to file names\")\n",
    "\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def create_prompt(metadata_df, file_names):\n",
    "    prompt = f\"\"\"Analyze the following metadata and list of file names:\n",
    "\n",
    "Metadata columns:\n",
    "{metadata_df.columns.tolist()}\n",
    "\n",
    "Metadata:\n",
    "{metadata_df.to_string()}\n",
    "\n",
    "File names:\n",
    "{file_names}\n",
    "\n",
    "Tasks:\n",
    "1. Identify the column most likely to contain sample identifiers. Provide the column name, a confidence score, and your reasoning.\n",
    "2. Match each sample from the identified column to the most likely corresponding file name. Consider variations in capitalization, spaces, dashes, and potential typos.\n",
    "3. Explain the logic you used to match samples to file names.\n",
    "\n",
    "Provide your analysis in a structured format.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_openai_response(prompt, openai_api_key):\n",
    "    client = instructor.patch(OpenAI(\n",
    "    api_key=openai_api_key))\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_model=MatchResult\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def update_metadata(metadata_df, match_result):\n",
    "    sample_column = match_result.column_identification.likely_sample_column\n",
    "    file_name_dict = {match.metadata_sample: match.file_name for match in match_result.matches}\n",
    "    metadata_df['file_name'] = metadata_df[sample_column].map(file_name_dict)\n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff40434-4b29-4d85-9749-f25219082f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    api_key = openai_api_key\n",
    "\n",
    "    # Path to your metadata CSV file\n",
    "    metadata_file_path = \"../InputData/SETBP1_Tests/SETBP1_RNAseq_samples.csv\"\n",
    "    \n",
    "    # Read the metadata CSV file\n",
    "    metadata_df = read_csv(metadata_file_path)\n",
    "    \n",
    "    # List of sample file names (you would typically read this from a directory or another source)\n",
    "    with open(\"../InputData/SETBP1_Tests/abundance_files.txt\", 'r') as file:\n",
    "        file_names = file.read()\n",
    "    \n",
    "    # Create a prompt with the metadata and file names\n",
    "    prompt = create_prompt(metadata_df, file_names)\n",
    "    \n",
    "    # Get the response from OpenAI\n",
    "    response = get_openai_response(prompt, openai_api_key)\n",
    "    # Print the identified sample name column and reasoning\n",
    "    print(f\"Likely sample name column: {response.column_identification.likely_sample_column}\")\n",
    "    print(f\"Confidence: {response.column_identification.confidence}\")\n",
    "    print(f\"Reasoning: {response.column_identification.reasoning}\\n\")\n",
    "    \n",
    "    # Print the matching logic\n",
    "    print(\"Matching logic:\")\n",
    "    print(response.matching_logic)\n",
    "    print()\n",
    "    \n",
    "    # Update the metadata DataFrame with matched file names\n",
    "    updated_metadata = update_metadata(metadata_df, response)\n",
    "    \n",
    "    # Print the updated metadata\n",
    "    print(\"Updated Metadata:\")\n",
    "    print(updated_metadata)\n",
    "    \n",
    "    # Optionally, save the updated metadata to a new CSV file\n",
    "    updated_metadata.to_csv(\"../results/2024_07_31_AutomatedDataProcessing/Clean_MetadataSampleMatching.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b5a8b-37e0-4dd3-ac7b-0536f5ca6320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry)",
   "language": "python",
   "name": "poetry-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
