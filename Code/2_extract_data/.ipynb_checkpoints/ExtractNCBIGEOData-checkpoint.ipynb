{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4410b7ed-cce0-443e-b3b8-c33c611bdb97",
   "metadata": {},
   "source": [
    "This notebook is intended to develop a framework for extracting data from NCBI GEO datasets.\n",
    "\n",
    "There are approaches I can take. I initially had wanted to extract the FASTQ files - however, I am now considering whether simply getting the expression matrices is sufficient.\n",
    "- this is reliant on all datasets having expression matrices. I'm not sure this is a safe assumption. \n",
    "- In my initial test cases, I was able to get expression matrices from either supp files or \"main\" (?) files. It is trivial to access either of these using GEOquery in R. However, automating this might be trickier.\n",
    "\n",
    "It feels a possible approach is:\n",
    "- Extract main data - \"is there a counts matrix?\"\n",
    "- Extract supplementary data - \"is there a counts matrix?\"\n",
    "- Extract metadata\n",
    "\n",
    "The obvious question in this case is \"will there always be a counts matrix,\" to which the answer is no. My train of thought is then to process FASTQ files.\n",
    "- As it happens, if I set up a pipeline to just do the analysis for FASTQ files, it would greatly simplify the insights that researchers can extract when getting new data? \n",
    "- Alternatively I could say \"start with a counts matrix,\" that could be a viable approach.\n",
    "\n",
    "Ah yeah - I will most likely be using R scripts to extract data. GEOquery worked well in my interactive test cases. \n",
    "\n",
    "Therefore, this notebook will be a Python script (where I call upon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8498eb7-4f14-429c-81c9-aa74c3390c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry)",
   "language": "python",
   "name": "poetry-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
