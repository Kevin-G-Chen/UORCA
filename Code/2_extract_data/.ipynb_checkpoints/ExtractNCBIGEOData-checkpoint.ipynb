{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4410b7ed-cce0-443e-b3b8-c33c611bdb97",
   "metadata": {},
   "source": [
    "This notebook is intended to develop a framework for extracting data from NCBI GEO datasets.\n",
    "\n",
    "There are approaches I can take. I initially had wanted to extract the FASTQ files - however, I am now considering whether simply getting the expression matrices is sufficient.\n",
    "- this is reliant on all datasets having expression matrices. I'm not sure this is a safe assumption. \n",
    "- In my initial test cases, I was able to get expression matrices from either supp files or \"main\" (?) files. It is trivial to access either of these using GEOquery in R. However, automating this might be trickier.\n",
    "\n",
    "It feels a possible approach is:\n",
    "- Extract main data - \"is there a counts matrix?\"\n",
    "- Extract supplementary data - \"is there a counts matrix?\"\n",
    "- Extract metadata\n",
    "\n",
    "The obvious question in this case is \"will there always be a counts matrix,\" to which the answer is no. My train of thought is then to process FASTQ files.\n",
    "- As it happens, if I set up a pipeline to just do the analysis for FASTQ files, it would greatly simplify the insights that researchers can extract when getting new data? \n",
    "- Alternatively I could say \"start with a counts matrix,\" that could be a viable approach.\n",
    "\n",
    "Ah yeah - I will most likely be using R scripts to extract data. GEOquery worked well in my interactive test cases. \n",
    "\n",
    "Therefore, this notebook will be a Python script (where I call upon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8498eb7-4f14-429c-81c9-aa74c3390c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import instructor # I'm not sure if this will actually be needed...\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b383a139-6f9b-4926-9305-7ef604408c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peregrine Falcon\n"
     ]
    }
   ],
   "source": [
    "load_dotenv('../../.env')\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Test OpenAI API...\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=openai_api_key,  # this is also the default, it can be omitted\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you state the name of a real species of bird? Only reply with the species name.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "result = chat_completion.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "605e4d07-3b52-4f55-9b05-e7f8186dfa53",
   "metadata": {},
   "source": [
    "Can't say I've heard of the above. (edit new test, yes i do know what a peregrine falcon is)\n",
    "\n",
    "Ok now it goes back to pica pica (Pikachu?). apparently a eurasian magpie?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448ec3f-e21f-4095-8990-58682dc9b48d",
   "metadata": {},
   "source": [
    "# Plan of action\n",
    "\n",
    "The ideal plan will be to:\n",
    "- Download FASTQ files\n",
    "- Download metadata\n",
    "\n",
    "With a bit of exploration, I have come to the realisation that some datasets will have both human and mouse samples (for example). Therefore, guiding the type of Kallisto analysis that I do will be important...\n",
    "\n",
    "What do I specifically need to do?\n",
    "- I do honestly think downloading the FASTQ files is straightforward (spoilers - it is not):\n",
    "- Download metadata, then identify the SRR ID (or whatever it's called - EDIT - turns out this is not so straightforward. There's either a knowledge gap in me, or I will need an LLM here)\n",
    "- prefetch (SRR)\n",
    "- fasterq dump (x)\n",
    "- kallisto quant (x)\n",
    "\n",
    "In between I can use an LLM call to just say \"did this work\"?\n",
    "\n",
    "So what will the pipeline look like?\n",
    "\n",
    "1. We start with a GEO accession. These come from the previous step. For testing purposes, we can manually specify this (there is no challenge here)\n",
    "2. From the GEO accession, we download the metadata. This can be done in R\n",
    "3. In the metadata download, we can print out the output, and query an LLM to check if this occurred correctly. I think it's also good to look through the metadata, and see if it makes sense given the title (i.e. that the metadata matches the GEO accession)\n",
    "4. Use a bit of bash scripting (Entrez e-utilities) to go from the GEO accession to SRA IDs.\n",
    "5. I do note at least in the metadata that I've looked at that the SRA IDs are also in there. We can use this as a sanity check.\n",
    "6. We also use the metadata to determine which Kallisto index we need to download (there is also the option to produce the index myself, but I'd prefer to download the pre-built indices if possible - that would be much easier).\n",
    "7. We take into the above information to download the FASTQ files and perform the Kallisto quantification."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc5b2fd3-981a-42d3-bbbb-ac3ca893faaa",
   "metadata": {},
   "source": [
    "Metadata download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853d42f2-a272-4cbe-8400-40339ce519b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R script output:\n",
      "Output directory exists: ../Testing/GSE273561 \n",
      "Retrieving metadata for GEO accession: GSE273561 \n",
      "Saving metadata to: ../Testing/GSE273561/GSE273561-GPL24247_series_matrix_metadata.csv \n",
      "Saving metadata to: ../Testing/GSE273561/GSE273561-GPL24676_series_matrix_metadata.csv \n",
      "Metadata saved successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the path to the R script\n",
    "r_script_path = \"./RScript_GetGEOMetadata.r\"  # Ensure the path is correct and script is executable\n",
    "\n",
    "# Define the GEO accession you want to use\n",
    "geo_accession = \"GSE273561\"  # Replace with the actual GEO accession number\n",
    "\n",
    "# Define output directory\n",
    "\n",
    "outdir = \"../Testing/GSE273561\"\n",
    "\n",
    "# Construct the command to run the R script with the GEO accession as an argument\n",
    "command = [\"Rscript\", r_script_path, \"-g\", geo_accession, \"-o\", outdir]\n",
    "\n",
    "# Call the R script using subprocess\n",
    "try:\n",
    "    result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "    print(\"R script output:\")\n",
    "    print(result.stdout)  # This will print the standard output from the R script\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error running R script:\")\n",
    "    print(e.stderr)  # This will print any error messages from the R script"
   ]
  },
  {
   "cell_type": "raw",
   "id": "553c0c77-6894-4cc5-a437-5ac931ab1b97",
   "metadata": {},
   "source": [
    "SRA ID identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91538daf-c846-403b-b65a-91b730a84cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script executed successfully\n",
      "Output:\n",
      " Processing GEO accession: GSE273561\n",
      "Processing sample: GSM8432354\n",
      "Processing sample: GSM8432353\n",
      "Processing sample: GSM8432352\n",
      "Processing sample: GSM8432351\n",
      "Processing sample: GSM8432350\n",
      "Processing sample: GSM8443197\n",
      "Processing sample: GSM8432349\n",
      "Processing sample: GSM8443196\n",
      "Processing sample: GSM8432348\n",
      "Processing sample: GSM8443195\n",
      "Processing sample: GSM8432347\n",
      "Processing sample: GSM8443194\n",
      "Processing sample: GSM8432346\n",
      "Processing sample: GSM8443193\n",
      "Processing sample: GSM8443192\n",
      "Processing sample: GSM8443191\n",
      "Processing sample: GSM8443190\n",
      "Processing sample: GSM8443189\n",
      "Processing sample: GSM8443188\n",
      "Processing sample: GSM8443187\n",
      "Processing sample: GSM8443186\n",
      "Processing sample: GSM8443185\n",
      "Processing sample: GSM8443184\n",
      "Processing sample: GSM8432345\n",
      "Processing sample: GSM8432344\n",
      "Processing sample: GSM8432343\n",
      "Processing sample: GSM8432342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your Bash script\n",
    "bash_script = './get_sra_ids.sh'\n",
    "\n",
    "# Define the GEO accession you want to pass to the script\n",
    "geo_accession = 'GSE273561'  # Replace with the actual GEO accession\n",
    "\n",
    "# Run the Bash script with the GEO accession as an argument\n",
    "try:\n",
    "    result = subprocess.run([bash_script, geo_accession], check=True, text=True, capture_output=True)\n",
    "    print(\"Script executed successfully\")\n",
    "    print(\"Output:\\n\", result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Script execution failed\")\n",
    "    print(\"Error:\\n\", e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4b0c6-864e-4581-8b31-f5b71ead3ebd",
   "metadata": {},
   "source": [
    "Now that I have at least a basic working example of extracting this data, I will work towards integrating these together. Specifically, this will entail:\n",
    "- Metadata: given the title/summary of the GEO accession, does the metadata overall make sense? Y/N\n",
    "- Is there anything to suggest consistency between the metadata and SRA IDs? (Experiments) Y/N\n",
    "\n",
    "Part of what I'm wondering is how much of the metadata I should be including. Clearly, including less would be cheaper, however I then risk losing information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e99712-b177-4db6-ad1d-fa5f35ab80e1",
   "metadata": {},
   "source": [
    "# Experimentation\n",
    "\n",
    "Here, I will work with the manually downloaded data for one example, GSE273651, to develop the LLM call. I can see that there should be matches between the sameple IDs and . This is also an interesting case where I have two metadata matrices.\n",
    "\n",
    "With a bit of testing, there is some hallucination (i.e. it will imagine up samples that do not exist). Therefore, there will need to be a mechanism that goes \"here's what we've reported, is this grounded in truth based on the metadata?\n",
    "\n",
    "So I think the workflow I will go with:\n",
    "- I extract metadata (1), GSM sample IDs, SRA experiment IDs, and SRA IDs (2) from a GEO dataset accession.\n",
    "  - The only LLM call I might want to make here is to print std.out and std.err output, and use this to check if things proceeded ok.\n",
    "- My expectation is that everything in (2) should align with eachother. However, we can do a check against the metadata to ensure this is the case:\n",
    "- In the metadata, link each GSM sample ID to a SRX ID (and perform an LLM check that this is correct, i.e. \"given the metadata, is this response correct?\")\n",
    "- Compare the metadata evaluataion to the (2) links. If they match, we can be confident (this conveniently gives us a way to link the metadata to the FASTQ files as well)\n",
    "- If the above checks all pass, then we can extract FASTQ files (as I did above - prefetch -> fasterq).\n",
    "- Independently, we also need to perform a check for what Kallisto file we need to download.\n",
    "\n",
    "In these steps, I do not see the value of performing any of these multiple times - for something which is just a \"yes/no\", I cannot why this would be valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75b509d-97ec-4dfa-9d09-1d8435237d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by loading my data. When I automate this, I need to ensure I capture the correct files\n",
    "meta1 = pd.read_csv(\"../Testing/GSE273561/GSE273561-GPL24247_series_matrix_metadata.csv\")\n",
    "meta2 = pd.read_csv(\"../Testing/GSE273561/GSE273561-GPL24676_series_matrix_metadata.csv\")\n",
    "SRA_IDs = pd.read_table(\"./results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70857f06-fa87-4e29-8c6c-6d9e06fb6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta1_string = meta1.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f6b254-8e47-4d14-bb88-ba51aecdb180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSMSample_ID=['GSM8432342', 'GSM8432343', 'GSM8432344', 'GSM8432345', 'GSM8443184', 'GSM8443185', 'GSM8443186', 'GSM8443187', 'GSM8443188', 'GSM8443189', 'GSM8443190', 'GSM8443191', 'GSM8443192', 'GSM8443193', 'GSM8443194', 'GSM8443195', 'GSM8443196', 'GSM8443197'] GSMSample_column='geo_accession' SRAExperiment_ID=['SRX25524103', 'SRX25524104', 'SRX25524105', 'SRX25524106', 'SRX25627123', 'SRX25627124', 'SRX25627125', 'SRX25627126', 'SRX25627127', 'SRX25627128', 'SRX25627129', 'SRX25627130', 'SRX25627131', 'SRX25627132', 'SRX25627133', 'SRX25627134', 'SRX25627135', 'SRX25627136'] SRAExperiment_column='relation.1'\n"
     ]
    }
   ],
   "source": [
    "class MetadataExtraction(BaseModel):\n",
    "    GSMSample_ID: list[str]\n",
    "    GSMSample_column: str\n",
    "    SRAExperiment_ID: list[str]\n",
    "    SRAExperiment_column: str\n",
    "\n",
    "prompt = f\"\"\"Consider the following information, which is a data frame which has been converted to JSON output. \n",
    "\n",
    "{meta1_string}\n",
    "\n",
    "Can you identify each of the unqiue GSM sample IDs, the column they are found in, and do the same for SRA experiment IDs?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chat_completion = client.beta.chat.completions.parse(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    response_format=MetadataExtraction,\n",
    ")\n",
    "\n",
    "result = chat_completion.choices[0].message.parsed\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5315ab1-c6e0-41d9-bb46-724591f74c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_accession</th>\n",
       "      <th>relation.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSM8432342</td>\n",
       "      <td>SRX25524103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GSM8432343</td>\n",
       "      <td>SRX25524104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GSM8432344</td>\n",
       "      <td>SRX25524105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSM8432345</td>\n",
       "      <td>SRX25524106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GSM8443184</td>\n",
       "      <td>SRX25627123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GSM8443185</td>\n",
       "      <td>SRX25627124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GSM8443186</td>\n",
       "      <td>SRX25627125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GSM8443187</td>\n",
       "      <td>SRX25627126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GSM8443188</td>\n",
       "      <td>SRX25627127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GSM8443189</td>\n",
       "      <td>SRX25627128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GSM8443190</td>\n",
       "      <td>SRX25627129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GSM8443191</td>\n",
       "      <td>SRX25627130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GSM8443192</td>\n",
       "      <td>SRX25627131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GSM8443193</td>\n",
       "      <td>SRX25627132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GSM8443194</td>\n",
       "      <td>SRX25627133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GSM8443195</td>\n",
       "      <td>SRX25627134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GSM8443196</td>\n",
       "      <td>SRX25627135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GSM8443197</td>\n",
       "      <td>SRX25627136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geo_accession   relation.1\n",
       "0     GSM8432342  SRX25524103\n",
       "1     GSM8432343  SRX25524104\n",
       "2     GSM8432344  SRX25524105\n",
       "3     GSM8432345  SRX25524106\n",
       "4     GSM8443184  SRX25627123\n",
       "5     GSM8443185  SRX25627124\n",
       "6     GSM8443186  SRX25627125\n",
       "7     GSM8443187  SRX25627126\n",
       "8     GSM8443188  SRX25627127\n",
       "9     GSM8443189  SRX25627128\n",
       "10    GSM8443190  SRX25627129\n",
       "11    GSM8443191  SRX25627130\n",
       "12    GSM8443192  SRX25627131\n",
       "13    GSM8443193  SRX25627132\n",
       "14    GSM8443194  SRX25627133\n",
       "15    GSM8443195  SRX25627134\n",
       "16    GSM8443196  SRX25627135\n",
       "17    GSM8443197  SRX25627136"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    result.GSMSample_column: result.GSMSample_ID,\n",
    "    result.SRAExperiment_column: result.SRAExperiment_ID\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry)",
   "language": "python",
   "name": "poetry-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
