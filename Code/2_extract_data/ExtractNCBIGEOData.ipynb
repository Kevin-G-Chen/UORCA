{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4410b7ed-cce0-443e-b3b8-c33c611bdb97",
   "metadata": {},
   "source": [
    "This notebook is intended to develop a framework for extracting data from NCBI GEO datasets.\n",
    "\n",
    "There are approaches I can take. I initially had wanted to extract the FASTQ files - however, I am now considering whether simply getting the expression matrices is sufficient.\n",
    "- this is reliant on all datasets having expression matrices. I'm not sure this is a safe assumption. \n",
    "- In my initial test cases, I was able to get expression matrices from either supp files or \"main\" (?) files. It is trivial to access either of these using GEOquery in R. However, automating this might be trickier.\n",
    "\n",
    "It feels a possible approach is:\n",
    "- Extract main data - \"is there a counts matrix?\"\n",
    "- Extract supplementary data - \"is there a counts matrix?\"\n",
    "- Extract metadata\n",
    "\n",
    "The obvious question in this case is \"will there always be a counts matrix,\" to which the answer is no. My train of thought is then to process FASTQ files.\n",
    "- As it happens, if I set up a pipeline to just do the analysis for FASTQ files, it would greatly simplify the insights that researchers can extract when getting new data? \n",
    "- Alternatively I could say \"start with a counts matrix,\" that could be a viable approach.\n",
    "\n",
    "Ah yeah - I will most likely be using R scripts to extract data. GEOquery worked well in my interactive test cases. \n",
    "\n",
    "Therefore, this notebook will be a Python script (where I call upon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8498eb7-4f14-429c-81c9-aa74c3390c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import instructor # I'm not sure if this will actually be needed...\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b383a139-6f9b-4926-9305-7ef604408c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pica pica\n"
     ]
    }
   ],
   "source": [
    "load_dotenv('../../.env')\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Test OpenAI API...\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=openai_api_key,  # this is also the default, it can be omitted\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you state the name of a real species of bird? Only reply with the species name.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "result = chat_completion.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "605e4d07-3b52-4f55-9b05-e7f8186dfa53",
   "metadata": {},
   "source": [
    "Can't say I've heard of the above. (edit new test, yes i do know what a peregrine falcon is)\n",
    "\n",
    "Ok now it goes back to pica pica (Pikachu?). apparently a eurasian magpie?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448ec3f-e21f-4095-8990-58682dc9b48d",
   "metadata": {},
   "source": [
    "# Plan of action\n",
    "\n",
    "The ideal plan will be to:\n",
    "- Download FASTQ files\n",
    "- Download metadata\n",
    "\n",
    "With a bit of exploration, I have come to the realisation that some datasets will have both human and mouse samples (for example). Therefore, guiding the type of Kallisto analysis that I do will be important...\n",
    "\n",
    "What do I specifically need to do?\n",
    "- I do honestly think downloading the FASTQ files is straightforward (spoilers - it is not):\n",
    "- Download metadata, then identify the SRR ID (or whatever it's called - EDIT - turns out this is not so straightforward. There's either a knowledge gap in me, or I will need an LLM here)\n",
    "- prefetch (SRR)\n",
    "- fasterq dump (x)\n",
    "- kallisto quant (x)\n",
    "\n",
    "In between I can use an LLM call to just say \"did this work\"?\n",
    "\n",
    "So what will the pipeline look like?\n",
    "\n",
    "1. We start with a GEO accession. These come from the previous step. For testing purposes, we can manually specify this (there is no challenge here)\n",
    "2. From the GEO accession, we download the metadata. This can be done in R\n",
    "3. In the metadata download, we can print out the output, and query an LLM to check if this occurred correctly. I think it's also good to look through the metadata, and see if it makes sense given the title (i.e. that the metadata matches the GEO accession)\n",
    "4. Use a bit of bash scripting (Entrez e-utilities) to go from the GEO accession to SRA IDs.\n",
    "5. I do note at least in the metadata that I've looked at that the SRA IDs are also in there. We can use this as a sanity check.\n",
    "6. We also use the metadata to determine which Kallisto index we need to download (there is also the option to produce the index myself, but I'd prefer to download the pre-built indices if possible - that would be much easier).\n",
    "7. We take into the above information to download the FASTQ files and perform the Kallisto quantification."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc5b2fd3-981a-42d3-bbbb-ac3ca893faaa",
   "metadata": {},
   "source": [
    "Metadata download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853d42f2-a272-4cbe-8400-40339ce519b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R script output:\n",
      "Output directory exists: ../Testing/GSE273561 \n",
      "Retrieving metadata for GEO accession: GSE273561 \n",
      "Saving metadata to: ../Testing/GSE273561/GSE273561-GPL24247_series_matrix_metadata.csv \n",
      "Saving metadata to: ../Testing/GSE273561/GSE273561-GPL24676_series_matrix_metadata.csv \n",
      "Metadata saved successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the path to the R script\n",
    "r_script_path = \"./RScript_GetGEOMetadata.r\"  # Ensure the path is correct and script is executable\n",
    "\n",
    "# Define the GEO accession you want to use\n",
    "geo_accession = \"GSE273561\"  # Replace with the actual GEO accession number\n",
    "\n",
    "# Define output directory\n",
    "\n",
    "outdir = \"../Testing/GSE273561\"\n",
    "\n",
    "# Construct the command to run the R script with the GEO accession as an argument\n",
    "command = [\"Rscript\", r_script_path, \"-g\", geo_accession, \"-o\", outdir]\n",
    "\n",
    "# Call the R script using subprocess\n",
    "try:\n",
    "    result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "    print(\"R script output:\")\n",
    "    print(result.stdout)  # This will print the standard output from the R script\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error running R script:\")\n",
    "    print(e.stderr)  # This will print any error messages from the R script"
   ]
  },
  {
   "cell_type": "raw",
   "id": "553c0c77-6894-4cc5-a437-5ac931ab1b97",
   "metadata": {},
   "source": [
    "SRA ID identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91538daf-c846-403b-b65a-91b730a84cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script executed successfully\n",
      "Output:\n",
      " Processing GEO accession: GSE273561\n",
      "Processing sample: GSM8432354\n",
      "Processing sample: GSM8432353\n",
      "Processing sample: GSM8432352\n",
      "Processing sample: GSM8432351\n",
      "Processing sample: GSM8432350\n",
      "Processing sample: GSM8443197\n",
      "Processing sample: GSM8432349\n",
      "Processing sample: GSM8443196\n",
      "Processing sample: GSM8432348\n",
      "Processing sample: GSM8443195\n",
      "Processing sample: GSM8432347\n",
      "Processing sample: GSM8443194\n",
      "Processing sample: GSM8432346\n",
      "Processing sample: GSM8443193\n",
      "Processing sample: GSM8443192\n",
      "Processing sample: GSM8443191\n",
      "Processing sample: GSM8443190\n",
      "Processing sample: GSM8443189\n",
      "Processing sample: GSM8443188\n",
      "Processing sample: GSM8443187\n",
      "Processing sample: GSM8443186\n",
      "Processing sample: GSM8443185\n",
      "Processing sample: GSM8443184\n",
      "Processing sample: GSM8432345\n",
      "Processing sample: GSM8432344\n",
      "Processing sample: GSM8432343\n",
      "Processing sample: GSM8432342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your Bash script\n",
    "bash_script = './get_sra_ids.sh'\n",
    "\n",
    "# Define the GEO accession you want to pass to the script\n",
    "geo_accession = 'GSE273561'  # Replace with the actual GEO accession\n",
    "\n",
    "# Run the Bash script with the GEO accession as an argument\n",
    "try:\n",
    "    result = subprocess.run([bash_script, geo_accession], check=True, text=True, capture_output=True)\n",
    "    print(\"Script executed successfully\")\n",
    "    print(\"Output:\\n\", result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Script execution failed\")\n",
    "    print(\"Error:\\n\", e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4b0c6-864e-4581-8b31-f5b71ead3ebd",
   "metadata": {},
   "source": [
    "Now that I have at least a basic working example of extracting this data, I will work towards integrating these together. Specifically, this will entail:\n",
    "- Metadata: given the title/summary of the GEO accession, does the metadata overall make sense? Y/N\n",
    "- Is there anything to suggest consistency between the metadata and SRA IDs? (Experiments) Y/N\n",
    "\n",
    "Part of what I'm wondering is how much of the metadata I should be including. Clearly, including less would be cheaper, however I then risk losing information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e99712-b177-4db6-ad1d-fa5f35ab80e1",
   "metadata": {},
   "source": [
    "# Experimentation\n",
    "\n",
    "Here, I will work with the manually downloaded data for one example, GSE273651, to develop the LLM call. I can see that there should be matches. This is also an interesting case where I have two metadata matrices.\n",
    "\n",
    "With a bit of testing, there is some hallucination (i.e. it will imagine up samples that do not exist). Therefore, there will need to be a mechanism that goes \"here's what we've reported, is this grounded in truth based on the metadata?\n",
    "\n",
    "So I think the workflow I will go with:\n",
    "- I extract metadata (1), GSM sample IDs, SRA experiment IDs, and SRA IDs (2) from a GEO dataset accession\n",
    "- My expectation is that everything in (2) should align with eachother. However, we can do a check against the metadata to ensure this is the case:\n",
    "- In the metadata, link each GSM sample ID to a SRX ID (and perform an LLM check that this is correct, i.e. \"given the metadata, is this response correct?\")\n",
    "- Compare the metadata evaluataion to the (2) links. If they match, we can be confident (this conveniently gives us a way to link the metadata to the FASTQ files as well)\n",
    "- If the above checks all pass, then we can extract FASTQ files (as I did above - prefetch -> fasterq).\n",
    "- Independently, we also need to perform a check for what Kallisto file we need to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d75b509d-97ec-4dfa-9d09-1d8435237d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by loading my data\n",
    "meta1 = pd.read_csv(\"../Testing/GSE273561/GSE273561-GPL24247_series_matrix_metadata.csv\")\n",
    "meta2 = pd.read_csv(\"../Testing/GSE273561/GSE273561-GPL24676_series_matrix_metadata.csv\")\n",
    "SRA_IDs = pd.read_table(\"./results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70857f06-fa87-4e29-8c6c-6d9e06fb6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta1_string = meta1.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6f6b254-8e47-4d14-bb88-ba51aecdb180",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Beta' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m     SRAExperiment_column: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mConsider the following information, which is a data frame which has been converted to JSON output. \u001b[39m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mmeta1_string\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m     16\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     17\u001b[0m         {\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m     20\u001b[0m         }\n\u001b[1;32m     21\u001b[0m     ],\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     response_format\u001b[38;5;241m=\u001b[39mMetadataExtraction,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m result \u001b[38;5;241m=\u001b[39m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mparsed\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Beta' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "class MetadataExtraction(BaseModel):\n",
    "    GSMSample_ID: list[str]\n",
    "    GSMSample_column: str\n",
    "    SRAExperiment_ID: list[str]\n",
    "    SRAExperiment_column: str\n",
    "\n",
    "prompt = f\"\"\"Consider the following information, which is a data frame which has been converted to JSON output. \n",
    "\n",
    "{meta1_string}\n",
    "\n",
    "Can you identify each of the unqiue GSM sample IDs, the column they are found in, and do the same for SRA experiment IDs?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chat_completion = client.beta.chat.completions.parse(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    response_format=MetadataExtraction,\n",
    ")\n",
    "\n",
    "result = chat_completion.choices[0].message.parsed\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5315ab1-c6e0-41d9-bb46-724591f74c72",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (547858539.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[54], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    client.chat.completions.\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "client.chat.completions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry)",
   "language": "python",
   "name": "poetry-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
