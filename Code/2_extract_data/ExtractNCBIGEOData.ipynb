{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4410b7ed-cce0-443e-b3b8-c33c611bdb97",
   "metadata": {},
   "source": [
    "This notebook is intended to develop a framework for extracting data from NCBI GEO datasets.\n",
    "\n",
    "There are approaches I can take. I initially had wanted to extract the FASTQ files - however, I am now considering whether simply getting the expression matrices is sufficient.\n",
    "- this is reliant on all datasets having expression matrices. I'm not sure this is a safe assumption. \n",
    "- In my initial test cases, I was able to get expression matrices from either supp files or \"main\" (?) files. It is trivial to access either of these using GEOquery in R. However, automating this might be trickier.\n",
    "\n",
    "It feels a possible approach is:\n",
    "- Extract main data - \"is there a counts matrix?\"\n",
    "- Extract supplementary data - \"is there a counts matrix?\"\n",
    "- Extract metadata\n",
    "\n",
    "The obvious question in this case is \"will there always be a counts matrix,\" to which the answer is no. My train of thought is then to process FASTQ files.\n",
    "- As it happens, if I set up a pipeline to just do the analysis for FASTQ files, it would greatly simplify the insights that researchers can extract when getting new data? \n",
    "- Alternatively I could say \"start with a counts matrix,\" that could be a viable approach.\n",
    "\n",
    "Ah yeah - I will most likely be using R scripts to extract data. GEOquery worked well in my interactive test cases. \n",
    "\n",
    "Therefore, this notebook will be a Python script (where I call upon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8498eb7-4f14-429c-81c9-aa74c3390c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import instructor # I'm not sure if this will actually be needed...\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b383a139-6f9b-4926-9305-7ef604408c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pica pica\n"
     ]
    }
   ],
   "source": [
    "load_dotenv('../.env')\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Test OpenAI API...\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=openai_api_key,  # this is also the default, it can be omitted\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you state the name of a real species of bird? Only reply with the species name.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "result = chat_completion.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "605e4d07-3b52-4f55-9b05-e7f8186dfa53",
   "metadata": {},
   "source": [
    "Can't say I've heard of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448ec3f-e21f-4095-8990-58682dc9b48d",
   "metadata": {},
   "source": [
    "# Plan of action\n",
    "\n",
    "The ideal plan will be to:\n",
    "- Download FASTQ files\n",
    "- Download metadata\n",
    "\n",
    "With a bit of exploration, I have come to the realisation that some datasets will have both human and mouse samples (for example). Therefore, guiding the type of Kallisto analysis that I do will be important...\n",
    "\n",
    "What do I specifically need to do?\n",
    "- I do honestly think downloading the FASTQ files is straightforward:\n",
    "- Download metadata, then identify the SRR ID (or whatever it's called - EDIT - turns out this is not so straightforward. There's either a knowledge gap in me, or I will need an LLM here)\n",
    "- prefetch (SRR)\n",
    "- fasterq dump (x)\n",
    "- kallisto quant (x)\n",
    "\n",
    "In between I can use an LLM call to just say \"did this work\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853d42f2-a272-4cbe-8400-40339ce519b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R script output:\n",
      "Output directory exists: ../Testing/GSE273561 \n",
      "Retrieving metadata for GEO accession: GSE273561 \n",
      "Saving metadata to: ../Testing/GSE273561/GSE273561-GPL24247_series_matrix_metadata.csv \n",
      "Saving metadata to: ../Testing/GSE273561/GSE273561-GPL24676_series_matrix_metadata.csv \n",
      "Metadata saved successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the path to the R script\n",
    "r_script_path = \"./RScript_GetGEOMetadata.r\"  # Ensure the path is correct and script is executable\n",
    "\n",
    "# Define the GEO accession you want to use\n",
    "geo_accession = \"GSE273561\"  # Replace with the actual GEO accession number\n",
    "\n",
    "# Define output directory\n",
    "\n",
    "outdir = \"../Testing/GSE273561\"\n",
    "\n",
    "# Construct the command to run the R script with the GEO accession as an argument\n",
    "command = [\"Rscript\", r_script_path, \"-g\", geo_accession, \"-o\", outdir]\n",
    "\n",
    "# Call the R script using subprocess\n",
    "try:\n",
    "    result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "    print(\"R script output:\")\n",
    "    print(result.stdout)  # This will print the standard output from the R script\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error running R script:\")\n",
    "    print(e.stderr)  # This will print any error messages from the R script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fb18c-85a0-453d-9f81-6f6557d21a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry)",
   "language": "python",
   "name": "poetry-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
