{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e006268-09ab-406d-b50c-f5e987bc2a3a",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This notebook will contain ONLY the code to complete the end to end workflow. The idea is to make sure I can execute everything from start to finish, and to understand where the challenges will lie.\n",
    "\n",
    "I am also finding that a bit of the code with structured outputs in the past is now outdated. I will modernise it.\n",
    "\n",
    "I will mark some of the hardcoded stuff with [HARDCODED]. (Ooh, fancy formatting. I hope that doesn't break anything...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17b777-2d57-4cc5-9ae1-77a18b81c9d4",
   "metadata": {},
   "source": [
    "# Part 0 - Loading modules/.env/preparing OpenAI client/reusable agents\n",
    "\n",
    "The goal of this section is to load all the necessary modules, as well as prepare the OpenAI client.\n",
    "\n",
    "Hmmm... I suspect in the end I would want all the classes/functions/prompts defined somewhere...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dce14a6-ddea-48b8-ac11-343f85a12fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook SingleDatasetAnalysis.ipynb to script\n",
      "[NbConvertApp] Writing 53488 bytes to SingleDatasetAnalysis.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script SingleDatasetAnalysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0570b6a7-1c21-4cd7-98b9-5d9a87f52c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from Bio import Entrez\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Literal, Optional\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import Counter\n",
    "import statistics\n",
    "import subprocess\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from io import StringIO\n",
    "import tempfile\n",
    "import csv\n",
    "\n",
    "\n",
    "# Prepare .env file\n",
    "\n",
    "load_dotenv('../../../.env') # [HARDCODED]\n",
    "\n",
    "Entrez.email = os.getenv('ENTREZ_EMAIL')\n",
    "Entrez.api_key = os.getenv('ENTREZ_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Prepare OpenAI API Client\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=openai_api_key,  # this is also the default, it can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe32b77f-7371-40b2-a634-b11e5cf86473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare internal evaluation prompt\n",
    "\n",
    "# This prompt will take in information about the output of a step and the expected inputs of the next step to determine... how to proceed...?\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    Evaluation: bool = Field(description=\"If the output meets input requirements for the next step\")\n",
    "    Justification: str = Field(description=\"Three sentence justification for the evaluation decision\")\n",
    "\n",
    "def EvaluateOutput(previous_step_purpose: str,\n",
    "                  previous_step_output: str,\n",
    "                  next_step_purpose: str,\n",
    "                  next_step_input: str):\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "## IDENTITY AND PURPOSE\n",
    "\n",
    "You are an expert assistant for bioinformatic analyses, and help ensure that bioinformatic pipelines proceed without issue. \n",
    "\n",
    "To facilitate this, you consider the steps that were performed in an analysis, and evaluate if the output will allow the analysis to proceed in a valid manner.\n",
    "\n",
    "## STEPS\n",
    "\n",
    "You will be given the following information:\n",
    "- The purpose of a previous step\n",
    "- The output generated from the previous step\n",
    "- The purpose of the next step\n",
    "- The necessary/intended input for the next step\n",
    "\n",
    "Carefully take into consideration the following steps:\n",
    "1. Carefully digest the purpose of the previous step\n",
    "2. Carefully digest the purpose and input of the next step\n",
    "3. Using both of the above information, consider whether the output generated from the previous step would enable the analysis to proceed reliably. Consider specifically:\n",
    "- Does the output meet the expected input for the next step (e.g. is there missing data in any output data frames)?\n",
    "- Are there any error messages, which would indicate the output failed?\n",
    "- If there are warning messages, do these warrant concern, or can they be ignored?\n",
    "\n",
    "## OUTPUT\n",
    "\n",
    "1. State whether evaluation has passed\n",
    "2. Justify your decision in three setnences\n",
    "\n",
    "## INPUT\n",
    "Description of previous step: {previous_step_purpose}\n",
    "\n",
    "Output of previous step: {previous_step_output}\n",
    "\n",
    "Description of next step: {next_step_purpose}\n",
    "\n",
    "Input for next step: {next_step_input}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    evaluation_result = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=1,\n",
    "        response_format=Evaluation,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    evaluation_result = evaluation_result.choices[0].message.parsed           \n",
    "    return evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4be9add-a32a-4721-868e-61c48bff0959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(Evaluation=True, Justification='The output provides a list of Kallisto commands that are fully specified, including all necessary parameters such as the index, fastq file paths, output directory, bootstraps, and fragmentation settings. This means that the output from the previous step matches the expected input format for performing Kallisto quantification. Therefore, the analysis should be able to proceed without any issues.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = \"Determine appropriate Kallisto parameters\"\n",
    "p2 = kallisto_params\n",
    "p3 = \"Perform Kallisto quantification\"\n",
    "p4 = \"Parameters that can be used for Kallisto quantification\"\n",
    "\n",
    "EvaluateOutput(previous_step_purpose=p1,\n",
    "               previous_step_output=p2,\n",
    "               next_step_purpose=p3,\n",
    "               next_step_input=p4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65020cbb-eb3f-4546-a6cd-3e3a4b8dbf88",
   "metadata": {},
   "source": [
    "# Part 1 - Dataset identification\n",
    "\n",
    "The goal from this section is to identify the datasets that will be analysed. \n",
    "\n",
    "Keep in mind - for this notebook, I will only be analysing one dataset.\n",
    "\n",
    "## Specific approach\n",
    "- A user query is given. This is customizable, and will be what the user supplies themselves (i.e. the question they are interested in)\n",
    "- Based on this user query, appropriate search terms are identified using AI. Perhaps an option to specify how many iterations are performed?\n",
    "- Datasets are extracted from this. For the moment, I only extract the first 20 datasets (hardcoded). I suppose this could be a parameter?\n",
    "- Based on information from the extracted datasets, the relevance of the datasets to the research question is determined. This is performed three times. It'd be good to specify how many iterations are performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e676e6-6d8e-4a54-8224-ce06afcdc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the initial search queries\n",
    "\n",
    "# Define some initial variables for demonstration purposes [HARDCODED]\n",
    "\n",
    "user_query = \"Identify datasets and which are relevant to exploring immunotherapies for lung cancer\"\n",
    "num_queries = 3\n",
    "\n",
    "# Prepare functions for term extraction from research query\n",
    "\n",
    "# Prepare output structures\n",
    "## For term extraction from user query\n",
    "class ExtractedTerms(BaseModel):\n",
    "    extracted_terms: List[str] = Field(description=\"List of terms extracted from the query\")\n",
    "    expanded_terms: List[str] = Field(description=\"List of related terms generated from the extracted terms\")\n",
    "\n",
    "## For determining dataset relevance\n",
    "class Assessment(BaseModel):\n",
    "    ID: str\n",
    "    RelevanceScore: int = Field(description=\"Score from 0 to 10, indicating relevance\")\n",
    "    Justification: str = Field(description=\"A brief explanation for the score\")\n",
    "\n",
    "class Assessments(BaseModel):\n",
    "    assessments: List[Assessment]\n",
    "\n",
    "# Define function for term extraction \n",
    "def extract_terms(user_query: str) -> List[str]:\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "## IDENTITY AND PURPOSE\n",
    "You are an expert in literature searches of biological ideas. Your task is to identify biological term(s) from a query, and generate related terms for the purposes of generating a search query. \n",
    "\n",
    "## STEPS\n",
    "\n",
    "- First, extract the biological term(s) from the input query. These should be specific and fall into one of the following categories:\n",
    "1. Genes - Examples: BRCA1, TP53\n",
    "2. Treatments/Methods - Examples: chemotherapy, CRISPR\n",
    "3. Tissues/Cells - Examples: lung, hepatocytes\n",
    "4. Diseases - Examples: Alzheimer's disease, lung cancer.\n",
    "\n",
    "Do not fabricate items if no relevant term exists. Avoid general terms such as \"disease\" or \"variant.\"\n",
    "\n",
    "- Second, for each extracted biological term, generate two related terms. Make a considered effort to keep these terms in the same category as the original term. These are examples of an identified term, and possible relevant terms:\n",
    "1. Genes: BRCA1 - Examples: BRCA2, oncogene\n",
    "2. Treatments: Chemotherapy - Examples: radiotherapy, monoclonal antibody\n",
    "3. Tissues/Cells: Lung - Examples: respiratory, alveoli\n",
    "4. Diseases: Alzheimer's disease - Examples: dementia, amyloid plaque\n",
    "\n",
    "## OUTPUT\n",
    "\n",
    "Provide two lists:\n",
    "1. Extracted terms: The primary terms identified directly from the query.\n",
    "2. Expanded terms: The related terms generated from the extracted terms.\n",
    "Do not include categories or justifications.\n",
    "\n",
    "## INPUT\n",
    "User query: {user_query}\"\"\"\n",
    "    \n",
    "    extracted_terms = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=1,\n",
    "        response_format=ExtractedTerms,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    extracted_terms = extracted_terms.choices[0].message.parsed\n",
    "\n",
    "#    print(f\"Raw extracted terms: {extracted_terms.extracted_terms}\")\n",
    "#    print(f\"Raw expanded terms: {extracted_terms.expanded_terms}\")\n",
    "    \n",
    "    all_terms = extracted_terms.extracted_terms + extracted_terms.expanded_terms\n",
    "    terms_with_filter = [term + ' AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]' for term in all_terms]\n",
    "    return terms_with_filter\n",
    "\n",
    "# Extension - define function to perform term extraction multiple times\n",
    "async def extract_terms_multiple(user_query: str, num_queries: int = 3) -> List[str]:\n",
    "    async def single_extract():\n",
    "        return extract_terms(user_query)\n",
    "    \n",
    "    tasks = [single_extract() for _ in range(num_queries)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Flatten the list of lists and remove duplicates\n",
    "    all_terms = list(set([term for sublist in results for term in sublist]))\n",
    "    return all_terms\n",
    "\n",
    "# Define function for performing search\n",
    "def perform_search(term):\n",
    "    search_handle = Entrez.esearch(db=\"gds\", term=term, retmode=\"xml\", retmax = 50) # CHANGE\n",
    "    search_results = Entrez.read(search_handle)\n",
    "    search_handle.close()\n",
    "    return search_results\n",
    "\n",
    "# Define function for extracting information from above search results\n",
    "def extract_geo_info_batch(geo_ids):\n",
    "    \"\"\"\n",
    "    Retrieve GEO information for a batch of GEO IDs.\n",
    "    \"\"\"\n",
    "    ids_str = \",\".join(geo_ids)\n",
    "    handle = Entrez.esummary(db=\"gds\", id=ids_str, retmode=\"xml\")\n",
    "    output = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    data = []\n",
    "    for geo_id, geo_data in zip(geo_ids, output):\n",
    "        if isinstance(geo_data, dict):\n",
    "            data.append({\n",
    "                'ID': geo_id,\n",
    "                'Title': geo_data.get('title', 'No title available'),\n",
    "                'Summary': geo_data.get('summary', 'No summary available'),\n",
    "                'Accession': geo_data.get('Accession', 'No accession available'),\n",
    "                'Species': geo_data.get('taxon', 'No taxon available'),\n",
    "                'Date': geo_data.get('PDAT', 'Date made public unknown')\n",
    "            })\n",
    "        else:\n",
    "            data.append({'ID': geo_id, 'Title': 'Error', 'Summary': 'Unable to fetch data', 'Accession': 'Error'})\n",
    "\n",
    "    return data\n",
    "\n",
    "def create_geo_dataframe(geo_ids, batch_size=10):\n",
    "    \"\"\"Create a DataFrame from GEO search results using batch processing.\"\"\"\n",
    "    data = []\n",
    "    for i in tqdm(range(0, len(geo_ids), batch_size), desc=\"Processing GEO IDs in batches\"):\n",
    "        batch_ids = geo_ids[i:i + batch_size]\n",
    "        data.extend(extract_geo_info_batch(batch_ids))\n",
    "        time.sleep(0.2)  # Be nice to NCBI servers\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Define function for determining relevance of datasets\n",
    "def assess_relevance_batch(df, query, batch_size=10):\n",
    "    results = []\n",
    "    total_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Determining dataset relevance\", total=total_batches):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        prompt = f\"\"\"\n",
    "## IDENTITY AND PURPOSE\n",
    "\n",
    "You are a highly knowledgeable biologist tasked with identifying relevant datasets for a given research query. Your goal is to assess NCBI GEO datasets based on their titles and summaries, and determine their relevance to the research question at hand.\n",
    "\n",
    "## STEPS\n",
    "\n",
    "1. For each dataset, carefully analyze the provided title and summary.\n",
    "2. Extract ALL biological concepts represented in the dataset, including but not limited to:\n",
    "   - Genes and variants investigated (e.g., p53, BRCA1)\n",
    "   - Species studied (e.g., Homo sapiens, Escherichia coli)\n",
    "   - Sample sources (e.g., organoid cultures, human samples)\n",
    "   - Diseases or phenotypes studied (e.g., Alzheimer's disease, lung cancer)\n",
    "   - Cell types or tissues examined (e.g., lung tissue, neural progenitor cells)\n",
    "   - Experimental techniques or methodologies used (e.g., RNA-seq, ChIP-seq)\n",
    "3. Extract ALL biological concepts represented in the research query using the same categories.\n",
    "4. Assign a relevance score from 0 to 10 in increments of 1, based solely on the provided information. \n",
    "   - Do not fabricate or assume information not explicitly stated about the dataset. \n",
    "   - If confirmed information about the dataset is POSSIBLY useful for the research question, view this favourably for determining dataset relevance. \n",
    "   - Note that the gene, disease, and cell type/tissue being studied are the most important in determining relevance. The other factors are considered minor aspects.\n",
    "   - Use the following scoring guide:\n",
    "\n",
    "   0: No relevance. All biological concepts (genes, species, samples, diseases, cell types, methods) are completely unrelated to the research query.\n",
    "   1: Minimal relevance. One minor aspect is loosely related, but the overall focus is different.\n",
    "   2: Low relevance. One major aspect aligns with the query, but other key elements differ significantly.\n",
    "   3: Somewhat low relevance. Two aspects align, but critical elements are still mismatched.\n",
    "   4: Moderate relevance. Multiple aspects align, but there are still significant differences in focus or approach.\n",
    "   5: Moderately relevant. Most major aspects align, but there are some notable differences that may limit direct applicability.\n",
    "   6: Relevant. All major aspects align, but there might be differences in specific genes, cell types, or methodologies that somewhat reduce direct applicability.\n",
    "   7: Highly relevant. Very close alignment in all major aspects, with only minor differences that don't significantly impact applicability.\n",
    "   8: Very highly relevant. Near-perfect alignment in all major aspects, with at most one or two minor differences.\n",
    "   9: Extremely relevant. Perfect alignment in all major aspects, with at most one negligible difference.\n",
    "   10: Perfectly relevant. The dataset appears to be an exact match for the research query in all aspects.\n",
    "\n",
    "5. Provide a brief justification (3-4 sentences) for the assigned score, highlighting key similarities and differences.\n",
    "\n",
    "## OUTPUT\n",
    "For each dataset, provide a JSON object with the ID, relevance score, and justification. \n",
    "- The relevance score should be a number, with no other information.\n",
    "- The justification should be a 4-5 sentence explanation for the relevance score. \n",
    "\n",
    "## HANDLING LIMITED INFORMATION\n",
    "If the dataset title or summary lacks sufficient detail:\n",
    "- Focus on the information that is available\n",
    "- Do not make assumptions about missing information\n",
    "- Assign a lower score if critical information is absent\n",
    "- Note the lack of information in the justification\n",
    "\n",
    "Remember, it's better to assign a lower score due to lack of information than to assume relevance without evidence.\n",
    "\n",
    "Given the following datasets and query, determine if each dataset is relevant.\n",
    "        Query: {query}\n",
    "        Datasets:\n",
    "        \"\"\"\n",
    "        for _, row in batch.iterrows():\n",
    "            prompt += f\"\"\"\n",
    "            ID: {row['ID']}\n",
    "            Title: {row['Title']}\n",
    "            Summary: {row['Summary']}\n",
    "            Species: {row['Species']}\n",
    "            \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.beta.chat.completions.parse(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.3,\n",
    "                response_format=Assessments,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \n",
    "                     \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=16384\n",
    "            )\n",
    "            response = response.choices[0].message.parsed\n",
    "            results.extend([assessment.dict() for assessment in response.assessments])\n",
    "        except Exception as e:\n",
    "            results.extend([{\"ID\": row['ID'], \"Relevance\": \"Error\", \"Justification\": str(e)} for _, row in batch.iterrows()])\n",
    "        time.sleep(1)  # Be nice to the API\n",
    "    return results\n",
    "\n",
    "# Extension - define function to assess relevance multiple times\n",
    "async def assess_relevance_batch_multiple(df, query, num_queries: int = 2, batch_size=20):\n",
    "    async def single_assess():\n",
    "        return assess_relevance_batch(df, query, batch_size)\n",
    "    \n",
    "    tasks = [single_assess() for _ in range(num_queries)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Collate results\n",
    "    collated_results = {}\n",
    "    for i, result_set in enumerate(results):\n",
    "        for assessment in result_set:\n",
    "            id = assessment['ID']\n",
    "            if id not in collated_results:\n",
    "                collated_results[id] = {'scores': [], 'justifications': []}\n",
    "            collated_results[id]['scores'].append(assessment['RelevanceScore'])\n",
    "            collated_results[id]['justifications'].append(assessment['Justification'])\n",
    "    \n",
    "    # Determine final relevance and format output\n",
    "    final_results = []\n",
    "    for id, data in collated_results.items():\n",
    "        mean_score = statistics.mean(data['scores'])\n",
    "        \n",
    "        result = {\n",
    "            'ID': id,\n",
    "            'RelevanceScore': round(mean_score, 1),\n",
    "        }\n",
    "        \n",
    "        # Add individual scores and justifications\n",
    "        for i in range(num_queries):\n",
    "            result[f'IndividualScore{i+1}'] = data['scores'][i] if i < len(data['scores']) else None\n",
    "            result[f'Justification{i+1}'] = data['justifications'][i] if i < len(data['justifications']) else None\n",
    "        \n",
    "        final_results.append(result)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "async def main(user_query):\n",
    "    # Extract terms\n",
    "    search_terms = await extract_terms_multiple(user_query)\n",
    "    print(\"Search terms:\", search_terms)\n",
    "\n",
    "    # Perform Entrez search and remove duplicates\n",
    "    geo_ids = set()  # Use a set to automatically remove duplicates\n",
    "    for term in search_terms:\n",
    "        search_results = perform_search(term)\n",
    "        geo_ids.update(search_results.get('IdList', []))  # Update the set with new IDs\n",
    "    if not geo_ids:\n",
    "        return pd.DataFrame({'Error': [\"No results found for the extracted terms\"]})\n",
    "\n",
    "    # Convert set back to list\n",
    "    geo_ids = list(geo_ids)[1:20] # for the moment only use a subset of the IDs [HARDCODED]\n",
    "\n",
    "    # Create DataFrame with GEO information\n",
    "    df = create_geo_dataframe(geo_ids)\n",
    "\n",
    "    # Assess relevance\n",
    "    relevance_results = await assess_relevance_batch_multiple(df, user_query, num_queries=num_queries) # Currently have this at 3\n",
    "    relevance_df = pd.DataFrame(relevance_results)\n",
    "\n",
    "    # Merge results\n",
    "    df['ID'] = df['ID'].astype(str)\n",
    "    relevance_df['ID'] = relevance_df['ID'].astype(str)\n",
    "    result_df = df.merge(relevance_df, on='ID', how='left')\n",
    "\n",
    "    # Dynamically create the desired order of columns\n",
    "    base_columns = ['ID', 'Title', 'Summary', 'Species', 'Accession', 'Date', 'RelevanceScore']\n",
    "    score_columns = [f'IndividualScore{i+1}' for i in range(num_queries)]\n",
    "    justification_columns = [f'Justification{i+1}' for i in range(num_queries)]\n",
    "    desired_order = base_columns + score_columns + justification_columns\n",
    "\n",
    "    # Reorder columns\n",
    "    result_df = result_df[desired_order]\n",
    "\n",
    "    # Reset index\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa9bc86-052f-4473-a0d3-397d52c300ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search terms: ['lung metastasis AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]', 'CAR T-cell therapy AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]', 'small cell lung cancer AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]', 'checkpoint inhibitors AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]', 'immunotherapies AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]', 'monoclonal antibodies AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]', 'non-small cell lung cancer AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]', 'lung cancer AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]', 'immune checkpoint inhibitors AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]', 'immunotherapy AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]', 'targeted therapy AND \"gse\"[Filter] AND \"expression profiling by high throughput sequencing\"[DataSet Type] AND \"Homo sapiens\"[Organism]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GEO IDs in batches: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]\n",
      "Determining dataset relevance: 100%|██████████| 1/1 [00:10<00:00, 10.80s/it]\n",
      "Determining dataset relevance: 100%|██████████| 1/1 [00:11<00:00, 11.97s/it]\n",
      "Determining dataset relevance: 100%|██████████| 1/1 [00:11<00:00, 11.83s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Species</th>\n",
       "      <th>Accession</th>\n",
       "      <th>Date</th>\n",
       "      <th>RelevanceScore</th>\n",
       "      <th>IndividualScore1</th>\n",
       "      <th>IndividualScore2</th>\n",
       "      <th>IndividualScore3</th>\n",
       "      <th>Justification1</th>\n",
       "      <th>Justification2</th>\n",
       "      <th>Justification3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200237876</td>\n",
       "      <td>Deciphering metastatic route-specific signals ...</td>\n",
       "      <td>Gastric cancer (GC) constitutes a significant ...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE237876</td>\n",
       "      <td>2024/07/01</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The dataset focuses on gastric cancer and its ...</td>\n",
       "      <td>This dataset focuses on gastric cancer and its...</td>\n",
       "      <td>This dataset focuses on gastric cancer and its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200216688</td>\n",
       "      <td>Identification of a transcription factor netwo...</td>\n",
       "      <td>CD4+ T-cells are key players in the pathogenes...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE216688</td>\n",
       "      <td>2024/09/25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This dataset investigates CD4+ T-cells and the...</td>\n",
       "      <td>This dataset investigates the role of CD4+ T-c...</td>\n",
       "      <td>While this dataset investigates CD4+ T-cells a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200272188</td>\n",
       "      <td>Single-cell transcriptomics reveals subset-spe...</td>\n",
       "      <td>Airway epithelial cells represent the first li...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE272188</td>\n",
       "      <td>2024/09/18</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>The dataset studies bronchial epithelial cells...</td>\n",
       "      <td>This dataset explores the bronchial epithelial...</td>\n",
       "      <td>This dataset explores the bronchial epithelial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200226967</td>\n",
       "      <td>Transcriptomic and metabolic effect of immune ...</td>\n",
       "      <td>Tumor microenvironmental cellular components l...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE226967</td>\n",
       "      <td>2024/03/07</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>This dataset examines immune checkpoint inhibi...</td>\n",
       "      <td>This dataset examines the effects of immune ch...</td>\n",
       "      <td>This dataset discusses immune checkpoint inhib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200159801</td>\n",
       "      <td>MUC1-C IS A MASTER REGULATOR OF SMALL CELL LUN...</td>\n",
       "      <td>Small cell lung cancer (SCLC) is a recalcitran...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE159801</td>\n",
       "      <td>2023/01/12</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>This dataset is highly relevant as it focuses ...</td>\n",
       "      <td>This dataset is highly relevant as it focuses ...</td>\n",
       "      <td>This dataset is highly relevant as it focuses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200230058</td>\n",
       "      <td>Eosinophils exhibit a unique transcriptional s...</td>\n",
       "      <td>Identifying mechanisms by which eosinophils in...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE230058</td>\n",
       "      <td>2024/09/29</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>The dataset discusses eosinophils in the conte...</td>\n",
       "      <td>This dataset investigates eosinophils in color...</td>\n",
       "      <td>This dataset examines eosinophils in colorecta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200191007</td>\n",
       "      <td>[RNA-seq] Transcriptional regulation of CD19 a...</td>\n",
       "      <td>Recent improvements in relapse/refractory B-ce...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE191007</td>\n",
       "      <td>2022/07/27</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This dataset focuses on CD19 in B-cell maligna...</td>\n",
       "      <td>This dataset focuses on B-cell malignancies an...</td>\n",
       "      <td>This dataset focuses on B-cell malignancies an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200279187</td>\n",
       "      <td>TEAL-seq (Targeted Expression Analysis Sequenc...</td>\n",
       "      <td>Metagenome sequencing enables discovery and ge...</td>\n",
       "      <td>Staphylococcus epidermidis; Homo sapiens; Esch...</td>\n",
       "      <td>GSE279187</td>\n",
       "      <td>2024/10/18</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This dataset focuses on Staphylococcus species...</td>\n",
       "      <td>This dataset deals with microbial communities ...</td>\n",
       "      <td>This dataset is focused on microbial communiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200249362</td>\n",
       "      <td>Non-Canonical BAF and mSWI/SNF Regulates POU2F...</td>\n",
       "      <td>~12% of SCLCs are marked by the lineage transc...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE249362</td>\n",
       "      <td>2024/06/14</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>This dataset discusses POU2F3-positive small c...</td>\n",
       "      <td>This dataset examines POU2F3-positive small ce...</td>\n",
       "      <td>This dataset discusses POU2F3-positive small c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200246219</td>\n",
       "      <td>Cross-tissue human fibroblast atlas reveals my...</td>\n",
       "      <td>This SuperSeries is composed of the SubSeries ...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE246219</td>\n",
       "      <td>2024/09/19</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>This dataset focuses on fibroblast subtypes an...</td>\n",
       "      <td>This dataset explores fibroblast subtypes and ...</td>\n",
       "      <td>This dataset provides insights into fibroblast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200224193</td>\n",
       "      <td>Type I Interferon Signaling via the EGR2 Trans...</td>\n",
       "      <td>Chimeric antigen receptor (CAR) T-cell therapy...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE224193</td>\n",
       "      <td>2023/02/20</td>\n",
       "      <td>8.7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>This dataset investigates CAR T-cell therapy a...</td>\n",
       "      <td>This dataset discusses CAR T-cell therapy and ...</td>\n",
       "      <td>This dataset is highly relevant as it discusse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200255145</td>\n",
       "      <td>Gene expression profiles of CAR-T cells engine...</td>\n",
       "      <td>The efficacy of chimeric antigen receptor (CAR...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE255145</td>\n",
       "      <td>2024/03/24</td>\n",
       "      <td>8.7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>This dataset focuses on CAR-T cells engineered...</td>\n",
       "      <td>This dataset focuses on enhancing CAR T-cell t...</td>\n",
       "      <td>This dataset focuses on CAR T-cell therapy and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200247471</td>\n",
       "      <td>Analysis of the transcriptomic profiles for HE...</td>\n",
       "      <td>T cell exhaustion and metabolic dysfunction in...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE247471</td>\n",
       "      <td>2023/11/21</td>\n",
       "      <td>8.3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>This dataset discusses strategies to enhance C...</td>\n",
       "      <td>This dataset investigates CAR T-cell exhaustio...</td>\n",
       "      <td>This dataset discusses CAR T-cell therapy and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200267928</td>\n",
       "      <td>Teriflunomide/leflunomide synergize with chemo...</td>\n",
       "      <td>Although up to 80% small-cell lung cancer (SCL...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE267928</td>\n",
       "      <td>2024/05/22</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>This dataset investigates the effects of treat...</td>\n",
       "      <td>This dataset examines therapeutic strategies f...</td>\n",
       "      <td>This dataset investigates the effects of terif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200271926</td>\n",
       "      <td>Evaluation of the Mammalian Aquaporin Inhibito...</td>\n",
       "      <td>AQPs contribute to breast cancer progression a...</td>\n",
       "      <td>Homo sapiens; Mus musculus</td>\n",
       "      <td>GSE271926</td>\n",
       "      <td>2024/08/01</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This dataset focuses on breast cancer and the ...</td>\n",
       "      <td>This dataset focuses on breast cancer and the ...</td>\n",
       "      <td>This dataset focuses on breast cancer and aqua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200251979</td>\n",
       "      <td>Endothelial Pim3 kinase protects the vascular ...</td>\n",
       "      <td>To investigate transcriptome changes of endoth...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE251979</td>\n",
       "      <td>2024/09/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>The dataset investigates endothelial cells in ...</td>\n",
       "      <td>This dataset investigates endothelial cell res...</td>\n",
       "      <td>This dataset investigates endothelial response...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200253872</td>\n",
       "      <td>Identification of a clinically efficacious CAR...</td>\n",
       "      <td>Adoptive immunotherapy with T cells expressing...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE253872</td>\n",
       "      <td>2024/02/29</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>This dataset focuses on CAR T-cell subsets in ...</td>\n",
       "      <td>This dataset focuses on CAR T-cell therapy for...</td>\n",
       "      <td>This dataset explores CAR T-cell subsets in B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200276387</td>\n",
       "      <td>Modeling Lung Adenocarcinoma Metastases Using ...</td>\n",
       "      <td>Approximately 50% of patients with surgically ...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE276387</td>\n",
       "      <td>2024/09/12</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>This dataset models lung adenocarcinoma metast...</td>\n",
       "      <td>This dataset is directly focused on lung adeno...</td>\n",
       "      <td>This dataset is perfectly relevant as it model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>200246342</td>\n",
       "      <td>CD4+ CAR-T cell exhaustion associated with ear...</td>\n",
       "      <td>Multiple myeloma is characterized by frequent ...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE246342</td>\n",
       "      <td>2024/07/29</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>This dataset discusses CAR-T cell therapy in m...</td>\n",
       "      <td>This dataset examines CD4+ CAR-T cell exhausti...</td>\n",
       "      <td>This dataset discusses CAR-T cell exhaustion i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                              Title  \\\n",
       "0   200237876  Deciphering metastatic route-specific signals ...   \n",
       "1   200216688  Identification of a transcription factor netwo...   \n",
       "2   200272188  Single-cell transcriptomics reveals subset-spe...   \n",
       "3   200226967  Transcriptomic and metabolic effect of immune ...   \n",
       "4   200159801  MUC1-C IS A MASTER REGULATOR OF SMALL CELL LUN...   \n",
       "5   200230058  Eosinophils exhibit a unique transcriptional s...   \n",
       "6   200191007  [RNA-seq] Transcriptional regulation of CD19 a...   \n",
       "7   200279187  TEAL-seq (Targeted Expression Analysis Sequenc...   \n",
       "8   200249362  Non-Canonical BAF and mSWI/SNF Regulates POU2F...   \n",
       "9   200246219  Cross-tissue human fibroblast atlas reveals my...   \n",
       "10  200224193  Type I Interferon Signaling via the EGR2 Trans...   \n",
       "11  200255145  Gene expression profiles of CAR-T cells engine...   \n",
       "12  200247471  Analysis of the transcriptomic profiles for HE...   \n",
       "13  200267928  Teriflunomide/leflunomide synergize with chemo...   \n",
       "14  200271926  Evaluation of the Mammalian Aquaporin Inhibito...   \n",
       "15  200251979  Endothelial Pim3 kinase protects the vascular ...   \n",
       "16  200253872  Identification of a clinically efficacious CAR...   \n",
       "17  200276387  Modeling Lung Adenocarcinoma Metastases Using ...   \n",
       "18  200246342  CD4+ CAR-T cell exhaustion associated with ear...   \n",
       "\n",
       "                                              Summary  \\\n",
       "0   Gastric cancer (GC) constitutes a significant ...   \n",
       "1   CD4+ T-cells are key players in the pathogenes...   \n",
       "2   Airway epithelial cells represent the first li...   \n",
       "3   Tumor microenvironmental cellular components l...   \n",
       "4   Small cell lung cancer (SCLC) is a recalcitran...   \n",
       "5   Identifying mechanisms by which eosinophils in...   \n",
       "6   Recent improvements in relapse/refractory B-ce...   \n",
       "7   Metagenome sequencing enables discovery and ge...   \n",
       "8   ~12% of SCLCs are marked by the lineage transc...   \n",
       "9   This SuperSeries is composed of the SubSeries ...   \n",
       "10  Chimeric antigen receptor (CAR) T-cell therapy...   \n",
       "11  The efficacy of chimeric antigen receptor (CAR...   \n",
       "12  T cell exhaustion and metabolic dysfunction in...   \n",
       "13  Although up to 80% small-cell lung cancer (SCL...   \n",
       "14  AQPs contribute to breast cancer progression a...   \n",
       "15  To investigate transcriptome changes of endoth...   \n",
       "16  Adoptive immunotherapy with T cells expressing...   \n",
       "17  Approximately 50% of patients with surgically ...   \n",
       "18  Multiple myeloma is characterized by frequent ...   \n",
       "\n",
       "                                              Species  Accession        Date  \\\n",
       "0                                        Homo sapiens  GSE237876  2024/07/01   \n",
       "1                                        Homo sapiens  GSE216688  2024/09/25   \n",
       "2                                        Homo sapiens  GSE272188  2024/09/18   \n",
       "3                                        Homo sapiens  GSE226967  2024/03/07   \n",
       "4                                        Homo sapiens  GSE159801  2023/01/12   \n",
       "5                                        Homo sapiens  GSE230058  2024/09/29   \n",
       "6                                        Homo sapiens  GSE191007  2022/07/27   \n",
       "7   Staphylococcus epidermidis; Homo sapiens; Esch...  GSE279187  2024/10/18   \n",
       "8                                        Homo sapiens  GSE249362  2024/06/14   \n",
       "9                                        Homo sapiens  GSE246219  2024/09/19   \n",
       "10                                       Homo sapiens  GSE224193  2023/02/20   \n",
       "11                                       Homo sapiens  GSE255145  2024/03/24   \n",
       "12                                       Homo sapiens  GSE247471  2023/11/21   \n",
       "13                                       Homo sapiens  GSE267928  2024/05/22   \n",
       "14                         Homo sapiens; Mus musculus  GSE271926  2024/08/01   \n",
       "15                                       Homo sapiens  GSE251979  2024/09/11   \n",
       "16                                       Homo sapiens  GSE253872  2024/02/29   \n",
       "17                                       Homo sapiens  GSE276387  2024/09/12   \n",
       "18                                       Homo sapiens  GSE246342  2024/07/29   \n",
       "\n",
       "    RelevanceScore  IndividualScore1  IndividualScore2  IndividualScore3  \\\n",
       "0              0.3                 0                 1                 0   \n",
       "1              2.0                 2                 2                 2   \n",
       "2              3.0                 3                 3                 3   \n",
       "3              4.3                 5                 4                 4   \n",
       "4              9.3                10                 9                 9   \n",
       "5              2.0                 2                 2                 2   \n",
       "6              2.3                 3                 2                 2   \n",
       "7              0.3                 0                 1                 0   \n",
       "8              8.0                10                 6                 8   \n",
       "9              2.3                 0                 3                 4   \n",
       "10             8.7                10                 8                 8   \n",
       "11             8.7                10                 8                 8   \n",
       "12             8.3                10                 7                 8   \n",
       "13             9.3                10                 9                 9   \n",
       "14             0.3                 0                 1                 0   \n",
       "15             1.0                 0                 3                 0   \n",
       "16             2.7                 0                 4                 4   \n",
       "17            10.0                10                10                10   \n",
       "18             3.7                 2                 5                 4   \n",
       "\n",
       "                                       Justification1  \\\n",
       "0   The dataset focuses on gastric cancer and its ...   \n",
       "1   This dataset investigates CD4+ T-cells and the...   \n",
       "2   The dataset studies bronchial epithelial cells...   \n",
       "3   This dataset examines immune checkpoint inhibi...   \n",
       "4   This dataset is highly relevant as it focuses ...   \n",
       "5   The dataset discusses eosinophils in the conte...   \n",
       "6   This dataset focuses on CD19 in B-cell maligna...   \n",
       "7   This dataset focuses on Staphylococcus species...   \n",
       "8   This dataset discusses POU2F3-positive small c...   \n",
       "9   This dataset focuses on fibroblast subtypes an...   \n",
       "10  This dataset investigates CAR T-cell therapy a...   \n",
       "11  This dataset focuses on CAR-T cells engineered...   \n",
       "12  This dataset discusses strategies to enhance C...   \n",
       "13  This dataset investigates the effects of treat...   \n",
       "14  This dataset focuses on breast cancer and the ...   \n",
       "15  The dataset investigates endothelial cells in ...   \n",
       "16  This dataset focuses on CAR T-cell subsets in ...   \n",
       "17  This dataset models lung adenocarcinoma metast...   \n",
       "18  This dataset discusses CAR-T cell therapy in m...   \n",
       "\n",
       "                                       Justification2  \\\n",
       "0   This dataset focuses on gastric cancer and its...   \n",
       "1   This dataset investigates the role of CD4+ T-c...   \n",
       "2   This dataset explores the bronchial epithelial...   \n",
       "3   This dataset examines the effects of immune ch...   \n",
       "4   This dataset is highly relevant as it focuses ...   \n",
       "5   This dataset investigates eosinophils in color...   \n",
       "6   This dataset focuses on B-cell malignancies an...   \n",
       "7   This dataset deals with microbial communities ...   \n",
       "8   This dataset examines POU2F3-positive small ce...   \n",
       "9   This dataset explores fibroblast subtypes and ...   \n",
       "10  This dataset discusses CAR T-cell therapy and ...   \n",
       "11  This dataset focuses on enhancing CAR T-cell t...   \n",
       "12  This dataset investigates CAR T-cell exhaustio...   \n",
       "13  This dataset examines therapeutic strategies f...   \n",
       "14  This dataset focuses on breast cancer and the ...   \n",
       "15  This dataset investigates endothelial cell res...   \n",
       "16  This dataset focuses on CAR T-cell therapy for...   \n",
       "17  This dataset is directly focused on lung adeno...   \n",
       "18  This dataset examines CD4+ CAR-T cell exhausti...   \n",
       "\n",
       "                                       Justification3  \n",
       "0   This dataset focuses on gastric cancer and its...  \n",
       "1   While this dataset investigates CD4+ T-cells a...  \n",
       "2   This dataset explores the bronchial epithelial...  \n",
       "3   This dataset discusses immune checkpoint inhib...  \n",
       "4   This dataset is highly relevant as it focuses ...  \n",
       "5   This dataset examines eosinophils in colorecta...  \n",
       "6   This dataset focuses on B-cell malignancies an...  \n",
       "7   This dataset is focused on microbial communiti...  \n",
       "8   This dataset discusses POU2F3-positive small c...  \n",
       "9   This dataset provides insights into fibroblast...  \n",
       "10  This dataset is highly relevant as it discusse...  \n",
       "11  This dataset focuses on CAR T-cell therapy and...  \n",
       "12  This dataset discusses CAR T-cell therapy and ...  \n",
       "13  This dataset investigates the effects of terif...  \n",
       "14  This dataset focuses on breast cancer and aqua...  \n",
       "15  This dataset investigates endothelial response...  \n",
       "16  This dataset explores CAR T-cell subsets in B-...  \n",
       "17  This dataset is perfectly relevant as it model...  \n",
       "18  This dataset discusses CAR-T cell exhaustion i...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View results\n",
    "\n",
    "dataset_relevance_df = await (main(user_query))\n",
    "dataset_relevance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a5cc8-6f17-4fdd-b5c0-bc0e3950875e",
   "metadata": {},
   "source": [
    "# Part 2 - Data extraction\n",
    "\n",
    "In this part, relevant data is extracted out of the NCBI GEO datasets.\n",
    "\n",
    "For this notebook, I will extract data only out of the best scoring dataset (i.e. highest relevance).\n",
    "\n",
    "This part relies on scripts that I generated in a different notebook, so I will make sure to call those.\n",
    "\n",
    "## Specific approach:\n",
    "- Determine appropriate input parameters for script (i.e. dataset ID, output directory name, number of \"spots\"/reads)\n",
    "- Download the metadata associated with the dataset ID (which is a NCBI GEO ID)\n",
    "- Download FASTQ files associated with the dataset ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf48b09-fcef-4569-8c76-a7a2529970b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-18 05:28:09 - Processing GEO accession: GSE276387\n",
      "2024-11-18 05:28:09 - Output directory: GSE276387_data\n",
      "2024-11-18 05:28:09 - Force overwrite is enabled.\n",
      "2024-11-18 05:28:09 - Number of spots to download: 80000\n",
      "2024-11-18 05:28:09 - Starting metadata download using download_metadata.R\n",
      "2024-11-18 05:28:09 - Executing: Rscript \"/home/myuser/work/notebooks/Clean_Notebooks/SingleDatasetAnalysis/OtherScripts/download_metadata.R\" --geo_accession 'GSE276387' --output_dir 'GSE276387_data'\n",
      "Using existing output directory: GSE276387_data\n",
      "Retrieving metadata for GEO accession: GSE276387\n",
      "Saving metadata to: GSE276387_data/GSE276387-GPL16791_series_matrix_metadata.csv\n",
      "Saving metadata to: GSE276387_data/GSE276387-GPL24676_series_matrix_metadata.csv\n",
      "Metadata downloaded and saved successfully!\n",
      "2024-11-18 05:28:38 - Metadata download completed.\n",
      "2024-11-18 05:28:38 - Starting FASTQ download using download_fastqs.sh\n",
      "2024-11-18 05:28:38 - Executing: \"/home/myuser/work/notebooks/Clean_Notebooks/SingleDatasetAnalysis/OtherScripts/download_fastqs.sh\" --geo_accession 'GSE276387' --output_dir 'GSE276387_data' --num_spots 80000 --force\n",
      "Read 80000 spots for SRR30552073\n",
      "Written 80000 spots for SRR30552073\n",
      "Read 80000 spots for SRR30552074\n",
      "Written 80000 spots for SRR30552074\n",
      "Read 80000 spots for SRR30552075\n",
      "Written 80000 spots for SRR30552075\n",
      "Read 80000 spots for SRR30552076\n",
      "Written 80000 spots for SRR30552076\n",
      "Read 80000 spots for SRR30552077\n",
      "Written 80000 spots for SRR30552077\n",
      "Read 80000 spots for SRR30552078\n",
      "Written 80000 spots for SRR30552078\n",
      "Read 80000 spots for SRR30552079\n",
      "Written 80000 spots for SRR30552079\n",
      "Read 80000 spots for SRR30552080\n",
      "Written 80000 spots for SRR30552080\n",
      "Read 80000 spots for SRR30552081\n",
      "Written 80000 spots for SRR30552081\n",
      "Read 80000 spots for SRR30552082\n",
      "Written 80000 spots for SRR30552082\n",
      "Read 80000 spots for SRR30552083\n",
      "Written 80000 spots for SRR30552083\n",
      "Read 80000 spots for SRR30552084\n",
      "Written 80000 spots for SRR30552084\n",
      "Read 80000 spots for SRR30552085\n",
      "Written 80000 spots for SRR30552085\n",
      "Read 80000 spots for SRR30552086\n",
      "Written 80000 spots for SRR30552086\n",
      "Read 80000 spots for SRR30552087\n",
      "Written 80000 spots for SRR30552087\n",
      "Read 80000 spots for SRR30552088\n",
      "Written 80000 spots for SRR30552088\n",
      "Read 80000 spots for SRR30552089\n",
      "Written 80000 spots for SRR30552089\n",
      "Read 80000 spots for SRR30552090\n",
      "Written 80000 spots for SRR30552090\n",
      "Read 80000 spots for SRR30552091\n",
      "Written 80000 spots for SRR30552091\n",
      "Read 80000 spots for SRR30552092\n",
      "Written 80000 spots for SRR30552092\n",
      "Read 80000 spots for SRR30552093\n",
      "Written 80000 spots for SRR30552093\n",
      "Read 80000 spots for SRR30552094\n",
      "Written 80000 spots for SRR30552094\n",
      "Read 80000 spots for SRR30552095\n",
      "Written 80000 spots for SRR30552095\n",
      "Read 80000 spots for SRR30552096\n",
      "Written 80000 spots for SRR30552096\n",
      "Read 80000 spots for SRR30552097\n",
      "Written 80000 spots for SRR30552097\n",
      "Read 80000 spots for SRR30552098\n",
      "Written 80000 spots for SRR30552098\n",
      "2024-11-18 05:57:32 - FASTQ download completed.\n",
      "2024-11-18 05:57:32 - All processing completed successfully for GEO accession: GSE276387\n"
     ]
    }
   ],
   "source": [
    "# [HARDCODED] For the moment, we will begin by determining the single dataset that we should analyse\n",
    "\n",
    "top_accession = dataset_relevance_df.sort_values(by=\"RelevanceScore\", ascending=False).iloc[0][\"Accession\"]\n",
    "# top_accession = \"GSE279637\" #[HARDCODED] while I develop evaluation mechanisms\n",
    "\n",
    "# We will then use this to determine input parameters. I think I am happy leaving these hardcoded.\n",
    "output_dir_name = top_accession + \"_data\"\n",
    "\n",
    "n_spots = 80000 # [HARDCODED]\n",
    "\n",
    "script_dir = \"OtherScripts\"  # Adjust if necessary\n",
    "\n",
    "# Construct the path to the process_geo.sh script\n",
    "script_path = os.path.join(script_dir, \"process_geo.sh\")\n",
    "\n",
    "# Run the subprocess\n",
    "result = subprocess.run([\n",
    "    script_path,\n",
    "    \"--geo_accession\", top_accession,\n",
    "    \"--output_dir\", output_dir_name,\n",
    "    \"--num_spots\", str(n_spots),\n",
    "    \"--force\"\n",
    "], check=True)  # check=True will raise an exception if the subprocess fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef89e41-f50d-47a8-bfbe-9c7540bc2685",
   "metadata": {},
   "source": [
    "Post performance notes:\n",
    "- There was a period of time where fastq-dump would not work (network issues - I could not isolate whether it was an issue on NCBI's end or my end). However, in other time periods it seems to work very well.\n",
    "- I will be using the sra_ids.txt file to link the various IDs and whatnot. This is necessary for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea91de-c0a6-41c2-9ee5-283565ce2ea4",
   "metadata": {},
   "source": [
    "# Part 3 - Data analysis\n",
    "\n",
    "Now that the data has been extracted, we will now want to perform the analysis. \n",
    "\n",
    "The specifics steps involved here are:\n",
    "## (3.1 - Kallisto quantification)\n",
    "- View the documentation\n",
    "- Identify the file locations (FASTQ files and Kallisto index files). In my original iteration, I had the file locations hardcoded, so I do need to determine how to resolve this... my vision was the have the AI determine appropriate values (that way I can rely on just a single function... (Hm, I think I can get away with the FASTQ files by specifying the output directory as defined earlier, but I don't have the same luxury for the index files. Perhaps I just search from the home directory...)\n",
    "- Identify sample metadata. I believe this is the SRA metadata... I need to iron this out, because this might be a duplicate (and perhaps it might be more suitable for the previous part...)\n",
    "- Get the study summary\n",
    "- Use the above information to determine the appropriate Kallisto parameters\n",
    "- Using these determined parameters, perform the Kallisto quantification\n",
    "\n",
    "## 3.2 - DEG analysis\n",
    "- Read in sample metadata (as extracted from before)\n",
    "- Identify location of abundance files\n",
    "- Determine appropriate contrasts from metadata, and structure this appropriately (i.e. compatible with makeContrasts)\n",
    "- Perform the DEG analysis, with the input files/contrasts\n",
    "\n",
    "Later - I would need an evaluation mechanism for.. pretty much every step. I'm hoping I can simply develop something which goes \"hey check these steps\" and can be flexible beyond that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abdff8dc-f6e8-4637-955b-1b63f7f931a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by getting the documentation. This is necessary to ensure the OpenAI API knows the versions etc. that are being dealt with.\n",
    "\n",
    "def get_documentation(command):\n",
    "    try:\n",
    "        # Execute the kallisto command\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        \n",
    "        # Capture the stdout\n",
    "        stdout = result.stdout\n",
    "        \n",
    "        # Return the results\n",
    "        return stdout\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "kallisto_docs = get_documentation(\"kallisto quant --help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c48dec4-fc55-4ecb-8ae8-afc62b4c68a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key detected.\n",
      "\n",
      "Processing SRA ID: SRR30552073\n",
      "Executing command: esearch -db sra -query SRR30552073 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552073.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552074\n",
      "Executing command: esearch -db sra -query SRR30552074 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552074.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552075\n",
      "Executing command: esearch -db sra -query SRR30552075 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552075.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552076\n",
      "Executing command: esearch -db sra -query SRR30552076 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552076.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552077\n",
      "Executing command: esearch -db sra -query SRR30552077 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552077.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552078\n",
      "Executing command: esearch -db sra -query SRR30552078 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552078.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552079\n",
      "Executing command: esearch -db sra -query SRR30552079 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552079.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552080\n",
      "Executing command: esearch -db sra -query SRR30552080 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552080.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552081\n",
      "Executing command: esearch -db sra -query SRR30552081 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552081.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552082\n",
      "Executing command: esearch -db sra -query SRR30552082 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552082.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552083\n",
      "Executing command: esearch -db sra -query SRR30552083 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552083.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552084\n",
      "Executing command: esearch -db sra -query SRR30552084 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552084.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552085\n",
      "Executing command: esearch -db sra -query SRR30552085 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552085.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552086\n",
      "Executing command: esearch -db sra -query SRR30552086 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552086.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552087\n",
      "Executing command: esearch -db sra -query SRR30552087 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552087.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552088\n",
      "Executing command: esearch -db sra -query SRR30552088 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552088.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552089\n",
      "Executing command: esearch -db sra -query SRR30552089 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552089.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552090\n",
      "Executing command: esearch -db sra -query SRR30552090 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552090.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552091\n",
      "Executing command: esearch -db sra -query SRR30552091 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552091.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552092\n",
      "Executing command: esearch -db sra -query SRR30552092 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552092.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552093\n",
      "Executing command: esearch -db sra -query SRR30552093 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552093.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552094\n",
      "Executing command: esearch -db sra -query SRR30552094 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552094.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552095\n",
      "Executing command: esearch -db sra -query SRR30552095 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552095.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552096\n",
      "Executing command: esearch -db sra -query SRR30552096 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552096.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552097\n",
      "Executing command: esearch -db sra -query SRR30552097 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552097.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR30552098\n",
      "Executing command: esearch -db sra -query SRR30552098 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR30552098.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Data fetching complete.\n"
     ]
    }
   ],
   "source": [
    "# Now determine the file locations. This gets me the FASTQ files, but is also necessary to get the SRA IDs, which I use to extract the SRA metadata.\n",
    "\n",
    "def list_files(directory, suffix, exclude_hidden=True):\n",
    "    \"\"\"\n",
    "    Recursively lists all files in a given directory and its subdirectories that end with the specified suffix,\n",
    "    optionally excluding hidden files and directories, returning their absolute paths.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): The path to the directory to search in.\n",
    "    suffix (str): The file suffix to look for (e.g., 'fastq.gz').\n",
    "    exclude_hidden (bool): If True, hidden files and directories are excluded. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of absolute file paths that match the given suffix.\n",
    "    \"\"\"\n",
    "    matched_files = []\n",
    "    \n",
    "    try:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            if exclude_hidden:\n",
    "                # Skip hidden directories\n",
    "                dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "            \n",
    "            for f in files:\n",
    "                if exclude_hidden and f.startswith('.'):\n",
    "                    continue\n",
    "                if f.endswith(suffix):\n",
    "                    matched_files.append(os.path.join(root, f))\n",
    "                    \n",
    "        return matched_files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory '{directory}' not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "# First extract the FASTQ files. I can use the directory defined above to get these.\n",
    "\n",
    "# output_dir_name = top_accession + \"_data\" This is how I defined this earlier\n",
    "\n",
    "fastq_directory = output_dir_name\n",
    "fastq_suffix = \".fastq.gz\" # [HARDCODED] Hoping that I can automate this\n",
    "fastq_files = list_files(fastq_directory, fastq_suffix)\n",
    "\n",
    "# Next is the Kallisto indices...\n",
    "\n",
    "index_directory = \"/home/myuser/work/\" # [HARDCODED] .. but maybe this is ok...\n",
    "index_suffix = \".idx\" # [HARDCODED]\n",
    "index_files = list_files(index_directory, index_suffix)\n",
    "\n",
    "# Now we extract the study summary\n",
    "\n",
    "def get_study_summary(accession):\n",
    "\n",
    "    # Define the command as a string\n",
    "    command = (\n",
    "        f'esearch -db gds -query \"{accession}[ACCN]\" | '\n",
    "        'efetch -format docsum | '\n",
    "        'xtract -pattern DocumentSummarySet -block DocumentSummary '\n",
    "        f'-if Accession -equals {accession} -element summary'\n",
    "    )\n",
    "\n",
    "    # Execute the command\n",
    "    result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Check if the command was successful\n",
    "    if result.returncode == 0:\n",
    "        # Return the output\n",
    "        return result.stdout.strip()\n",
    "    else:\n",
    "        # Raise an error with the stderr output\n",
    "        raise Exception(f\"Error: {result.stderr}\")\n",
    "\n",
    "study_summary = get_study_summary(top_accession)\n",
    "\n",
    "# Now extract the SRA sample metadata...\n",
    "\n",
    "def fetch_sra_metadata_shell(\n",
    "    sra_ids_file,\n",
    "    entrez_api_key=None,\n",
    "    delay=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetches metadata for a list of SRA IDs using NCBI's esearch and efetch command-line tools.\n",
    "\n",
    "    Parameters:\n",
    "    - sra_ids_file (str): Path to the file containing SRA IDs with headers (tab-separated).\n",
    "    - entrez_api_key (str, optional): NCBI Entrez API key. If not provided, it will be read from the\n",
    "      'ENTREZ_API_KEY' environment variable.\n",
    "    - delay (float, optional): Delay in seconds between requests to respect rate limits.\n",
    "      If not provided, it defaults to 0.5 seconds without an API key and 0.1 seconds with an API key.\n",
    "    - verbose (bool, optional): If True, prints progress messages. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: Combined DataFrame containing metadata for all fetched SRA IDs.\n",
    "    \"\"\"\n",
    "    # Set Entrez API key from parameter or environment variable\n",
    "    if entrez_api_key is None:\n",
    "        entrez_api_key = os.getenv('ENTREZ_API_KEY')\n",
    "    \n",
    "    if entrez_api_key:\n",
    "        Entrez.api_key = entrez_api_key\n",
    "        if verbose:\n",
    "            print(\"API key detected.\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No API key detected; proceeding without it.\")\n",
    "    \n",
    "    # Set default delay based on API key presence\n",
    "    if delay is None:\n",
    "        delay = 0.1 if entrez_api_key else 0.5\n",
    "    \n",
    "    # List to store each SRA ID's fetched data as a DataFrame\n",
    "    data = []\n",
    "    \n",
    "    # Check if the SRA IDs file exists\n",
    "    if not os.path.isfile(sra_ids_file):\n",
    "        raise FileNotFoundError(f\"SRA IDs file '{sra_ids_file}' does not exist.\")\n",
    "    \n",
    "    # Open and read the SRA IDs file using csv.DictReader for tab-separated values\n",
    "    with open(sra_ids_file, 'r', newline='') as ids_file:\n",
    "        reader = csv.DictReader(ids_file, delimiter='\\t')\n",
    "        if 'SRA_ID' not in reader.fieldnames:\n",
    "            raise ValueError(\"Input file must contain a 'SRA_ID' column.\")\n",
    "        \n",
    "        for line_num, row in enumerate(reader, start=2):  # start=2 accounts for header\n",
    "            sra_id = row.get('SRA_ID', '').strip()\n",
    "            if not sra_id:\n",
    "                if verbose:\n",
    "                    print(f\"Line {line_num}: Missing 'SRA_ID'. Skipping.\")\n",
    "                continue  # Skip if SRA_ID is missing\n",
    "        \n",
    "            if verbose:\n",
    "                print(f\"\\nProcessing SRA ID: {sra_id}\")\n",
    "        \n",
    "            # Construct the command\n",
    "            command = f\"esearch -db sra -query {sra_id} | efetch -format runinfo\"\n",
    "        \n",
    "            if verbose:\n",
    "                print(f\"Executing command: {command}\")\n",
    "        \n",
    "            try:\n",
    "                # Execute the command\n",
    "                result = subprocess.run(\n",
    "                    command,\n",
    "                    shell=True,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    check=True\n",
    "                )\n",
    "        \n",
    "                # Check if output is not empty\n",
    "                if not result.stdout.strip():\n",
    "                    if verbose:\n",
    "                        print(f\"No data returned for SRA ID: {sra_id}.\")\n",
    "                    continue\n",
    "        \n",
    "                # Convert the CSV output to a DataFrame\n",
    "                csv_data = StringIO(result.stdout)\n",
    "                df = pd.read_csv(csv_data)\n",
    "                data.append(df)\n",
    "        \n",
    "                if verbose:\n",
    "                    print(f\"Successfully fetched data for SRA ID: {sra_id}.\")\n",
    "        \n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error processing {sra_id} on line {line_num}: {e}\")\n",
    "                print(f\"Command output: {e.output}\")\n",
    "                continue  # Skip to the next SRA ID if there’s an error\n",
    "        \n",
    "            # Respect API rate limits\n",
    "            if verbose:\n",
    "                print(f\"Sleeping for {delay} seconds to respect rate limits.\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # Combine all DataFrames into one\n",
    "    if data:\n",
    "        combined_df = pd.concat(data, ignore_index=True)\n",
    "        \n",
    "        # Remove columns where all entries are NaN\n",
    "        combined_df.dropna(axis=1, how='all', inplace=True)\n",
    "    \n",
    "        if verbose:\n",
    "            print(\"\\nData fetching complete.\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No data was fetched.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "sra_file = list_files(fastq_directory,\n",
    "                      \"sra_ids.txt\")\n",
    "\n",
    "sra_metadata = fetch_sra_metadata_shell(sra_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e397ab-0db9-4f46-a3c2-d7f0a7dc0aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:  990\n",
      "Prompt tokens:  6825\n",
      "Total tokens:  7815\n",
      "Generated tokens:  910\n",
      "Prompt tokens:  6825\n",
      "Total tokens:  7735\n",
      "Executing Kallisto command for GSE276387_data/SRR30552080_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output/SRR30552080_SRR30552081 -t 4 --plaintext --bootstrap-samples=100 --fr-stranded GSE276387_data/SRR30552080_1.fastq.gz GSE276387_data/SRR30552081_2.fastq.gz\n",
      "Usage: kallisto quant [arguments] FASTQ-files\n",
      "\n",
      "Required arguments:\n",
      "-i, --index=STRING            Filename for the kallisto index to be used for\n",
      "                              quantification\n",
      "-o, --output-dir=STRING       Directory to write output to\n",
      "\n",
      "Optional arguments:\n",
      "-b, --bootstrap-samples=INT   Number of bootstrap samples (default: 0)\n",
      "    --seed=INT                Seed for the bootstrap sampling (default: 42)\n",
      "    --plaintext               Output plaintext instead of HDF5\n",
      "    --single                  Quantify single-end reads\n",
      "    --single-overhang         Include reads where unobserved rest of fragment is\n",
      "                              predicted to lie outside a transcript\n",
      "    --fr-stranded             Strand specific reads, first read forward\n",
      "    --rf-stranded             Strand specific reads, first read reverse\n",
      "-l, --fragment-length=DOUBLE  Estimated average fragment length\n",
      "-s, --sd=DOUBLE               Estimated standard deviation of fragment length\n",
      "                              (default: -l, -s values are estimated from paired\n",
      "                               end data, but are required when using --single)\n",
      "-p, --priors                  Priors for the EM algorithm, either as raw counts or as\n",
      "                              probabilities. Pseudocounts are added to raw reads to\n",
      "                              prevent zero valued priors. Supplied in the same order\n",
      "                              as the transcripts in the transcriptome\n",
      "-t, --threads=INT             Number of threads to use (default: 1)\n",
      "    --verbose                 Print out progress information every 1M proccessed reads\n",
      "Error executing Kallisto for GSE276387_data/SRR30552080_1.fastq.gz: Command 'kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output/SRR30552080_SRR30552081 -t 4 --plaintext --bootstrap-samples=100 --fr-stranded GSE276387_data/SRR30552080_1.fastq.gz GSE276387_data/SRR30552081_2.fastq.gz' returned non-zero exit status 1.\n",
      "\n",
      "Justification saved to output/SRR30552080_SRR30552081/justification.txt\n",
      "\n",
      "Executing Kallisto command for GSE276387_data/SRR30552082_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output/SRR30552082_SRR30552083 -t 4 --plaintext --bootstrap-samples=100 --fr-stranded GSE276387_data/SRR30552082_1.fastq.gz GSE276387_data/SRR30552083_2.fastq.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "Error: could not create directory output/SRR30552080_SRR30552081\n",
      "\n",
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 227,665\n",
      "[index] number of k-mers: 139,900,295\n",
      "[index] number of D-list k-mers: 5,477,475\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: GSE276387_data/SRR30552082_1.fastq.gz\n",
      "                             GSE276387_data/SRR30552083_2.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 80,000 reads, 7,142 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 242.341\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 542 rounds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kallisto quantification completed for GSE276387_data/SRR30552082_1.fastq.gz\n",
      "\n",
      "Justification saved to output/SRR30552082_SRR30552083/justification.txt\n",
      "\n",
      "Executing Kallisto command for GSE276387_data/SRR30552090_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output/SRR30552090_SRR30552091 -t 4 --plaintext --bootstrap-samples=100 --fr-stranded GSE276387_data/SRR30552090_1.fastq.gz GSE276387_data/SRR30552091_2.fastq.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 227,665\n",
      "[index] number of k-mers: 139,900,295\n",
      "[index] number of D-list k-mers: 5,477,475\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: GSE276387_data/SRR30552090_1.fastq.gz\n",
      "                             GSE276387_data/SRR30552091_2.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 80,000 reads, 1,862 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 164.727\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 729 rounds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kallisto quantification completed for GSE276387_data/SRR30552090_1.fastq.gz\n",
      "\n",
      "Justification saved to output/SRR30552090_SRR30552091/justification.txt\n",
      "\n",
      "Executing Kallisto command for GSE276387_data/SRR30552076_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output/SRR30552076_SRR30552077 -t 4 --plaintext --bootstrap-samples=100 --fr-stranded GSE276387_data/SRR30552076_1.fastq.gz GSE276387_data/SRR30552077_2.fastq.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 227,665\n",
      "[index] number of k-mers: 139,900,295\n",
      "[index] number of D-list k-mers: 5,477,475\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: GSE276387_data/SRR30552076_1.fastq.gz\n",
      "                             GSE276387_data/SRR30552077_2.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 80,000 reads, 4,162 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 193.559\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 583 rounds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kallisto quantification completed for GSE276387_data/SRR30552076_1.fastq.gz\n",
      "\n",
      "Justification saved to output/SRR30552076_SRR30552077/justification.txt\n",
      "\n",
      "Executing Kallisto command for GSE276387_data/SRR30552074_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output/SRR30552074_SRR30552075 -t 4 --plaintext --bootstrap-samples=100 --fr-stranded GSE276387_data/SRR30552074_1.fastq.gz GSE276387_data/SRR30552075_2.fastq.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 227,665\n",
      "[index] number of k-mers: 139,900,295\n",
      "[index] number of D-list k-mers: 5,477,475\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: GSE276387_data/SRR30552074_1.fastq.gz\n",
      "                             GSE276387_data/SRR30552075_2.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 80,000 reads, 6,021 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 201.017\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 516 rounds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kallisto quantification completed for GSE276387_data/SRR30552074_1.fastq.gz\n",
      "\n",
      "Justification saved to output/SRR30552074_SRR30552075/justification.txt\n",
      "\n",
      "Executing Kallisto command for GSE276387_data/SRR30552076_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output/SRR30552076_SRR30552077 -t 4 --plaintext --bootstrap-samples=100 --fr-stranded GSE276387_data/SRR30552076_1.fastq.gz GSE276387_data/SRR30552077_2.fastq.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 227,665\n",
      "[index] number of k-mers: 139,900,295\n",
      "[index] number of D-list k-mers: 5,477,475\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: GSE276387_data/SRR30552076_1.fastq.gz\n",
      "                             GSE276387_data/SRR30552077_2.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 80,000 reads, 4,162 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 193.559\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 583 rounds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kallisto quantification completed for GSE276387_data/SRR30552076_1.fastq.gz\n",
      "\n",
      "Justification saved to output/SRR30552076_SRR30552077/justification.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Above is getting a bit too chunky, so next section.\n",
    "\n",
    "# This is where I'll do some Kallisto stuff.\n",
    "\n",
    "class KallistoCommand(BaseModel):\n",
    "    index: str = Field(..., description=\"Filename for the Kallisto index to be used for quantification\")\n",
    "    fastq1: str = Field(..., description=\"Filename for the first FASTQ file (Read 1) to be quantified\")\n",
    "    fastq2: Optional[str] = Field(description=\"Filename for the second FASTQ file (Read 2) to be quantified (optional for single-end reads)\")\n",
    "    output: str = Field(..., description=\"Directory to write output to\")\n",
    "    bootstraps: int = Field(..., description=\"Number of bootstrap samples\")\n",
    "    single: bool = Field(..., description=\"If the reads are single-end\")\n",
    "    fr_stranded: bool = Field(..., description=\"If the reads are strand-specific, with first read forward\")\n",
    "    rf_stranded: bool = Field(..., description=\"If the reads are strand-specific, with first read reverse\")\n",
    "    frag_length: Optional[int] = Field(description=\"Estimated average fragment length (required for single-end reads)\")\n",
    "    sd: Optional[int] = Field(description=\"Estimated standard deviation of fragment length (required for single-end reads)\")\n",
    "    justification: str = Field(..., description=\"Justification for each chosen parameter, including if the parameter was excluded\")\n",
    "\n",
    "class KallistoCommands(BaseModel):\n",
    "    commands: List[KallistoCommand] = Field(description=\"List of Kallisto quantification commands for each sample\")\n",
    "\n",
    "def identify_kallisto_params():\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "## IDENTITY AND PURPOSE\n",
    "\n",
    "You are an expert in bioinformatic analyses. You will be provided with various pieces of information, and use this information to determine the appropriate parameters for a Kallisto analysis.\n",
    "\n",
    "## STEPS\n",
    "\n",
    "1. Carefully digest the contents of the provided Kallisto documentation. Note that any existing knowledge you have of Kallisto may not be correct, so follow the documentation closely.\n",
    "2. Carefully consider the contents of the sample metadata. Not all information will be relevant, however there will be content that will be needed.\n",
    "3. Carefully look through the dataset metadata. This may contain details that are useful.\n",
    "4. After considering all of the above, determine which Kallisto parameters should be set. Do not make any assumptions that are not explicitly stated for any optional fields. If unsure, leave blank.\n",
    "5. In determining parameters, make sure you only choose valid files (i.e. pick out of the options which are provided)\n",
    "6. Ensure that the chosen parameters allow for a robust analysis that would satisfy the most critical peer reviewers.\n",
    "7. You should prioritize scientific robustness over ease of computational burden.\n",
    "8. Note the following guidelines for some specific parameters:\n",
    "- the output directory should be named such that the sample being quantified can be identified from this output directory.\n",
    "\n",
    "## OUTPUT\n",
    "\n",
    "Your output should consist of each parameter, and either:\n",
    "- the value to be included for the parameter\n",
    "- if the parameter should not be included, you should state NA\n",
    "- For ALL chosen parameters, describe the justification for including the particular value, or excluding it.\n",
    "\n",
    "This should be applied to all parameters identified as per the provided Kallisto documentation.\n",
    "\n",
    "## INPUT\n",
    "\n",
    "Kallisto documentation: {kallisto_docs}\n",
    "\n",
    "Dataset summary: {study_summary}\n",
    "\n",
    "FASTQ files: {fastq_files}\n",
    "\n",
    "Possible Kallisto indices: {index_files}\n",
    "\n",
    "Sample metadata: {sra_metadata.to_json}\n",
    "\n",
    "\"\"\"\n",
    "    chat_completion = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format = KallistoCommands\n",
    "        )\n",
    "    result = chat_completion.choices[0].message.parsed\n",
    "    print(f\"Generated tokens: \", chat_completion.usage.completion_tokens)\n",
    "    print(f\"Prompt tokens: \", chat_completion.usage.prompt_tokens)\n",
    "    print(f\"Total tokens: \", chat_completion.usage.total_tokens)\n",
    "    return(result)\n",
    "\n",
    "kallisto_params = identify_kallisto_params()\n",
    "\n",
    "for cmd in kallisto_params.commands:\n",
    "    # Construct the Kallisto command string\n",
    "    kallisto_cmd = f\"kallisto quant -i {cmd.index} -o {cmd.output} -t 4\"\n",
    "    \n",
    "    if cmd.bootstraps > 0:\n",
    "        kallisto_cmd += f\" --bootstrap-samples={cmd.bootstraps}\"\n",
    "    \n",
    "    if cmd.single:\n",
    "        kallisto_cmd += \" --single\"\n",
    "        if cmd.frag_length:\n",
    "            kallisto_cmd += f\" -l {cmd.frag_length}\"\n",
    "        if cmd.sd:\n",
    "            kallisto_cmd += f\" -s {cmd.sd}\"\n",
    "    else:\n",
    "        # Paired-end\n",
    "        if cmd.fr_stranded:\n",
    "            kallisto_cmd += \" --fr-stranded\"\n",
    "        elif cmd.rf_stranded:\n",
    "            kallisto_cmd += \" --rf-stranded\"\n",
    "    \n",
    "    # Append FASTQ files\n",
    "    kallisto_cmd += f\" {cmd.fastq1} {cmd.fastq2}\"\n",
    "\n",
    "def execute_kallisto_commands(kallisto_commands: KallistoCommands):\n",
    "    for cmd in kallisto_commands.commands:\n",
    "        # Construct the Kallisto command string\n",
    "        kallisto_cmd = f\"kallisto quant -i {cmd.index} -o {cmd.output} -t 4 --plaintext\"\n",
    "        \n",
    "        if cmd.bootstraps > 0:\n",
    "            kallisto_cmd += f\" --bootstrap-samples={cmd.bootstraps}\"\n",
    "        \n",
    "        if cmd.single:\n",
    "            kallisto_cmd += \" --single\"\n",
    "            if cmd.frag_length:\n",
    "                kallisto_cmd += f\" -l {cmd.frag_length}\"\n",
    "            if cmd.sd:\n",
    "                kallisto_cmd += f\" -s {cmd.sd}\"\n",
    "        else:\n",
    "            # Paired-end\n",
    "            if cmd.fr_stranded:\n",
    "                kallisto_cmd += \" --fr-stranded\"\n",
    "            elif cmd.rf_stranded:\n",
    "                kallisto_cmd += \" --rf-stranded\"\n",
    "        \n",
    "        # Append FASTQ files\n",
    "        if cmd.fastq2 and cmd.fastq2.lower() != 'na':\n",
    "            kallisto_cmd += f\" {cmd.fastq1} {cmd.fastq2}\"\n",
    "        else:\n",
    "            kallisto_cmd += f\" {cmd.fastq1}\"\n",
    "        \n",
    "        print(f\"Executing Kallisto command for {cmd.fastq1}:\")\n",
    "        print(kallisto_cmd)\n",
    "\n",
    "        # Execute the command\n",
    "        try:\n",
    "            subprocess.run(kallisto_cmd, shell=True, check=True)\n",
    "            print(f\"Kallisto quantification completed for {cmd.fastq1}\\n\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing Kallisto for {cmd.fastq1}: {e}\\n\")\n",
    "        \n",
    "        # Optionally, log the justification\n",
    "        justification_path = os.path.join(cmd.output, \"justification.txt\")\n",
    "        os.makedirs(cmd.output, exist_ok=True)\n",
    "        with open(justification_path, \"w\") as f:\n",
    "            f.write(cmd.justification)\n",
    "        print(f\"Justification saved to {justification_path}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kallisto_commands = identify_kallisto_params()\n",
    "    execute_kallisto_commands(kallisto_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d81d93-e6b7-4e6c-bc96-2c270529c8d6",
   "metadata": {},
   "source": [
    "## Part 3b - Analysing the quantification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51bb023-bd66-4f19-86f8-bda2d65b9e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:  168\n",
      "Prompt tokens:  3417\n",
      "Total tokens:  3585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ColumnMerging(merge=True, cols=['source_name_ch1', 'characteristics_ch1', 'characteristics_ch1.2'], justification=\"The columns 'source_name_ch1', 'characteristics_ch1', and 'characteristics_ch1.2' contain distinct but complementary biological information relevant to the samples. 'source_name_ch1' indicates the tissue source (Primary-Lung, Metastasis-Brain), 'characteristics_ch1' specifies the tissue type, and 'characteristics_ch1.2' specifies cell type (PDO, MDO, Tumor). The merge will help capture the unique biological context of the samples by combining this information into a format that reflects both tissue and cell type. A viable merged output could look like 'Primary-Lung_PDO', 'Metastasis-Brain_MDO', etc.\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by reading the metadata... I hope this won't lead to any complications...\n",
    "\n",
    "metadata_csv = list_files(directory = fastq_directory, \n",
    "                          suffix = \".csv\")\n",
    "df = pd.read_csv(metadata_csv[0])\n",
    "df = df.loc[:, df.nunique() > 1]\n",
    "metadata_json = df.to_json(orient='records', lines=False, indent=2) # parse to JSON\n",
    "\n",
    "# Quick inspection - will be interesting, because I previously had an example reliant on merging. This one does not. Will see if I've hardcoded\n",
    "\n",
    "class ColumnMerging(BaseModel):\n",
    "    merge: bool = Field(..., description=\"Whether or not columns should be merged\")\n",
    "    cols: Optional[list[str]] = Field(..., description=\"List of columns to be merged\")\n",
    "    justification: str = Field(..., description = \"Justification of columns being merged/why no columns needed to be merged\")\n",
    "\n",
    "def Identify_ColMerges():\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "### IDENTITY AND PURPOSE\n",
    "\n",
    "You are an expert in bioinformatics. You advise on the most scientifically valuable experiments that can be performed, and have a deep awareness of DEG analysis tools, such as limma and edgeR.\n",
    "\n",
    "Your task is to study the provided metadata, and determine which columns to use in proceeding with the analysis.\n",
    "\n",
    "### STEPS\n",
    "\n",
    "Note that a future step of the analysis will involve design of a matrix as follows:\n",
    "design <- model.matrix(data = DGE.final$samples,\n",
    "                       ~0 + column)\n",
    "\n",
    "Crucially, this only includes a single column. As such, if there are columns with DISTINCT SCIENTIFIC INFORMATION, these should be merged. Columns with similar information DO NOT need to be merged. Therefore, take a deep breath, and follow these steps to ensure that subsequent analyses are as robust as possible:\n",
    "\n",
    "1. Assess the content of each column in the provided metadata\n",
    "2. Determine which columns contain anything of biological relevance\n",
    "3. Determine if any columns are redundant, and do not need to be considered (e.g. similar content). In this case, only consider the column with simpler values (i.e. fewer special characters)\n",
    "4. Determine which columns contain information that would be scientifically valuable to analyse, i.e. could result in a meaningful biological finding.\n",
    "5. If there are multiple columns that contain scientifically valuable information, identify these columns as needing to be merged.\n",
    "6. If there is one one column containing scientifically valuable information, no columns need to be merged\n",
    "7. If you would be merging two redundant columns, these do not need to be merged. As such, no merge should occur (i.e. set merge to FALSE). Note that in this case, merging will COMPLICATE the analysis. Instead, IGNORING one of these columns is the best way to proceed.\n",
    "8. Be very aware that no merging can be perfectly viable. Do not force a suboptimal merge.\n",
    "\n",
    "Take into consideration that, suppose the values in one column are\n",
    "A\n",
    "B\n",
    "C\n",
    "\n",
    "And another column are \n",
    "1\n",
    "2\n",
    "3\n",
    "\n",
    "The merged output would be\n",
    "A_1\n",
    "B_2\n",
    "C_3 \n",
    "\n",
    "(or something comparable to that)\n",
    "\n",
    "### OUTPUT\n",
    "\n",
    "- Specify if any columns will need to be merged\n",
    "- State the names of the columns to be merged\n",
    "- Justify your choice\n",
    "\n",
    "### INPUT METADATA\n",
    "\n",
    "{metadata_json}\n",
    "\n",
    "\"\"\"\n",
    "    chat_completion = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format = ColumnMerging\n",
    "        )\n",
    "    result = chat_completion.choices[0].message.parsed\n",
    "    print(f\"Generated tokens: \", chat_completion.usage.completion_tokens)\n",
    "    print(f\"Prompt tokens: \", chat_completion.usage.prompt_tokens)\n",
    "    print(f\"Total tokens: \", chat_completion.usage.total_tokens)\n",
    "    return(result)\n",
    "\n",
    "col_merge_info = Identify_ColMerges()\n",
    "col_merge_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c8e077-9d1d-4779-8860-ba3ea0855146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged columns ['source_name_ch1', 'characteristics_ch1', 'characteristics_ch1.2'] into 'merged_analysis_group'.\n"
     ]
    }
   ],
   "source": [
    "# omg I actually give up. I think a completely different approach is needed. Now, I don't think this will change any of the results, however it is indicative of this not working to the standard I want.\n",
    "\n",
    "def clean_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean a string by normalizing special characters, replacing spaces with underscores,\n",
    "    and removing non-word characters.\n",
    "\n",
    "    Args:\n",
    "        s (str): The string to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned string.\n",
    "    \"\"\"\n",
    "    if pd.isnull(s):\n",
    "        return \"NA\"  # Handle missing values\n",
    "    s = str(s)\n",
    "    s = s.strip()  # Remove leading and trailing whitespaces\n",
    "    s = unidecode(s)  # Normalize special characters to ASCII\n",
    "    s = s.replace(\" \", \"_\")  # Replace spaces with underscores\n",
    "    s = re.sub(r'[^\\w]', '', s)  # Remove non-word characters (retain letters, digits, underscores)\n",
    "    return s\n",
    "\n",
    "def process_column_merging(df: pd.DataFrame, column_merge_info: ColumnMerging) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process column merging based on ColumnMerging information.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The sample metadata DataFrame.\n",
    "        column_merge_info (ColumnMerging): Information about column merging.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with merged columns if applicable.\n",
    "    \"\"\"\n",
    "    if column_merge_info.merge:\n",
    "        # Ensure that at least two columns are provided for merging\n",
    "        if not column_merge_info.cols or len(column_merge_info.cols) < 2:\n",
    "            raise ValueError(\"At least two columns must be specified for merging when merge=True.\")\n",
    "        \n",
    "        cols_to_merge = column_merge_info.cols\n",
    "\n",
    "        # Generate new column name by combining base names of the columns to merge\n",
    "        # For example, merging 'genotype:ch1' and 'treatment:ch1' becomes 'genotype_treatment_clean'\n",
    "        base_names = [col.split(\":\")[0] for col in cols_to_merge]\n",
    "        new_col_name = \"merged_analysis_group\"\n",
    "\n",
    "        # Clean the values in the columns to be merged\n",
    "        cleaned_columns = df[cols_to_merge].map(clean_string)\n",
    "\n",
    "        # Merge the cleaned columns by concatenating their values with underscores\n",
    "        df[new_col_name] = cleaned_columns.apply(lambda row: \"_\".join(row.values), axis=1)\n",
    "\n",
    "        print(f\"Merged columns {cols_to_merge} into '{new_col_name}'.\")\n",
    "    else:\n",
    "        # When merging is not required, ensure exactly one column is specified\n",
    "        if not column_merge_info.cols or len(column_merge_info.cols) != 1:\n",
    "            raise ValueError(\"Exactly one column must be specified for cleaning when merge=False.\")\n",
    "        \n",
    "        col_to_clean = column_merge_info.cols[0]\n",
    "\n",
    "        # Generate a new column name by appending '_clean' to the original column name\n",
    "        new_col_name = \"merged_analysis_group\"\n",
    "\n",
    "        # Rename the column in the DataFrame\n",
    "        df = df.rename(columns={col_to_clean: new_col_name})\n",
    "\n",
    "        # Clean the values in the renamed column\n",
    "        df[new_col_name] = df[new_col_name].apply(clean_string)\n",
    "\n",
    "        print(f\"Cleaned column '{col_to_clean}' into '{new_col_name}'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "cleaned_metadata_df = process_column_merging(df, col_merge_info)\n",
    "cleaned_metadata_json = cleaned_metadata_df.to_json(orient='records', lines=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b623d4-a46e-47a0-bde3-56475a3e7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:  355\n",
      "Prompt tokens:  4177\n",
      "Total tokens:  4532\n"
     ]
    }
   ],
   "source": [
    "# Now to identify the contrasts...\n",
    "\n",
    "class Contrast(BaseModel):\n",
    "    name: str = Field(..., description = \"Name of contrast to perform\")\n",
    "    values: list[str] = Field(..., description = \"Values involved in analysis of the contrast\")\n",
    "    description: str = Field(..., description = \"Description of the contrast\")\n",
    "    justification: str = Field(..., description = \"Justification of why the contrast is of interest to analyse\")\n",
    "\n",
    "class AllAnalysisContrasts(BaseModel):\n",
    "    contrasts: list[Contrast]\n",
    "\n",
    "def IdentifyContrasts():\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "### IDENTITY AND PURPOSE\n",
    "\n",
    "You are an expert in bioinformatics. You advise on the most scientifically valuable experiments that can be performed, and have a deep awareness of DEG analysis tools, such as limma and edgeR.\n",
    "\n",
    "Your task is to study the provided information, and determine what contrasts would be interesting to study.\n",
    "\n",
    "### STEPS\n",
    "\n",
    "1. You will be given input sample metadata. The crux of the decision making should be based on this.\n",
    "2. You will be given some input information about a \"merged column\" called \"merged_analysis_group\". You should focus on the values in this column. However, the information will also detail where the merged values are derived from, so you can use this information as well.\n",
    "3. You will be provided information about the dataset summary. Use this to inform about the scientific purpose of the dataset.\n",
    "4. Having considered and digested the input information, carefully decide what the most valuable contrasts to analyse will be. Keep in mind the following guidelines:\n",
    "- The values you specify should be derived ONLY from the merged column\n",
    "- The contrasts you analyse should have scientific value, and not simply be \"control experiments\"\n",
    "- The contrasts should be focussed and have a clear defined purpose\n",
    "- Here are some examples of how to structure the contrasts:\n",
    "    - If the samples to be compared are, for example \"Treatment X vs. Y in genotpye A samples\", the output should be \"X_A, Y_A\" (where X_A refers to the EXACT value in the merged_analysis_group column)\n",
    "    - If the samples to be compared are, for example \"Treatment X vs. Y\", the output should be \"X_A, X_B, Y_A, Y_B\". \n",
    "5. Once you have produced the output, double check that:\n",
    "- You have considered the correct column\n",
    "- The values you have stated are derived from the correct column\n",
    "\n",
    "\n",
    "### OUTPUT\n",
    "\n",
    "- Assign a name for each contrast\n",
    "- State the values required to correctly analyse each contrast. These values must EXACTLY match the value in the merged_analysis_group column\n",
    "- Describe what the contrast is investigating\n",
    "- Justify why the contrast is scientifically valuable\n",
    "\n",
    "### INPUTS\n",
    "\n",
    "Sample metadata: {cleaned_metadata_json}\n",
    "Information about merged columns: {col_merge_info}\n",
    "Dataset summary: {study_summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    chat_completion = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format = AllAnalysisContrasts\n",
    "        )\n",
    "    result = chat_completion.choices[0].message.parsed\n",
    "    print(f\"Generated tokens: \", chat_completion.usage.completion_tokens)\n",
    "    print(f\"Prompt tokens: \", chat_completion.usage.prompt_tokens)\n",
    "    print(f\"Total tokens: \", chat_completion.usage.total_tokens)\n",
    "    return(result)\n",
    "\n",
    "contrasts_data = IdentifyContrasts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f95de346-b8ef-4db6-8e3d-40c18861eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:  125\n",
      "Prompt tokens:  714\n",
      "Total tokens:  839\n"
     ]
    }
   ],
   "source": [
    "# Generate expressions for these contrasts\n",
    "\n",
    "class Expressions(BaseModel):\n",
    "    name: str = Field(..., description = \"Name of contrast to perform\")\n",
    "    expressions: str = Field(..., description = \"Expressions representing contrasts\")\n",
    "\n",
    "class ContrastMatrix(BaseModel):\n",
    "    contrasts: list[Expressions]\n",
    "\n",
    "def GenerateContrastExpressions():\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "### IDENTITY AND PURPOSE\n",
    "\n",
    "You are an expert in bioinformatics. You advise on the most scientifically valuable experiments that can be performed, and have a deep awareness of DEG analysis tools, such as limma and edgeR.\n",
    "\n",
    "Your task is to study the provided information, and determine the epxressions to use to construct the contrast matrix.\n",
    "\n",
    "### STEPS\n",
    "\n",
    "1. You will be given input information about the contrasts to use. Make note of the description of the contrast, as well as the values\n",
    "2. For each suggested contrast, state a simple name to represent it (e.g. TreatmentInKO). The fewer characters the better, however it should still be informative.\n",
    "3. For each suggested contrast, use an expression to represent it. The expression must only use values, exactly as written, indicated in the information about contrasts. Note that this expression MUST be compatible with the makeContrasts function. See below for some examples:\n",
    "\"GNASknockout - WT\"\n",
    "\"(GNASknockout_A - GNASknockout_B) - (WT_A - WT_B)\"\n",
    "\n",
    "\n",
    "### OUTPUT\n",
    "\n",
    "- State a simple name for each contrast\n",
    "- State an appropriate expression for each contrast\n",
    "\n",
    "### INPUTS\n",
    "\n",
    "Contrast information: {contrasts_data}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    chat_completion = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format = ContrastMatrix\n",
    "        )\n",
    "    result = chat_completion.choices[0].message.parsed\n",
    "    print(f\"Generated tokens: \", chat_completion.usage.completion_tokens)\n",
    "    print(f\"Prompt tokens: \", chat_completion.usage.prompt_tokens)\n",
    "    print(f\"Total tokens: \", chat_completion.usage.total_tokens)\n",
    "    return(result)\n",
    "\n",
    "exprs = GenerateContrastExpressions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7667d1ad-f16a-4c0a-ac9f-ae146fdbc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The contrast (singular in this case, I would agree this is reasonable) to analyse has been determined, now to generate DGEList object to perform the DEG analysis.\n",
    "\n",
    "# Part 1 of this is to identify the input files that will be needed. This includes the metadata, linking samples to their abundance files, \n",
    "# the actual abundance files, transcript to gene text file\n",
    "\n",
    "# First the Kallisto and tx2gene files.\n",
    "\n",
    "abundance_directory = \".\" # [HARDCODED]\n",
    "abundance_suffix = \"abundance.tsv\" # [HARDCODED]\n",
    "abundance_files = list_files(abundance_directory, abundance_suffix) # just for my own sanity I didn't print the output, but I can see it was able to find all the files\n",
    "tx2gene_files = list_files(directory = \"/home/myuser/work\", # Ah. [HARDCODED]. This won't work well...\n",
    "                          suffix = \"t2g.txt\")\n",
    "\n",
    "SRA_IDs = pd.read_csv(sra_file[0], sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea760b2-e21a-45a4-8ef9-df45e3b3fa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:  134\n",
      "Prompt tokens:  536\n",
      "Total tokens:  670\n"
     ]
    }
   ],
   "source": [
    "class IDMatching(BaseModel):\n",
    "    SRA_ID: str = Field(..., description = \"Name of SRA ID\")\n",
    "    Kallisto_path: str = Field(..., description = \"Name of matching Kallisto path\")\n",
    "\n",
    "class AllIDMatches(BaseModel):\n",
    "    AllMatches: list[IDMatching]\n",
    "\n",
    "def Match_SRAIDs():\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "You will be given inputs for SRA IDs, as well as the path to abundance files generated from Kallisto. Your task is to generate 1-to-1 matches between abundance files and the SRA IDs.\n",
    "\n",
    "That is - for each SRA ID, identify the single path that is most likely to correspond to that SRA ID.\n",
    "\n",
    "Your output should consist of ONLY the SRA ID, and their matching Kallisto path. The output should match the input text EXACTLY with no other formatting included.\n",
    "\n",
    "### INPUTS\n",
    "\n",
    "SRA IDs: {SRA_IDs['SRA_ID']}\n",
    "Kallisto paths: {abundance_files}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    chat_completion = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format = AllIDMatches\n",
    "        )\n",
    "    result = chat_completion.choices[0].message.parsed\n",
    "    print(f\"Generated tokens: \", chat_completion.usage.completion_tokens)\n",
    "    print(f\"Prompt tokens: \", chat_completion.usage.prompt_tokens)\n",
    "    print(f\"Total tokens: \", chat_completion.usage.total_tokens)\n",
    "    return(result)\n",
    "\n",
    "# Now a commands to link these together...\n",
    "\n",
    "SRA_ID_links = Match_SRAIDs()\n",
    "SRA_IDs = pd.read_csv(sra_file[0], sep = '\\t')\n",
    "json_data = SRA_ID_links.json()\n",
    "all_id_matches = AllIDMatches.parse_raw(json_data)\n",
    "sra_to_kallisto = {match.SRA_ID: match.Kallisto_path for match in all_id_matches.AllMatches}\n",
    "SRA_IDs['Kallisto_path'] = SRA_IDs['SRA_ID'].map(sra_to_kallisto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11b9987d-77f8-41ea-ac6e-7e4b1c93308f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_ID</th>\n",
       "      <th>experiment</th>\n",
       "      <th>SRA_ID</th>\n",
       "      <th>Kallisto_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSM8497661</td>\n",
       "      <td>SRX25975444</td>\n",
       "      <td>SRR30552073</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GSM8497660</td>\n",
       "      <td>SRX25975443</td>\n",
       "      <td>SRR30552074</td>\n",
       "      <td>./output/SRR30552074_SRR30552075/abundance.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GSM8497659</td>\n",
       "      <td>SRX25975442</td>\n",
       "      <td>SRR30552075</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSM8497658</td>\n",
       "      <td>SRX25975441</td>\n",
       "      <td>SRR30552076</td>\n",
       "      <td>./output/SRR30552076_SRR30552077/abundance.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GSM8497657</td>\n",
       "      <td>SRX25975440</td>\n",
       "      <td>SRR30552077</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GSM8497656</td>\n",
       "      <td>SRX25975439</td>\n",
       "      <td>SRR30552078</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GSM8497655</td>\n",
       "      <td>SRX25975438</td>\n",
       "      <td>SRR30552079</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GSM8497654</td>\n",
       "      <td>SRX25975437</td>\n",
       "      <td>SRR30552080</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GSM8497653</td>\n",
       "      <td>SRX25975436</td>\n",
       "      <td>SRR30552081</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GSM8497652</td>\n",
       "      <td>SRX25975435</td>\n",
       "      <td>SRR30552082</td>\n",
       "      <td>./output/SRR30552082_SRR30552083/abundance.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GSM8497651</td>\n",
       "      <td>SRX25975434</td>\n",
       "      <td>SRR30552083</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GSM8497650</td>\n",
       "      <td>SRX25975433</td>\n",
       "      <td>SRR30552084</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GSM8497649</td>\n",
       "      <td>SRX25975432</td>\n",
       "      <td>SRR30552085</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GSM8497648</td>\n",
       "      <td>SRX25975431</td>\n",
       "      <td>SRR30552086</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GSM8497647</td>\n",
       "      <td>SRX25975430</td>\n",
       "      <td>SRR30552087</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GSM8497646</td>\n",
       "      <td>SRX25975429</td>\n",
       "      <td>SRR30552088</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GSM8497645</td>\n",
       "      <td>SRX25975428</td>\n",
       "      <td>SRR30552089</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GSM8497644</td>\n",
       "      <td>SRX25975427</td>\n",
       "      <td>SRR30552090</td>\n",
       "      <td>./output/SRR30552090_SRR30552091/abundance.tsv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GSM8497643</td>\n",
       "      <td>SRX25975426</td>\n",
       "      <td>SRR30552091</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GSM8497642</td>\n",
       "      <td>SRX25975425</td>\n",
       "      <td>SRR30552092</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GSM8497641</td>\n",
       "      <td>SRX25975424</td>\n",
       "      <td>SRR30552093</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GSM8497640</td>\n",
       "      <td>SRX25975423</td>\n",
       "      <td>SRR30552094</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GSM8497639</td>\n",
       "      <td>SRX25975422</td>\n",
       "      <td>SRR30552095</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GSM8497638</td>\n",
       "      <td>SRX25975421</td>\n",
       "      <td>SRR30552096</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GSM8497637</td>\n",
       "      <td>SRX25975420</td>\n",
       "      <td>SRR30552097</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GSM8497636</td>\n",
       "      <td>SRX25975419</td>\n",
       "      <td>SRR30552098</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_ID   experiment       SRA_ID  \\\n",
       "0   GSM8497661  SRX25975444  SRR30552073   \n",
       "1   GSM8497660  SRX25975443  SRR30552074   \n",
       "2   GSM8497659  SRX25975442  SRR30552075   \n",
       "3   GSM8497658  SRX25975441  SRR30552076   \n",
       "4   GSM8497657  SRX25975440  SRR30552077   \n",
       "5   GSM8497656  SRX25975439  SRR30552078   \n",
       "6   GSM8497655  SRX25975438  SRR30552079   \n",
       "7   GSM8497654  SRX25975437  SRR30552080   \n",
       "8   GSM8497653  SRX25975436  SRR30552081   \n",
       "9   GSM8497652  SRX25975435  SRR30552082   \n",
       "10  GSM8497651  SRX25975434  SRR30552083   \n",
       "11  GSM8497650  SRX25975433  SRR30552084   \n",
       "12  GSM8497649  SRX25975432  SRR30552085   \n",
       "13  GSM8497648  SRX25975431  SRR30552086   \n",
       "14  GSM8497647  SRX25975430  SRR30552087   \n",
       "15  GSM8497646  SRX25975429  SRR30552088   \n",
       "16  GSM8497645  SRX25975428  SRR30552089   \n",
       "17  GSM8497644  SRX25975427  SRR30552090   \n",
       "18  GSM8497643  SRX25975426  SRR30552091   \n",
       "19  GSM8497642  SRX25975425  SRR30552092   \n",
       "20  GSM8497641  SRX25975424  SRR30552093   \n",
       "21  GSM8497640  SRX25975423  SRR30552094   \n",
       "22  GSM8497639  SRX25975422  SRR30552095   \n",
       "23  GSM8497638  SRX25975421  SRR30552096   \n",
       "24  GSM8497637  SRX25975420  SRR30552097   \n",
       "25  GSM8497636  SRX25975419  SRR30552098   \n",
       "\n",
       "                                     Kallisto_path  \n",
       "0                                              NaN  \n",
       "1   ./output/SRR30552074_SRR30552075/abundance.tsv  \n",
       "2                                              NaN  \n",
       "3   ./output/SRR30552076_SRR30552077/abundance.tsv  \n",
       "4                                              NaN  \n",
       "5                                              NaN  \n",
       "6                                              NaN  \n",
       "7                                              NaN  \n",
       "8                                              NaN  \n",
       "9   ./output/SRR30552082_SRR30552083/abundance.tsv  \n",
       "10                                             NaN  \n",
       "11                                             NaN  \n",
       "12                                             NaN  \n",
       "13                                             NaN  \n",
       "14                                             NaN  \n",
       "15                                             NaN  \n",
       "16                                             NaN  \n",
       "17  ./output/SRR30552090_SRR30552091/abundance.tsv  \n",
       "18                                             NaN  \n",
       "19                                             NaN  \n",
       "20                                             NaN  \n",
       "21                                             NaN  \n",
       "22                                             NaN  \n",
       "23                                             NaN  \n",
       "24                                             NaN  \n",
       "25                                             NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRA_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1dfc97e-6237-468d-9241-e306b8208f7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linked_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, delete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tmp_meta:\n\u001b[1;32m     10\u001b[0m     metadata_path \u001b[38;5;241m=\u001b[39m tmp_meta\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mlinked_data\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(metadata_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create a temporary R script file\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, delete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.R\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tmp_r_script:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linked_data' is not defined"
     ]
    }
   ],
   "source": [
    "# With this being done, now I prepare the reading of the files and the execution of the R script\n",
    "\n",
    "\n",
    "# Paths and data (Assuming these are defined elsewhere in your code)\n",
    "tx2gene_path = tx2gene_files[1] # [HARDCODED] ... I think I need this as an LLM prompt to determine which path to use... \n",
    "analysis_group = \"merged_analysis_group\" # [HARDCODED] ...\n",
    "\n",
    "# Export metadata to a temporary CSV file for R to read\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8') as tmp_meta:\n",
    "    metadata_path = tmp_meta.name\n",
    "    linked_data.to_csv(metadata_path, index=False)\n",
    "\n",
    "# Create a temporary R script file\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.R', encoding='utf-8') as tmp_r_script:\n",
    "    r_script_path = tmp_r_script.name\n",
    "\n",
    "    # Start constructing the R script\n",
    "    r_script = f\"\"\"\n",
    "    library(tximport)\n",
    "    library(tidyverse)\n",
    "    library(edgeR)\n",
    "    \n",
    "    # Read tx2gene\n",
    "    tx2gene <- read_tsv(\"{tx2gene_path}\", col_names = FALSE) %>%\n",
    "      dplyr::select(1, 3) %>%\n",
    "      drop_na()\n",
    "    \n",
    "    # Define abundance files\n",
    "    files <- c({', '.join([f'\"{file}\"' for file in abundance_files])})\n",
    "    \n",
    "    # Import data using tximport\n",
    "    kallisto <- tximport(files = files,\n",
    "                        type = \"kallisto\",\n",
    "                        tx2gene = tx2gene,\n",
    "                        ignoreAfterBar = TRUE,\n",
    "                        countsFromAbundance = \"lengthScaledTPM\")\n",
    "    \n",
    "    # Read metadata\n",
    "    meta <- read.csv(\"{metadata_path}\", row.names = 1)\n",
    "    \n",
    "    # Create DGEList\n",
    "    DGE <- DGEList(counts = kallisto$counts,\n",
    "                  samples = meta)\n",
    "    \n",
    "    keep.exprs <- filterByExpr(DGE, group = DGE$samples${analysis_group})\n",
    "    DGE.filtered <- DGE[keep.exprs, keep.lib.sizes = FALSE]\n",
    "    print(dim(DGE.filtered))\n",
    "    # Normalize\n",
    "    DGE.final <- calcNormFactors(DGE.filtered)\n",
    "    \"\"\"\n",
    "\n",
    "    # Add Design Matrix Code\n",
    "    r_script += f\"\"\"\n",
    "    library(stringr)\n",
    "    \n",
    "    # Create design matrix using the specified grouping variable\n",
    "    design <- model.matrix(~0 + {analysis_group}, data = DGE.final$samples)\n",
    "    \n",
    "    # Clean column names by removing the grouping variable string\n",
    "    colnames(design) <- str_remove_all(colnames(design), \"{analysis_group}\")\n",
    "    \n",
    "    print(design)\n",
    "    \"\"\"\n",
    "\n",
    "    # Add Contrast Matrix Code\n",
    "    # Extract contrast names and expressions from exprs\n",
    "    contrast_entries = []\n",
    "    for contrast in exprs.dict()['contrasts']:\n",
    "        name = contrast['name']\n",
    "        expression = contrast['expressions']\n",
    "        # Escape double quotes in expressions\n",
    "        expression = expression.replace('\"', '\\\\\"')\n",
    "        contrast_entries.append(f'{name} = \"{expression}\"')\n",
    "\n",
    "    contrast_matrix_str = \",\\n  \".join(contrast_entries)\n",
    "\n",
    "    # Use single quotes in message to avoid conflicts with double quotes in contrast_matrix_str\n",
    "    r_script += f\"\"\"\n",
    "    colnames(design)\n",
    "    # Create contrast matrix\n",
    "    contrast.matrix <- makeContrasts(\n",
    "      {contrast_matrix_str},\n",
    "      levels = colnames(design)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Optionally, you can proceed with fitting the model and other downstream analysis\n",
    "    v <- voom(DGE.final,\n",
    "          design)\n",
    "    vfit <- lmFit(v,\n",
    "              design)\n",
    "              \n",
    "    vfit <- contrasts.fit(vfit,\n",
    "                      contrast.matrix)\n",
    "                      \n",
    "    efit <- eBayes(vfit)\n",
    "\n",
    "    contrasts <- colnames(contrast.matrix)\n",
    "    \n",
    "    LFC.summary <- sapply(contrasts, function(x){{\n",
    "    lfc.list <- list()\n",
    "    top <- topTable(efit,\n",
    "                    coef = x,\n",
    "                    number = Inf) %>%\n",
    "    list()\n",
    "    lfc.list <- append(lfc.list, top)\n",
    "    }})\n",
    "\n",
    "    saveRDS(LFC.summary, \"LFCs.RDS\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Write the complete R script to the temporary file\n",
    "    tmp_r_script.write(r_script)\n",
    "\n",
    "# Optional: Print the generated R script for debugging\n",
    "print(\"Generated R Script:\\n\")\n",
    "with open(r_script_path, 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Execute the R script\n",
    "try:\n",
    "    print(\"Executing R script...\")\n",
    "    subprocess.run([\"Rscript\", r_script_path], check=True)\n",
    "    print(\"R script executed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"An error occurred while executing the R script.\")\n",
    "    print(\"Error message:\")\n",
    "    print(e.stderr if e.stderr else e)\n",
    "finally:\n",
    "    # Clean up temporary files if desired\n",
    "    os.remove(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "519e464a-4f23-4856-b2b9-0fe5a4f612ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SRR30552074': './output/SRR30552074_SRR30552075/abundance.tsv',\n",
       " 'SRR30552076': './output/SRR30552076_SRR30552077/abundance.tsv',\n",
       " 'SRR30552082': './output/SRR30552082_SRR30552083/abundance.tsv',\n",
       " 'SRR30552090': './output/SRR30552090_SRR30552091/abundance.tsv'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sra_to_kallisto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4966ae7-0444-424d-a07e-cc7dc8c85279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry)",
   "language": "python",
   "name": "poetry-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
