{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e006268-09ab-406d-b50c-f5e987bc2a3a",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This notebook will contain ONLY the code to complete the end to end workflow. The idea is to make sure I can execute everything from start to finish, and to understand where the challenges will lie.\n",
    "\n",
    "I am also finding that a bit of the code with structured outputs in the past is now outdated. I will modernise it.\n",
    "\n",
    "I will mark some of the hardcoded stuff with [HARDCODED]. (Ooh, fancy formatting. I hope that doesn't break anything...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17b777-2d57-4cc5-9ae1-77a18b81c9d4",
   "metadata": {},
   "source": [
    "# Part 0 - Loading modules/.env/preparing OpenAI client\n",
    "\n",
    "The goal of this section is to load all the necessary modules, as well as prepare the OpenAI client.\n",
    "\n",
    "Hmmm... I suspect in the end I would want all the classes/functions/prompts defined somewhere...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4dce14a6-ddea-48b8-ac11-343f85a12fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook SingleDatasetAnalysis.ipynb to script\n",
      "[NbConvertApp] Writing 53260 bytes to SingleDatasetAnalysis.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script SingleDatasetAnalysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0570b6a7-1c21-4cd7-98b9-5d9a87f52c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from Bio import Entrez\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Literal, Optional\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import Counter\n",
    "import statistics\n",
    "import subprocess\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import tempfile\n",
    "\n",
    "\n",
    "# Prepare .env file\n",
    "\n",
    "load_dotenv('../../.env') # [HARDCODED]\n",
    "\n",
    "Entrez.email = os.getenv('ENTREZ_EMAIL')\n",
    "Entrez.api_key = os.getenv('ENTREZ_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Prepare OpenAI API Client\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=openai_api_key,  # this is also the default, it can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65020cbb-eb3f-4546-a6cd-3e3a4b8dbf88",
   "metadata": {},
   "source": [
    "# Part 1 - Dataset identification\n",
    "\n",
    "The goal from this section is to identify the datasets that will be analysed. \n",
    "\n",
    "Keep in mind - for this notebook, I will only be analysing one dataset.\n",
    "\n",
    "## Specific approach\n",
    "- A user query is given. This is customizable, and will be what the user supplies themselves (i.e. the question they are interested in)\n",
    "- Based on this user query, appropriate search terms are identified using AI. Perhaps an option to specify how many iterations are performed?\n",
    "- Datasets are extracted from this. For the moment, I only extract the first 20 datasets (hardcoded). I suppose this could be a parameter?\n",
    "- Based on information from the extracted datasets, the relevance of the datasets to the research question is determined. This is performed three times. It'd be good to specify how many iterations are performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23e676e6-6d8e-4a54-8224-ce06afcdc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the initial search queries\n",
    "\n",
    "# Define some initial variables for demonstration purposes [HARDCODED]\n",
    "\n",
    "user_query = \"Identify datasets and samples which are relevant to exploring immunotherapies for lung cancer\"\n",
    "num_queries = 3\n",
    "\n",
    "# \n",
    "\n",
    "# Prepare functions for term extraction from research query\n",
    "\n",
    "# Prepare output structures\n",
    "## For term extraction from user query\n",
    "class ExtractedTerms(BaseModel):\n",
    "    extracted_terms: List[str] = Field(description=\"List of terms extracted from the query\")\n",
    "    expanded_terms: List[str] = Field(description=\"List of related terms generated from the extracted terms\")\n",
    "\n",
    "## For determining dataset relevance\n",
    "class Assessment(BaseModel):\n",
    "    ID: str\n",
    "    RelevanceScore: int = Field(description=\"Score from 0 to 10, indicating relevance\")\n",
    "    Justification: str = Field(description=\"A brief explanation for the score\")\n",
    "\n",
    "class Assessments(BaseModel):\n",
    "    assessments: List[Assessment]\n",
    "\n",
    "# Define function for term extraction \n",
    "def extract_terms(user_query: str) -> List[str]:\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "## IDENTITY AND PURPOSE\n",
    "You are an expert in literature searches of biological ideas. Your task is to identify biological term(s) from a query, and generate related terms for the purposes of generating a search query. \n",
    "\n",
    "## STEPS\n",
    "\n",
    "- First, extract the biological term(s) from the input query. These should be specific and fall into one of the following categories:\n",
    "1. Genes - Examples: BRCA1, TP53\n",
    "2. Treatments/Methods - Examples: chemotherapy, CRISPR\n",
    "3. Tissues/Cells - Examples: lung, hepatocytes\n",
    "4. Diseases - Examples: Alzheimer's disease, lung cancer.\n",
    "\n",
    "Do not fabricate items if no relevant term exists. Avoid general terms such as \"disease\" or \"variant.\"\n",
    "\n",
    "- Second, for each extracted biological term, generate two related terms. Make a considered effort to keep these terms in the same category as the original term. These are examples of an identified term, and possible relevant terms:\n",
    "1. Genes: BRCA1 - Examples: BRCA2, oncogene\n",
    "2. Treatments: Chemotherapy - Examples: radiotherapy, monoclonal antibody\n",
    "3. Tissues/Cells: Lung - Examples: respiratory, alveoli\n",
    "4. Diseases: Alzheimer's disease - Examples: dementia, amyloid plaque\n",
    "\n",
    "## OUTPUT\n",
    "\n",
    "Provide two lists:\n",
    "1. Extracted terms: The primary terms identified directly from the query.\n",
    "2. Expanded terms: The related terms generated from the extracted terms.\n",
    "Do not include categories or justifications.\n",
    "\n",
    "## INPUT\n",
    "User query: {user_query}\"\"\"\n",
    "    \n",
    "    extracted_terms = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=1,\n",
    "        response_format=ExtractedTerms,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    extracted_terms = extracted_terms.choices[0].message.parsed\n",
    "\n",
    "#    print(f\"Raw extracted terms: {extracted_terms.extracted_terms}\")\n",
    "#    print(f\"Raw expanded terms: {extracted_terms.expanded_terms}\")\n",
    "    \n",
    "    all_terms = extracted_terms.extracted_terms + extracted_terms.expanded_terms\n",
    "    terms_with_filter = [term + ' AND \"gse\"[Filter]' for term in all_terms]\n",
    "    return terms_with_filter\n",
    "\n",
    "# Extension - define function to perform term extraction multiple times\n",
    "async def extract_terms_multiple(user_query: str, num_queries: int = 3) -> List[str]:\n",
    "    async def single_extract():\n",
    "        return extract_terms(user_query)\n",
    "    \n",
    "    tasks = [single_extract() for _ in range(num_queries)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Flatten the list of lists and remove duplicates\n",
    "    all_terms = list(set([term for sublist in results for term in sublist]))\n",
    "    return all_terms\n",
    "\n",
    "# Define function for performing search\n",
    "def perform_search(term):\n",
    "    search_handle = Entrez.esearch(db=\"gds\", term=term, retmode=\"xml\", retmax = 50) # CHANGE\n",
    "    search_results = Entrez.read(search_handle)\n",
    "    search_handle.close()\n",
    "    return search_results\n",
    "\n",
    "# Define function for extracting information from above search results\n",
    "def extract_geo_info_batch(geo_ids):\n",
    "    \"\"\"\n",
    "    Retrieve GEO information for a batch of GEO IDs.\n",
    "    \"\"\"\n",
    "    ids_str = \",\".join(geo_ids)\n",
    "    handle = Entrez.esummary(db=\"gds\", id=ids_str, retmode=\"xml\")\n",
    "    output = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    data = []\n",
    "    for geo_id, geo_data in zip(geo_ids, output):\n",
    "        if isinstance(geo_data, dict):\n",
    "            data.append({\n",
    "                'ID': geo_id,\n",
    "                'Title': geo_data.get('title', 'No title available'),\n",
    "                'Summary': geo_data.get('summary', 'No summary available'),\n",
    "                'Accession': geo_data.get('Accession', 'No accession available'),\n",
    "                'Species': geo_data.get('taxon', 'No taxon available'),\n",
    "                'Date': geo_data.get('PDAT', 'Date made public unknown')\n",
    "            })\n",
    "        else:\n",
    "            data.append({'ID': geo_id, 'Title': 'Error', 'Summary': 'Unable to fetch data', 'Accession': 'Error'})\n",
    "\n",
    "    return data\n",
    "\n",
    "def create_geo_dataframe(geo_ids, batch_size=10):\n",
    "    \"\"\"Create a DataFrame from GEO search results using batch processing.\"\"\"\n",
    "    data = []\n",
    "    for i in tqdm(range(0, len(geo_ids), batch_size), desc=\"Processing GEO IDs in batches\"):\n",
    "        batch_ids = geo_ids[i:i + batch_size]\n",
    "        data.extend(extract_geo_info_batch(batch_ids))\n",
    "        time.sleep(0.2)  # Be nice to NCBI servers\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Define function for determining relevance of datasets\n",
    "def assess_relevance_batch(df, query, batch_size=10):\n",
    "    results = []\n",
    "    total_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Determining dataset relevance\", total=total_batches):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        prompt = f\"\"\"\n",
    "## IDENTITY AND PURPOSE\n",
    "\n",
    "You are a highly knowledgeable biologist tasked with identifying relevant datasets for a given research query. Your goal is to assess NCBI GEO datasets based on their titles and summaries, and determine their relevance to the research question at hand.\n",
    "\n",
    "## STEPS\n",
    "\n",
    "1. For each dataset, carefully analyze the provided title and summary.\n",
    "2. Extract ALL biological concepts represented in the dataset, including but not limited to:\n",
    "   - Genes and variants investigated (e.g., p53, BRCA1)\n",
    "   - Species studied (e.g., Homo sapiens, Escherichia coli)\n",
    "   - Sample sources (e.g., organoid cultures, human samples)\n",
    "   - Diseases or phenotypes studied (e.g., Alzheimer's disease, lung cancer)\n",
    "   - Cell types or tissues examined (e.g., lung tissue, neural progenitor cells)\n",
    "   - Experimental techniques or methodologies used (e.g., RNA-seq, ChIP-seq)\n",
    "3. Extract ALL biological concepts represented in the research query using the same categories.\n",
    "4. Assign a relevance score from 0 to 10 in increments of 1, based solely on the provided information. \n",
    "   - Do not fabricate or assume information not explicitly stated about the dataset. \n",
    "   - If confirmed information about the dataset is POSSIBLY useful for the research question, view this favourably for determining dataset relevance. \n",
    "   - Note that the gene, disease, and cell type/tissue being studied are the most important in determining relevance. The other factors are considered minor aspects.\n",
    "   - Use the following scoring guide:\n",
    "\n",
    "   0: No relevance. All biological concepts (genes, species, samples, diseases, cell types, methods) are completely unrelated to the research query.\n",
    "   1: Minimal relevance. One minor aspect is loosely related, but the overall focus is different.\n",
    "   2: Low relevance. One major aspect aligns with the query, but other key elements differ significantly.\n",
    "   3: Somewhat low relevance. Two aspects align, but critical elements are still mismatched.\n",
    "   4: Moderate relevance. Multiple aspects align, but there are still significant differences in focus or approach.\n",
    "   5: Moderately relevant. Most major aspects align, but there are some notable differences that may limit direct applicability.\n",
    "   6: Relevant. All major aspects align, but there might be differences in specific genes, cell types, or methodologies that somewhat reduce direct applicability.\n",
    "   7: Highly relevant. Very close alignment in all major aspects, with only minor differences that don't significantly impact applicability.\n",
    "   8: Very highly relevant. Near-perfect alignment in all major aspects, with at most one or two minor differences.\n",
    "   9: Extremely relevant. Perfect alignment in all major aspects, with at most one negligible difference.\n",
    "   10: Perfectly relevant. The dataset appears to be an exact match for the research query in all aspects.\n",
    "\n",
    "5. Provide a brief justification (3-4 sentences) for the assigned score, highlighting key similarities and differences.\n",
    "\n",
    "## OUTPUT\n",
    "For each dataset, provide a JSON object with the ID, relevance score, and justification. \n",
    "- The relevance score should be a number, with no other information.\n",
    "- The justification should be a 4-5 sentence explanation for the relevance score. \n",
    "\n",
    "## HANDLING LIMITED INFORMATION\n",
    "If the dataset title or summary lacks sufficient detail:\n",
    "- Focus on the information that is available\n",
    "- Do not make assumptions about missing information\n",
    "- Assign a lower score if critical information is absent\n",
    "- Note the lack of information in the justification\n",
    "\n",
    "Remember, it's better to assign a lower score due to lack of information than to assume relevance without evidence.\n",
    "\n",
    "Given the following datasets and query, determine if each dataset is relevant.\n",
    "        Query: {query}\n",
    "        Datasets:\n",
    "        \"\"\"\n",
    "        for _, row in batch.iterrows():\n",
    "            prompt += f\"\"\"\n",
    "            ID: {row['ID']}\n",
    "            Title: {row['Title']}\n",
    "            Summary: {row['Summary']}\n",
    "            Species: {row['Species']}\n",
    "            \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.beta.chat.completions.parse(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.3,\n",
    "                response_format=Assessments,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \n",
    "                     \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=10000\n",
    "            )\n",
    "            response = response.choices[0].message.parsed\n",
    "            results.extend([assessment.dict() for assessment in response.assessments])\n",
    "        except Exception as e:\n",
    "            results.extend([{\"ID\": row['ID'], \"Relevance\": \"Error\", \"Justification\": str(e)} for _, row in batch.iterrows()])\n",
    "        time.sleep(1)  # Be nice to the API\n",
    "    return results\n",
    "\n",
    "# Extension - define function to assess relevance multiple times\n",
    "async def assess_relevance_batch_multiple(df, query, num_queries: int = 2, batch_size=20):\n",
    "    async def single_assess():\n",
    "        return assess_relevance_batch(df, query, batch_size)\n",
    "    \n",
    "    tasks = [single_assess() for _ in range(num_queries)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Collate results\n",
    "    collated_results = {}\n",
    "    for i, result_set in enumerate(results):\n",
    "        for assessment in result_set:\n",
    "            id = assessment['ID']\n",
    "            if id not in collated_results:\n",
    "                collated_results[id] = {'scores': [], 'justifications': []}\n",
    "            collated_results[id]['scores'].append(assessment['RelevanceScore'])\n",
    "            collated_results[id]['justifications'].append(assessment['Justification'])\n",
    "    \n",
    "    # Determine final relevance and format output\n",
    "    final_results = []\n",
    "    for id, data in collated_results.items():\n",
    "        mean_score = statistics.mean(data['scores'])\n",
    "        \n",
    "        result = {\n",
    "            'ID': id,\n",
    "            'RelevanceScore': round(mean_score, 1),\n",
    "        }\n",
    "        \n",
    "        # Add individual scores and justifications\n",
    "        for i in range(num_queries):\n",
    "            result[f'IndividualScore{i+1}'] = data['scores'][i] if i < len(data['scores']) else None\n",
    "            result[f'Justification{i+1}'] = data['justifications'][i] if i < len(data['justifications']) else None\n",
    "        \n",
    "        final_results.append(result)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "async def main(user_query):\n",
    "    # Extract terms\n",
    "    search_terms = await extract_terms_multiple(user_query)\n",
    "    print(\"Search terms:\", search_terms)\n",
    "\n",
    "    # Perform Entrez search and remove duplicates\n",
    "    geo_ids = set()  # Use a set to automatically remove duplicates\n",
    "    for term in search_terms:\n",
    "        search_results = perform_search(term)\n",
    "        geo_ids.update(search_results.get('IdList', []))  # Update the set with new IDs\n",
    "    if not geo_ids:\n",
    "        return pd.DataFrame({'Error': [\"No results found for the extracted terms\"]})\n",
    "\n",
    "    # Convert set back to list\n",
    "    geo_ids = list(geo_ids)[1:20] # for the moment only use a subset of the IDs [HARDCODED]\n",
    "\n",
    "    # Create DataFrame with GEO information\n",
    "    df = create_geo_dataframe(geo_ids)\n",
    "\n",
    "    # Assess relevance\n",
    "    relevance_results = await assess_relevance_batch_multiple(df, user_query, num_queries=num_queries) # Currently have this at 3\n",
    "    relevance_df = pd.DataFrame(relevance_results)\n",
    "\n",
    "    # Merge results\n",
    "    df['ID'] = df['ID'].astype(str)\n",
    "    relevance_df['ID'] = relevance_df['ID'].astype(str)\n",
    "    result_df = df.merge(relevance_df, on='ID', how='left')\n",
    "\n",
    "    # Dynamically create the desired order of columns\n",
    "    base_columns = ['ID', 'Title', 'Summary', 'Species', 'Accession', 'Date', 'RelevanceScore']\n",
    "    score_columns = [f'IndividualScore{i+1}' for i in range(num_queries)]\n",
    "    justification_columns = [f'Justification{i+1}' for i in range(num_queries)]\n",
    "    desired_order = base_columns + score_columns + justification_columns\n",
    "\n",
    "    # Reorder columns\n",
    "    result_df = result_df[desired_order]\n",
    "\n",
    "    # Reset index\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fa9bc86-052f-4473-a0d3-397d52c300ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search terms: ['lung cancer AND \"gse\"[Filter]', 'immunotherapies AND \"gse\"[Filter]', 'checkpoint inhibitors AND \"gse\"[Filter]', 'immune checkpoint therapy AND \"gse\"[Filter]', 'tumor microenvironment AND \"gse\"[Filter]', 'non-small cell lung cancer AND \"gse\"[Filter]', 'lung tumors AND \"gse\"[Filter]', 'lung carcinoma AND \"gse\"[Filter]', 'CAR T-cell therapy AND \"gse\"[Filter]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GEO IDs in batches: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]\n",
      "Determining dataset relevance: 100%|██████████| 1/1 [00:09<00:00,  9.50s/it]\n",
      "Determining dataset relevance: 100%|██████████| 1/1 [00:09<00:00,  9.08s/it]\n",
      "Determining dataset relevance: 100%|██████████| 1/1 [00:09<00:00,  9.11s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Species</th>\n",
       "      <th>Accession</th>\n",
       "      <th>Date</th>\n",
       "      <th>RelevanceScore</th>\n",
       "      <th>IndividualScore1</th>\n",
       "      <th>IndividualScore2</th>\n",
       "      <th>IndividualScore3</th>\n",
       "      <th>Justification1</th>\n",
       "      <th>Justification2</th>\n",
       "      <th>Justification3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200280853</td>\n",
       "      <td>Evolutionary fingerprint in rodent PD1 confers...</td>\n",
       "      <td>Mechanistic understanding of the immune checkp...</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>GSE280853</td>\n",
       "      <td>2024/11/05</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>This dataset focuses on the PD1 immune checkpo...</td>\n",
       "      <td>The dataset focuses on the PD1 immune checkpoi...</td>\n",
       "      <td>This dataset focuses on PD1 in a mouse melanom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200274509</td>\n",
       "      <td>Consequences of the perivascular niche remodel...</td>\n",
       "      <td>We investigated the mechanisms by which inflam...</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>GSE274509</td>\n",
       "      <td>2024/10/15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>While this dataset investigates T cell traffic...</td>\n",
       "      <td>This dataset investigates T-cell trafficking i...</td>\n",
       "      <td>The study examines T-cell trafficking in ovari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200241822</td>\n",
       "      <td>miRNA array data of EZH2-knockdown small cell ...</td>\n",
       "      <td>Enhancer of zeste homolog 2 (EZH2) is highly e...</td>\n",
       "      <td>Homo sapiens; synthetic construct</td>\n",
       "      <td>GSE241822</td>\n",
       "      <td>2024/10/23</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>This dataset examines small cell lung cancer (...</td>\n",
       "      <td>This dataset examines small cell lung cancer (...</td>\n",
       "      <td>This dataset investigates EZH2 in small cell l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200249316</td>\n",
       "      <td>High-specificity CRISPR-mediated genome engine...</td>\n",
       "      <td>Allogeneic chimeric antigen receptor (CAR)-T c...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE249316</td>\n",
       "      <td>2024/02/12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>This dataset discusses CAR-T cell therapy in t...</td>\n",
       "      <td>This dataset discusses CAR-T cell therapy in m...</td>\n",
       "      <td>This dataset discusses CAR-T cell therapy in m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200262592</td>\n",
       "      <td>single cell-RNA sequencing for Immune cells fr...</td>\n",
       "      <td>To evaluate the influence of ablating Fcgr2b o...</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>GSE262592</td>\n",
       "      <td>2024/11/06</td>\n",
       "      <td>5.3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>This dataset involves immune cells from a mous...</td>\n",
       "      <td>This dataset involves immune cells from a mous...</td>\n",
       "      <td>This dataset involves immune cells from a mous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200279137</td>\n",
       "      <td>T Cell Receptor-mimic STAR Conveys Superior Se...</td>\n",
       "      <td>Targeting tumor-specific neoantigens is promis...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE279137</td>\n",
       "      <td>2024/10/31</td>\n",
       "      <td>6.3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>This dataset discusses T cell receptor-mimic t...</td>\n",
       "      <td>This dataset discusses T cell receptor-mimic S...</td>\n",
       "      <td>This dataset discusses T cell receptor-mimic S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200274865</td>\n",
       "      <td>Epigenetic regulators of clonal hematopoiesis ...</td>\n",
       "      <td>Epigenetic reinforcement of T cell exhaustion ...</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>GSE274865</td>\n",
       "      <td>2024/10/10</td>\n",
       "      <td>5.3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>This dataset explores epigenetic regulators in...</td>\n",
       "      <td>This dataset investigates epigenetic regulator...</td>\n",
       "      <td>This study investigates epigenetic regulators ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200279637</td>\n",
       "      <td>Senescent lung fibroblasts in idiopathic pulmo...</td>\n",
       "      <td>Lung cancer is a fatal complication of idiopat...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE279637</td>\n",
       "      <td>2024/10/21</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>This dataset investigates the role of senescen...</td>\n",
       "      <td>This dataset directly investigates the role of...</td>\n",
       "      <td>This dataset explores the role of senescent lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200240490</td>\n",
       "      <td>Functional mapping of Glioblastoma recurrence ...</td>\n",
       "      <td>Resistance to genotoxic therapies and tumor re...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE240490</td>\n",
       "      <td>2024/08/30</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>This dataset focuses on glioblastoma and its r...</td>\n",
       "      <td>This dataset focuses on glioblastoma recurrenc...</td>\n",
       "      <td>This dataset focuses on glioblastoma and its r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200274477</td>\n",
       "      <td>Type I interferon signaling pathway enhances i...</td>\n",
       "      <td>Lung cancer is the leading cause of cancer mor...</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>GSE274477</td>\n",
       "      <td>2024/08/12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>This dataset discusses the type I interferon s...</td>\n",
       "      <td>This dataset explores the type I interferon si...</td>\n",
       "      <td>This dataset investigates the type I interfero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200274352</td>\n",
       "      <td>Type I interferon signaling pathway enhances i...</td>\n",
       "      <td>Lung cancer is the leading cause of cancer mor...</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>GSE274352</td>\n",
       "      <td>2024/08/12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Similar to the previous dataset, this one also...</td>\n",
       "      <td>Similar to the previous dataset, this one also...</td>\n",
       "      <td>Similar to the previous dataset, this one also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200246354</td>\n",
       "      <td>GCN2 is a determinant of the response to WEE1 ...</td>\n",
       "      <td>Patients with small-cell lung cancer (SCLC) ar...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE246354</td>\n",
       "      <td>2024/07/16</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>This dataset investigates GCN2's role in small...</td>\n",
       "      <td>This dataset looks at GCN2's role in SCLC resp...</td>\n",
       "      <td>This dataset discusses GCN2's role in small-ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200275929</td>\n",
       "      <td>Immune niches in brain metastases contain TCF1...</td>\n",
       "      <td>We analyzed immune infiltration in 67 brain me...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE275929</td>\n",
       "      <td>2024/09/15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>This dataset focuses on brain metastases and i...</td>\n",
       "      <td>This dataset analyzes immune niches in brain m...</td>\n",
       "      <td>This dataset analyzes immune niches in brain m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200263932</td>\n",
       "      <td>Selective CAR-T cell mediated B cell depletion...</td>\n",
       "      <td>Applying advanced molecular profiling together...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE263932</td>\n",
       "      <td>2024/05/13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>This dataset pertains to systemic lupus erythe...</td>\n",
       "      <td>This dataset focuses on CAR-T cell therapy in ...</td>\n",
       "      <td>This dataset focuses on CAR-T cell therapy in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200236654</td>\n",
       "      <td>Disclosing potential therapeutic targets assoc...</td>\n",
       "      <td>Osimertinib, a third-generation epidermal grow...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE236654</td>\n",
       "      <td>2024/08/05</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>This dataset discusses osimertinib resistance ...</td>\n",
       "      <td>This dataset investigates osimertinib resistan...</td>\n",
       "      <td>This dataset investigates therapeutic targets ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200277290</td>\n",
       "      <td>A STAG2-PAXIP1/PAGR1 axis suppresses lung tumo...</td>\n",
       "      <td>The cohesin complex is a critical regulator of...</td>\n",
       "      <td>Mus musculus</td>\n",
       "      <td>GSE277290</td>\n",
       "      <td>2024/10/22</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>This dataset explores the STAG2-PAXIP1/PAGR1 a...</td>\n",
       "      <td>This dataset explores the role of STAG2 in lun...</td>\n",
       "      <td>This study explores the STAG2-PAXIP1/PAGR1 axi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200268004</td>\n",
       "      <td>Structural and molecular properties of inclusi...</td>\n",
       "      <td>Viral RNA synthesis of mononegaviruses occurs ...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE268004</td>\n",
       "      <td>2024/10/31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>This dataset focuses on mumps virus properties...</td>\n",
       "      <td>This dataset focuses on mumps virus inclusion ...</td>\n",
       "      <td>This dataset focuses on the mumps virus and it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200185208</td>\n",
       "      <td>Immunogenic Senescence Sensitizes Lung Cancer ...</td>\n",
       "      <td>The higher immunogenicity of tumors usually pr...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE185208</td>\n",
       "      <td>2024/10/03</td>\n",
       "      <td>8.3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>This dataset investigates immunogenic senescen...</td>\n",
       "      <td>This dataset investigates immunogenic senescen...</td>\n",
       "      <td>This dataset investigates immunogenic senescen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>200279774</td>\n",
       "      <td>Multi-omic analysis of dendritic cell populati...</td>\n",
       "      <td>The female genital tract (FGT) represents a co...</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>GSE279774</td>\n",
       "      <td>2024/11/08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>This dataset focuses on dendritic cell populat...</td>\n",
       "      <td>This dataset focuses on dendritic cell populat...</td>\n",
       "      <td>This dataset focuses on dendritic cells in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                              Title  \\\n",
       "0   200280853  Evolutionary fingerprint in rodent PD1 confers...   \n",
       "1   200274509  Consequences of the perivascular niche remodel...   \n",
       "2   200241822  miRNA array data of EZH2-knockdown small cell ...   \n",
       "3   200249316  High-specificity CRISPR-mediated genome engine...   \n",
       "4   200262592  single cell-RNA sequencing for Immune cells fr...   \n",
       "5   200279137  T Cell Receptor-mimic STAR Conveys Superior Se...   \n",
       "6   200274865  Epigenetic regulators of clonal hematopoiesis ...   \n",
       "7   200279637  Senescent lung fibroblasts in idiopathic pulmo...   \n",
       "8   200240490  Functional mapping of Glioblastoma recurrence ...   \n",
       "9   200274477  Type I interferon signaling pathway enhances i...   \n",
       "10  200274352  Type I interferon signaling pathway enhances i...   \n",
       "11  200246354  GCN2 is a determinant of the response to WEE1 ...   \n",
       "12  200275929  Immune niches in brain metastases contain TCF1...   \n",
       "13  200263932  Selective CAR-T cell mediated B cell depletion...   \n",
       "14  200236654  Disclosing potential therapeutic targets assoc...   \n",
       "15  200277290  A STAG2-PAXIP1/PAGR1 axis suppresses lung tumo...   \n",
       "16  200268004  Structural and molecular properties of inclusi...   \n",
       "17  200185208  Immunogenic Senescence Sensitizes Lung Cancer ...   \n",
       "18  200279774  Multi-omic analysis of dendritic cell populati...   \n",
       "\n",
       "                                              Summary  \\\n",
       "0   Mechanistic understanding of the immune checkp...   \n",
       "1   We investigated the mechanisms by which inflam...   \n",
       "2   Enhancer of zeste homolog 2 (EZH2) is highly e...   \n",
       "3   Allogeneic chimeric antigen receptor (CAR)-T c...   \n",
       "4   To evaluate the influence of ablating Fcgr2b o...   \n",
       "5   Targeting tumor-specific neoantigens is promis...   \n",
       "6   Epigenetic reinforcement of T cell exhaustion ...   \n",
       "7   Lung cancer is a fatal complication of idiopat...   \n",
       "8   Resistance to genotoxic therapies and tumor re...   \n",
       "9   Lung cancer is the leading cause of cancer mor...   \n",
       "10  Lung cancer is the leading cause of cancer mor...   \n",
       "11  Patients with small-cell lung cancer (SCLC) ar...   \n",
       "12  We analyzed immune infiltration in 67 brain me...   \n",
       "13  Applying advanced molecular profiling together...   \n",
       "14  Osimertinib, a third-generation epidermal grow...   \n",
       "15  The cohesin complex is a critical regulator of...   \n",
       "16  Viral RNA synthesis of mononegaviruses occurs ...   \n",
       "17  The higher immunogenicity of tumors usually pr...   \n",
       "18  The female genital tract (FGT) represents a co...   \n",
       "\n",
       "                              Species  Accession        Date  RelevanceScore  \\\n",
       "0                        Mus musculus  GSE280853  2024/11/05             1.3   \n",
       "1                        Mus musculus  GSE274509  2024/10/15             1.0   \n",
       "2   Homo sapiens; synthetic construct  GSE241822  2024/10/23             5.7   \n",
       "3                        Homo sapiens  GSE249316  2024/02/12             1.0   \n",
       "4                        Mus musculus  GSE262592  2024/11/06             5.3   \n",
       "5                        Homo sapiens  GSE279137  2024/10/31             6.3   \n",
       "6                        Mus musculus  GSE274865  2024/10/10             5.3   \n",
       "7                        Homo sapiens  GSE279637  2024/10/21             9.0   \n",
       "8                        Homo sapiens  GSE240490  2024/08/30             1.7   \n",
       "9                        Mus musculus  GSE274477  2024/08/12             8.0   \n",
       "10                       Mus musculus  GSE274352  2024/08/12             8.0   \n",
       "11                       Homo sapiens  GSE246354  2024/07/16             5.7   \n",
       "12                       Homo sapiens  GSE275929  2024/09/15             1.0   \n",
       "13                       Homo sapiens  GSE263932  2024/05/13             1.0   \n",
       "14                       Homo sapiens  GSE236654  2024/08/05             6.0   \n",
       "15                       Mus musculus  GSE277290  2024/10/22             6.7   \n",
       "16                       Homo sapiens  GSE268004  2024/10/31             1.0   \n",
       "17                       Homo sapiens  GSE185208  2024/10/03             8.3   \n",
       "18                       Homo sapiens  GSE279774  2024/11/08             1.0   \n",
       "\n",
       "    IndividualScore1  IndividualScore2  IndividualScore3  \\\n",
       "0                  1                 2                 1   \n",
       "1                  1                 1                 1   \n",
       "2                  5                 6                 6   \n",
       "3                  1                 1                 1   \n",
       "4                  6                 4                 6   \n",
       "5                  7                 5                 7   \n",
       "6                  6                 4                 6   \n",
       "7                  9                 9                 9   \n",
       "8                  1                 3                 1   \n",
       "9                  8                 8                 8   \n",
       "10                 8                 8                 8   \n",
       "11                 5                 6                 6   \n",
       "12                 1                 1                 1   \n",
       "13                 1                 1                 1   \n",
       "14                 5                 6                 7   \n",
       "15                 7                 7                 6   \n",
       "16                 1                 1                 1   \n",
       "17                 8                 8                 9   \n",
       "18                 1                 1                 1   \n",
       "\n",
       "                                       Justification1  \\\n",
       "0   This dataset focuses on the PD1 immune checkpo...   \n",
       "1   While this dataset investigates T cell traffic...   \n",
       "2   This dataset examines small cell lung cancer (...   \n",
       "3   This dataset discusses CAR-T cell therapy in t...   \n",
       "4   This dataset involves immune cells from a mous...   \n",
       "5   This dataset discusses T cell receptor-mimic t...   \n",
       "6   This dataset explores epigenetic regulators in...   \n",
       "7   This dataset investigates the role of senescen...   \n",
       "8   This dataset focuses on glioblastoma and its r...   \n",
       "9   This dataset discusses the type I interferon s...   \n",
       "10  Similar to the previous dataset, this one also...   \n",
       "11  This dataset investigates GCN2's role in small...   \n",
       "12  This dataset focuses on brain metastases and i...   \n",
       "13  This dataset pertains to systemic lupus erythe...   \n",
       "14  This dataset discusses osimertinib resistance ...   \n",
       "15  This dataset explores the STAG2-PAXIP1/PAGR1 a...   \n",
       "16  This dataset focuses on mumps virus properties...   \n",
       "17  This dataset investigates immunogenic senescen...   \n",
       "18  This dataset focuses on dendritic cell populat...   \n",
       "\n",
       "                                       Justification2  \\\n",
       "0   The dataset focuses on the PD1 immune checkpoi...   \n",
       "1   This dataset investigates T-cell trafficking i...   \n",
       "2   This dataset examines small cell lung cancer (...   \n",
       "3   This dataset discusses CAR-T cell therapy in m...   \n",
       "4   This dataset involves immune cells from a mous...   \n",
       "5   This dataset discusses T cell receptor-mimic S...   \n",
       "6   This dataset investigates epigenetic regulator...   \n",
       "7   This dataset directly investigates the role of...   \n",
       "8   This dataset focuses on glioblastoma recurrenc...   \n",
       "9   This dataset explores the type I interferon si...   \n",
       "10  Similar to the previous dataset, this one also...   \n",
       "11  This dataset looks at GCN2's role in SCLC resp...   \n",
       "12  This dataset analyzes immune niches in brain m...   \n",
       "13  This dataset focuses on CAR-T cell therapy in ...   \n",
       "14  This dataset investigates osimertinib resistan...   \n",
       "15  This dataset explores the role of STAG2 in lun...   \n",
       "16  This dataset focuses on mumps virus inclusion ...   \n",
       "17  This dataset investigates immunogenic senescen...   \n",
       "18  This dataset focuses on dendritic cell populat...   \n",
       "\n",
       "                                       Justification3  \n",
       "0   This dataset focuses on PD1 in a mouse melanom...  \n",
       "1   The study examines T-cell trafficking in ovari...  \n",
       "2   This dataset investigates EZH2 in small cell l...  \n",
       "3   This dataset discusses CAR-T cell therapy in m...  \n",
       "4   This dataset involves immune cells from a mous...  \n",
       "5   This dataset discusses T cell receptor-mimic S...  \n",
       "6   This study investigates epigenetic regulators ...  \n",
       "7   This dataset explores the role of senescent lu...  \n",
       "8   This dataset focuses on glioblastoma and its r...  \n",
       "9   This dataset investigates the type I interfero...  \n",
       "10  Similar to the previous dataset, this one also...  \n",
       "11  This dataset discusses GCN2's role in small-ce...  \n",
       "12  This dataset analyzes immune niches in brain m...  \n",
       "13  This dataset focuses on CAR-T cell therapy in ...  \n",
       "14  This dataset investigates therapeutic targets ...  \n",
       "15  This study explores the STAG2-PAXIP1/PAGR1 axi...  \n",
       "16  This dataset focuses on the mumps virus and it...  \n",
       "17  This dataset investigates immunogenic senescen...  \n",
       "18  This dataset focuses on dendritic cells in the...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View results\n",
    "\n",
    "dataset_relevance_df = await (main(user_query))\n",
    "dataset_relevance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a5cc8-6f17-4fdd-b5c0-bc0e3950875e",
   "metadata": {},
   "source": [
    "# Part 2 - Data extraction\n",
    "\n",
    "In this part, relevant data is extracted out of the NCBI GEO datasets.\n",
    "\n",
    "For this notebook, I will extract data only out of the best scoring dataset (i.e. highest relevance).\n",
    "\n",
    "This part relies on scripts that I generated in a different notebook, so I will make sure to call those.\n",
    "\n",
    "## Specific approach:\n",
    "- Determine appropriate input parameters for script (i.e. dataset ID, output directory name, number of \"spots\"/reads)\n",
    "- Download the metadata associated with the dataset ID (which is a NCBI GEO ID)\n",
    "- Download FASTQ files associated with the dataset ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aaf48b09-fcef-4569-8c76-a7a2529970b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-13 05:20:37 - Processing GEO accession: GSE279637\n",
      "2024-11-13 05:20:37 - Output directory: GSE279637_data\n",
      "2024-11-13 05:20:37 - Force overwrite is enabled.\n",
      "2024-11-13 05:20:37 - Number of spots to download: 80000\n",
      "2024-11-13 05:20:37 - Starting metadata download using download_metadata.R\n",
      "2024-11-13 05:20:37 - Executing: Rscript \"/home/myuser/work/notebooks/Clean_Notebooks/OtherScripts/download_metadata.R\" --geo_accession 'GSE279637' --output_dir 'GSE279637_data'\n",
      "Using existing output directory: GSE279637_data\n",
      "Retrieving metadata for GEO accession: GSE279637\n",
      "Saving metadata to: GSE279637_data/GSE279637_series_matrix_metadata.csv\n",
      "Metadata downloaded and saved successfully!\n",
      "2024-11-13 05:21:08 - Metadata download completed.\n",
      "2024-11-13 05:21:08 - Starting FASTQ download using download_fastqs.sh\n",
      "2024-11-13 05:21:08 - Executing: \"/home/myuser/work/notebooks/Clean_Notebooks/OtherScripts/download_fastqs.sh\" --geo_accession 'GSE279637' --output_dir 'GSE279637_data' --num_spots 80000 --force\n",
      "Read 80000 spots for SRR31017114\n",
      "Written 80000 spots for SRR31017114\n",
      "Read 80000 spots for SRR31017115\n",
      "Written 80000 spots for SRR31017115\n",
      "Read 80000 spots for SRR31017116\n",
      "Written 80000 spots for SRR31017116\n",
      "Read 80000 spots for SRR31017117\n",
      "Written 80000 spots for SRR31017117\n",
      "Read 80000 spots for SRR31017118\n",
      "Written 80000 spots for SRR31017118\n",
      "Read 80000 spots for SRR31017119\n",
      "Written 80000 spots for SRR31017119\n",
      "2024-11-13 05:25:47 - FASTQ download completed.\n",
      "2024-11-13 05:25:47 - All processing completed successfully for GEO accession: GSE279637\n"
     ]
    }
   ],
   "source": [
    "# [HARDCODED] For the moment, we will begin by determining the single dataset that we should analyse\n",
    "\n",
    "top_accession = dataset_relevance_df.sort_values(by=\"RelevanceScore\", ascending=False).iloc[0][\"Accession\"]\n",
    "\n",
    "# We will then use this to determine input parameters. I think I am happy leaving these hardcoded.\n",
    "output_dir_name = top_accession + \"_data\"\n",
    "\n",
    "n_spots = 80000 # [HARDCODED]\n",
    "\n",
    "script_dir = \"OtherScripts\"  # Adjust if necessary\n",
    "\n",
    "# Construct the path to the process_geo.sh script\n",
    "script_path = os.path.join(script_dir, \"process_geo.sh\")\n",
    "\n",
    "# Run the subprocess\n",
    "result = subprocess.run([\n",
    "    script_path,\n",
    "    \"--geo_accession\", top_accession,\n",
    "    \"--output_dir\", output_dir_name,\n",
    "    \"--num_spots\", str(n_spots),\n",
    "    \"--force\"\n",
    "], check=True)  # check=True will raise an exception if the subprocess fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef89e41-f50d-47a8-bfbe-9c7540bc2685",
   "metadata": {},
   "source": [
    "Post performance notes:\n",
    "- There was a period of time where fastq-dump would not work (network issues - I could not isolate whether it was an issue on NCBI's end or my end). However, in other time periods it seems to work very well.\n",
    "- I will be using the sra_ids.txt file to link the various IDs and whatnot. This is necessary for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea91de-c0a6-41c2-9ee5-283565ce2ea4",
   "metadata": {},
   "source": [
    "# Part 3 - Data analysis\n",
    "\n",
    "Now that the data has been extracted, we will now want to perform the analysis. \n",
    "\n",
    "The specifics steps involved here are:\n",
    "## (3.1 - Kallisto quantification)\n",
    "- View the documentation\n",
    "- Identify the file locations (FASTQ files and Kallisto index files). In my original iteration, I had the file locations hardcoded, so I do need to determine how to resolve this... my vision was the have the AI determine appropriate values (that way I can rely on just a single function... (Hm, I think I can get away with the FASTQ files by specifying the output directory as defined earlier, but I don't have the same luxury for the index files. Perhaps I just search from the home directory...)\n",
    "- Identify sample metadata. I believe this is the SRA metadata... I need to iron this out, because this might be a duplicate (and perhaps it might be more suitable for the previous part...)\n",
    "- Get the study summary\n",
    "- Use the above information to determine the appropriate Kallisto parameters\n",
    "- Using these determined parameters, perform the Kallisto quantification\n",
    "\n",
    "## 3.2 - DEG analysis\n",
    "- Read in sample metadata (as extracted from before)\n",
    "- Identify location of abundance files\n",
    "- Determine appropriate contrasts from metadata, and structure this appropriately (i.e. compatible with makeContrasts)\n",
    "- Perform the DEG analysis, with the input files/contrasts\n",
    "\n",
    "Later - I would need an evaluation mechanism for.. pretty much every step. I'm hoping I can simply develop something which goes \"hey check these steps\" and can be flexible beyond that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "abdff8dc-f6e8-4637-955b-1b63f7f931a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by getting the documentation. This is necessary to ensure the OpenAI API knows the versions etc. that are being dealt with.\n",
    "\n",
    "def get_documentation(command):\n",
    "    try:\n",
    "        # Execute the kallisto command\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        \n",
    "        # Capture the stdout\n",
    "        stdout = result.stdout\n",
    "        \n",
    "        # Return the results\n",
    "        return stdout\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "kallisto_docs = get_documentation(\"kallisto quant --help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c48dec4-fc55-4ecb-8ae8-afc62b4c68a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key detected.\n",
      "\n",
      "Processing SRA ID: SRR31017114\n",
      "Executing command: esearch -db sra -query SRR31017114 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR31017114.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR31017115\n",
      "Executing command: esearch -db sra -query SRR31017115 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR31017115.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR31017116\n",
      "Executing command: esearch -db sra -query SRR31017116 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR31017116.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR31017117\n",
      "Executing command: esearch -db sra -query SRR31017117 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR31017117.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR31017118\n",
      "Executing command: esearch -db sra -query SRR31017118 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR31017118.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Processing SRA ID: SRR31017119\n",
      "Executing command: esearch -db sra -query SRR31017119 | efetch -format runinfo\n",
      "Successfully fetched data for SRA ID: SRR31017119.\n",
      "Sleeping for 0.1 seconds to respect rate limits.\n",
      "\n",
      "Data fetching complete.\n"
     ]
    }
   ],
   "source": [
    "# Now determine the file locations. This gets me the FASTQ files, but is also necessary to get the SRA IDs, which I use to extract the SRA metadata.\n",
    "\n",
    "def list_files(directory, suffix, exclude_hidden=True):\n",
    "    \"\"\"\n",
    "    Recursively lists all files in a given directory and its subdirectories that end with the specified suffix,\n",
    "    optionally excluding hidden files and directories, returning their absolute paths.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): The path to the directory to search in.\n",
    "    suffix (str): The file suffix to look for (e.g., 'fastq.gz').\n",
    "    exclude_hidden (bool): If True, hidden files and directories are excluded. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of absolute file paths that match the given suffix.\n",
    "    \"\"\"\n",
    "    matched_files = []\n",
    "    \n",
    "    try:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            if exclude_hidden:\n",
    "                # Skip hidden directories\n",
    "                dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "            \n",
    "            for f in files:\n",
    "                if exclude_hidden and f.startswith('.'):\n",
    "                    continue\n",
    "                if f.endswith(suffix):\n",
    "                    matched_files.append(os.path.join(root, f))\n",
    "                    \n",
    "        return matched_files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory '{directory}' not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "# First extract the FASTQ files. I can use the directory defined above to get these.\n",
    "\n",
    "# output_dir_name = top_accession + \"_data\" This is how I defined this earlier\n",
    "\n",
    "fastq_directory = output_dir_name\n",
    "fastq_suffix = \".fastq.gz\" # [HARDCODED] Hoping that I can automate this\n",
    "fastq_files = list_files(fastq_directory, fastq_suffix)\n",
    "\n",
    "# Next is the Kallisto indices...\n",
    "\n",
    "index_directory = \"/home/myuser/work/\" # [HARDCODED] .. but maybe this is ok...\n",
    "index_suffix = \".idx\" # [HARDCODED]\n",
    "index_files = list_files(index_directory, index_suffix)\n",
    "\n",
    "# Now we extract the study summary\n",
    "\n",
    "def get_study_summary(accession):\n",
    "\n",
    "    # Define the command as a string\n",
    "    command = (\n",
    "        f'esearch -db gds -query \"{accession}[ACCN]\" | '\n",
    "        'efetch -format docsum | '\n",
    "        'xtract -pattern DocumentSummarySet -block DocumentSummary '\n",
    "        f'-if Accession -equals {accession} -element summary'\n",
    "    )\n",
    "\n",
    "    # Execute the command\n",
    "    result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Check if the command was successful\n",
    "    if result.returncode == 0:\n",
    "        # Return the output\n",
    "        return result.stdout.strip()\n",
    "    else:\n",
    "        # Raise an error with the stderr output\n",
    "        raise Exception(f\"Error: {result.stderr}\")\n",
    "\n",
    "study_summary = get_study_summary(top_accession)\n",
    "\n",
    "# Now extract the SRA sample metadata...\n",
    "\n",
    "def fetch_sra_metadata_shell(\n",
    "    sra_ids_file,\n",
    "    entrez_api_key=None,\n",
    "    delay=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetches metadata for a list of SRA IDs using NCBI's esearch and efetch command-line tools.\n",
    "\n",
    "    Parameters:\n",
    "    - sra_ids_file (str): Path to the file containing SRA IDs with headers (tab-separated).\n",
    "    - entrez_api_key (str, optional): NCBI Entrez API key. If not provided, it will be read from the\n",
    "      'ENTREZ_API_KEY' environment variable.\n",
    "    - delay (float, optional): Delay in seconds between requests to respect rate limits.\n",
    "      If not provided, it defaults to 0.5 seconds without an API key and 0.1 seconds with an API key.\n",
    "    - verbose (bool, optional): If True, prints progress messages. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: Combined DataFrame containing metadata for all fetched SRA IDs.\n",
    "    \"\"\"\n",
    "    # Set Entrez API key from parameter or environment variable\n",
    "    if entrez_api_key is None:\n",
    "        entrez_api_key = os.getenv('ENTREZ_API_KEY')\n",
    "    \n",
    "    if entrez_api_key:\n",
    "        Entrez.api_key = entrez_api_key\n",
    "        if verbose:\n",
    "            print(\"API key detected.\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No API key detected; proceeding without it.\")\n",
    "    \n",
    "    # Set default delay based on API key presence\n",
    "    if delay is None:\n",
    "        delay = 0.1 if entrez_api_key else 0.5\n",
    "    \n",
    "    # List to store each SRA ID's fetched data as a DataFrame\n",
    "    data = []\n",
    "    \n",
    "    # Check if the SRA IDs file exists\n",
    "    if not os.path.isfile(sra_ids_file):\n",
    "        raise FileNotFoundError(f\"SRA IDs file '{sra_ids_file}' does not exist.\")\n",
    "    \n",
    "    # Open and read the SRA IDs file using csv.DictReader for tab-separated values\n",
    "    with open(sra_ids_file, 'r', newline='') as ids_file:\n",
    "        reader = csv.DictReader(ids_file, delimiter='\\t')\n",
    "        if 'SRA_ID' not in reader.fieldnames:\n",
    "            raise ValueError(\"Input file must contain a 'SRA_ID' column.\")\n",
    "        \n",
    "        for line_num, row in enumerate(reader, start=2):  # start=2 accounts for header\n",
    "            sra_id = row.get('SRA_ID', '').strip()\n",
    "            if not sra_id:\n",
    "                if verbose:\n",
    "                    print(f\"Line {line_num}: Missing 'SRA_ID'. Skipping.\")\n",
    "                continue  # Skip if SRA_ID is missing\n",
    "        \n",
    "            if verbose:\n",
    "                print(f\"\\nProcessing SRA ID: {sra_id}\")\n",
    "        \n",
    "            # Construct the command\n",
    "            command = f\"esearch -db sra -query {sra_id} | efetch -format runinfo\"\n",
    "        \n",
    "            if verbose:\n",
    "                print(f\"Executing command: {command}\")\n",
    "        \n",
    "            try:\n",
    "                # Execute the command\n",
    "                result = subprocess.run(\n",
    "                    command,\n",
    "                    shell=True,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    check=True\n",
    "                )\n",
    "        \n",
    "                # Check if output is not empty\n",
    "                if not result.stdout.strip():\n",
    "                    if verbose:\n",
    "                        print(f\"No data returned for SRA ID: {sra_id}.\")\n",
    "                    continue\n",
    "        \n",
    "                # Convert the CSV output to a DataFrame\n",
    "                csv_data = StringIO(result.stdout)\n",
    "                df = pd.read_csv(csv_data)\n",
    "                data.append(df)\n",
    "        \n",
    "                if verbose:\n",
    "                    print(f\"Successfully fetched data for SRA ID: {sra_id}.\")\n",
    "        \n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error processing {sra_id} on line {line_num}: {e}\")\n",
    "                print(f\"Command output: {e.output}\")\n",
    "                continue  # Skip to the next SRA ID if there’s an error\n",
    "        \n",
    "            # Respect API rate limits\n",
    "            if verbose:\n",
    "                print(f\"Sleeping for {delay} seconds to respect rate limits.\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # Combine all DataFrames into one\n",
    "    if data:\n",
    "        combined_df = pd.concat(data, ignore_index=True)\n",
    "        \n",
    "        # Remove columns where all entries are NaN\n",
    "        combined_df.dropna(axis=1, how='all', inplace=True)\n",
    "    \n",
    "        if verbose:\n",
    "            print(\"\\nData fetching complete.\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No data was fetched.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "sra_file = list_files(fastq_directory,\n",
    "                      \"sra_ids.txt\")\n",
    "\n",
    "sra_metadata = fetch_sra_metadata_shell(sra_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "42e397ab-0db9-4f46-a3c2-d7f0a7dc0aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:  830\n",
      "Prompt tokens:  3229\n",
      "Total tokens:  4059\n",
      "Generated tokens:  758\n",
      "Prompt tokens:  3229\n",
      "Total tokens:  3987\n",
      "Executing Kallisto command for GSE279637_data/SRR31017115_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output_SR31017115 -t 4 --plaintext --bootstrap-samples=100 GSE279637_data/SRR31017115_1.fastq.gz GSE279637_data/SRR31017115_2.fastq.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 227,665\n",
      "[index] number of k-mers: 139,900,295\n",
      "[index] number of D-list k-mers: 5,477,475\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: GSE279637_data/SRR31017115_1.fastq.gz\n",
      "                             GSE279637_data/SRR31017115_2.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 80,000 reads, 66,394 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 267.656\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 732 rounds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kallisto quantification completed for GSE279637_data/SRR31017115_1.fastq.gz\n",
      "\n",
      "Justification saved to output_SR31017115/justification.txt\n",
      "\n",
      "Executing Kallisto command for GSE279637_data/SRR31017116_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output_SR31017116 -t 4 --plaintext --bootstrap-samples=100 GSE279637_data/SRR31017116_1.fastq.gz GSE279637_data/SRR31017116_2.fastq.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 227,665\n",
      "[index] number of k-mers: 139,900,295\n",
      "[index] number of D-list k-mers: 5,477,475\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: GSE279637_data/SRR31017116_1.fastq.gz\n",
      "                             GSE279637_data/SRR31017116_2.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 80,000 reads, 66,663 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 270.031\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 894 rounds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kallisto quantification completed for GSE279637_data/SRR31017116_1.fastq.gz\n",
      "\n",
      "Justification saved to output_SR31017116/justification.txt\n",
      "\n",
      "Executing Kallisto command for GSE279637_data/SRR31017117_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output_SR31017117 -t 4 --plaintext --bootstrap-samples=100 GSE279637_data/SRR31017117_1.fastq.gz GSE279637_data/SRR31017117_2.fastq.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 227,665\n",
      "[index] number of k-mers: 139,900,295\n",
      "[index] number of D-list k-mers: 5,477,475\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: GSE279637_data/SRR31017117_1.fastq.gz\n",
      "                             GSE279637_data/SRR31017117_2.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 80,000 reads, 64,121 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 251.43\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 863 rounds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kallisto quantification completed for GSE279637_data/SRR31017117_1.fastq.gz\n",
      "\n",
      "Justification saved to output_SR31017117/justification.txt\n",
      "\n",
      "Executing Kallisto command for GSE279637_data/SRR31017118_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output_SR31017118 -t 4 --plaintext --bootstrap-samples=100 GSE279637_data/SRR31017118_1.fastq.gz GSE279637_data/SRR31017118_2.fastq.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 227,665\n",
      "[index] number of k-mers: 139,900,295\n",
      "[index] number of D-list k-mers: 5,477,475\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: GSE279637_data/SRR31017118_1.fastq.gz\n",
      "                             GSE279637_data/SRR31017118_2.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 80,000 reads, 64,309 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 256.576\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 829 rounds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kallisto quantification completed for GSE279637_data/SRR31017118_1.fastq.gz\n",
      "\n",
      "Justification saved to output_SR31017118/justification.txt\n",
      "\n",
      "Executing Kallisto command for GSE279637_data/SRR31017119_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output_SR31017119 -t 4 --plaintext --bootstrap-samples=100 GSE279637_data/SRR31017119_1.fastq.gz GSE279637_data/SRR31017119_2.fastq.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 227,665\n",
      "[index] number of k-mers: 139,900,295\n",
      "[index] number of D-list k-mers: 5,477,475\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: GSE279637_data/SRR31017119_1.fastq.gz\n",
      "                             GSE279637_data/SRR31017119_2.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 80,000 reads, 64,378 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 265.005\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 751 rounds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kallisto quantification completed for GSE279637_data/SRR31017119_1.fastq.gz\n",
      "\n",
      "Justification saved to output_SR31017119/justification.txt\n",
      "\n",
      "Executing Kallisto command for GSE279637_data/SRR31017114_1.fastq.gz:\n",
      "kallisto quant -i /home/myuser/work/data/kallisto_indices/human/index.idx -o output_SR31017114 -t 4 --plaintext --bootstrap-samples=100 GSE279637_data/SRR31017114_1.fastq.gz GSE279637_data/SRR31017114_2.fastq.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution will be estimated from the data\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 227,665\n",
      "[index] number of k-mers: 139,900,295\n",
      "[index] number of D-list k-mers: 5,477,475\n",
      "[quant] running in paired-end mode\n",
      "[quant] will process pair 1: GSE279637_data/SRR31017114_1.fastq.gz\n",
      "                             GSE279637_data/SRR31017114_2.fastq.gz\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 80,000 reads, 66,165 reads pseudoaligned\n",
      "[quant] estimated average fragment length: 287.005\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 753 rounds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kallisto quantification completed for GSE279637_data/SRR31017114_1.fastq.gz\n",
      "\n",
      "Justification saved to output_SR31017114/justification.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Above is getting a bit too chunky, so next section.\n",
    "\n",
    "# This is where I'll do some Kallisto stuff.\n",
    "\n",
    "class KallistoCommand(BaseModel):\n",
    "    index: str = Field(..., description=\"Filename for the Kallisto index to be used for quantification\")\n",
    "    fastq1: str = Field(..., description=\"Filename for the first FASTQ file (Read 1) to be quantified\")\n",
    "    fastq2: Optional[str] = Field(description=\"Filename for the second FASTQ file (Read 2) to be quantified (optional for single-end reads)\")\n",
    "    output: str = Field(..., description=\"Directory to write output to\")\n",
    "    bootstraps: int = Field(..., description=\"Number of bootstrap samples\")\n",
    "    single: bool = Field(..., description=\"If the reads are single-end\")\n",
    "    fr_stranded: bool = Field(..., description=\"If the reads are strand-specific, with first read forward\")\n",
    "    rf_stranded: bool = Field(..., description=\"If the reads are strand-specific, with first read reverse\")\n",
    "    frag_length: Optional[int] = Field(description=\"Estimated average fragment length (required for single-end reads)\")\n",
    "    sd: Optional[int] = Field(description=\"Estimated standard deviation of fragment length (required for single-end reads)\")\n",
    "    justification: str = Field(..., description=\"Justification for each chosen parameter, including if the parameter was excluded\")\n",
    "\n",
    "class KallistoCommands(BaseModel):\n",
    "    commands: List[KallistoCommand] = Field(description=\"List of Kallisto quantification commands for each sample\")\n",
    "\n",
    "def identify_kallisto_params():\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "## IDENTITY AND PURPOSE\n",
    "\n",
    "You are an expert in bioinformatic analyses. You will be provided with various pieces of information, and use this information to determine the appropriate parameters for a Kallisto analysis.\n",
    "\n",
    "## STEPS\n",
    "\n",
    "1. Carefully digest the contents of the provided Kallisto documentation. Note that any existing knowledge you have of Kallisto may not be correct, so follow the documentation closely.\n",
    "2. Carefully consider the contents of the sample metadata. Not all information will be relevant, however there will be content that will be needed.\n",
    "3. Carefully look through the dataset metadata. This may contain details that are useful.\n",
    "4. After considering all of the above, determine which Kallisto parameters should be set. Do not make any assumptions that are not explicitly stated for any optional fields. If unsure, leave blank.\n",
    "5. In determining parameters, make sure you only choose valid files (i.e. pick out of the options which are provided)\n",
    "6. Ensure that the chosen parameters allow for a robust analysis that would satisfy the most critical peer reviewers.\n",
    "7. You should prioritize scientific robustness over ease of computational burden.\n",
    "8. Note the following guidelines for some specific parameters:\n",
    "- the output directory should be named such that the sample being quantified can be identified from this output directory.\n",
    "\n",
    "## OUTPUT\n",
    "\n",
    "Your output should consist of each parameter, and either:\n",
    "- the value to be included for the parameter\n",
    "- if the parameter should not be included, you should state NA\n",
    "- For ALL chosen parameters, describe the justification for including the particular value, or excluding it.\n",
    "\n",
    "This should be applied to all parameters identified as per the provided Kallisto documentation.\n",
    "\n",
    "## INPUT\n",
    "\n",
    "Kallisto documentation: {kallisto_docs}\n",
    "\n",
    "Dataset summary: {study_summary}\n",
    "\n",
    "FASTQ files: {fastq_files}\n",
    "\n",
    "Possible Kallisto indices: {index_files}\n",
    "\n",
    "Sample metadata: {sra_metadata.to_json}\n",
    "\n",
    "\"\"\"\n",
    "    chat_completion = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format = KallistoCommands\n",
    "        )\n",
    "    result = chat_completion.choices[0].message.parsed\n",
    "    print(f\"Generated tokens: \", chat_completion.usage.completion_tokens)\n",
    "    print(f\"Prompt tokens: \", chat_completion.usage.prompt_tokens)\n",
    "    print(f\"Total tokens: \", chat_completion.usage.total_tokens)\n",
    "    return(result)\n",
    "\n",
    "kallisto_params = identify_kallisto_params()\n",
    "\n",
    "for cmd in kallisto_params.commands:\n",
    "    # Construct the Kallisto command string\n",
    "    kallisto_cmd = f\"kallisto quant -i {cmd.index} -o {cmd.output} -t 4\"\n",
    "    \n",
    "    if cmd.bootstraps > 0:\n",
    "        kallisto_cmd += f\" --bootstrap-samples={cmd.bootstraps}\"\n",
    "    \n",
    "    if cmd.single:\n",
    "        kallisto_cmd += \" --single\"\n",
    "        if cmd.frag_length:\n",
    "            kallisto_cmd += f\" -l {cmd.frag_length}\"\n",
    "        if cmd.sd:\n",
    "            kallisto_cmd += f\" -s {cmd.sd}\"\n",
    "    else:\n",
    "        # Paired-end\n",
    "        if cmd.fr_stranded:\n",
    "            kallisto_cmd += \" --fr-stranded\"\n",
    "        elif cmd.rf_stranded:\n",
    "            kallisto_cmd += \" --rf-stranded\"\n",
    "    \n",
    "    # Append FASTQ files\n",
    "    kallisto_cmd += f\" {cmd.fastq1} {cmd.fastq2}\"\n",
    "\n",
    "def execute_kallisto_commands(kallisto_commands: KallistoCommands):\n",
    "    for cmd in kallisto_commands.commands:\n",
    "        # Construct the Kallisto command string\n",
    "        kallisto_cmd = f\"kallisto quant -i {cmd.index} -o {cmd.output} -t 4 --plaintext\"\n",
    "        \n",
    "        if cmd.bootstraps > 0:\n",
    "            kallisto_cmd += f\" --bootstrap-samples={cmd.bootstraps}\"\n",
    "        \n",
    "        if cmd.single:\n",
    "            kallisto_cmd += \" --single\"\n",
    "            if cmd.frag_length:\n",
    "                kallisto_cmd += f\" -l {cmd.frag_length}\"\n",
    "            if cmd.sd:\n",
    "                kallisto_cmd += f\" -s {cmd.sd}\"\n",
    "        else:\n",
    "            # Paired-end\n",
    "            if cmd.fr_stranded:\n",
    "                kallisto_cmd += \" --fr-stranded\"\n",
    "            elif cmd.rf_stranded:\n",
    "                kallisto_cmd += \" --rf-stranded\"\n",
    "        \n",
    "        # Append FASTQ files\n",
    "        if cmd.fastq2 and cmd.fastq2.lower() != 'na':\n",
    "            kallisto_cmd += f\" {cmd.fastq1} {cmd.fastq2}\"\n",
    "        else:\n",
    "            kallisto_cmd += f\" {cmd.fastq1}\"\n",
    "        \n",
    "        print(f\"Executing Kallisto command for {cmd.fastq1}:\")\n",
    "        print(kallisto_cmd)\n",
    "\n",
    "        # Execute the command\n",
    "        try:\n",
    "            subprocess.run(kallisto_cmd, shell=True, check=True)\n",
    "            print(f\"Kallisto quantification completed for {cmd.fastq1}\\n\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing Kallisto for {cmd.fastq1}: {e}\\n\")\n",
    "        \n",
    "        # Optionally, log the justification\n",
    "        justification_path = os.path.join(cmd.output, \"justification.txt\")\n",
    "        os.makedirs(cmd.output, exist_ok=True)\n",
    "        with open(justification_path, \"w\") as f:\n",
    "            f.write(cmd.justification)\n",
    "        print(f\"Justification saved to {justification_path}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kallisto_commands = identify_kallisto_params()\n",
    "    execute_kallisto_commands(kallisto_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d81d93-e6b7-4e6c-bc96-2c270529c8d6",
   "metadata": {},
   "source": [
    "## Part 3b - Analysing the quantification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e51bb023-bd66-4f19-86f8-bda2d65b9e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:  92\n",
      "Prompt tokens:  1284\n",
      "Total tokens:  1376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ColumnMerging(merge=True, cols=['characteristics_ch1.4', 'treatment:ch1'], justification=\"Both 'characteristics_ch1.4' and 'treatment:ch1' contain the treatment status of the samples and provide scientifically valuable information about the experimental conditions. Merging these columns will allow for a clearer distinction between the different treatments (with versus without exosomes) in the analysis, thus leading to more meaningful biological interpretations.\")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by reading the metadata... I hope this won't lead to any complications...\n",
    "\n",
    "metadata_csv = list_files(directory = fastq_directory, \n",
    "                          suffix = \".csv\")\n",
    "df = pd.read_csv(metadata_csv[0])\n",
    "df = df.loc[:, df.nunique() > 1]\n",
    "metadata_json = df.to_json(orient='records', lines=False, indent=2) # parse to JSON\n",
    "\n",
    "# Quick inspection - will be interesting, because I previously had an example reliant on merging. This one does not. Will see if I've hardcoded\n",
    "\n",
    "class ColumnMerging(BaseModel):\n",
    "    merge: bool = Field(..., description=\"Whether or not columns should be merged\")\n",
    "    cols: Optional[list[str]] = Field(..., description=\"List of columns to be merged\")\n",
    "    justification: str = Field(..., description = \"Justification of columns being merged/why no columns needed to be merged\")\n",
    "\n",
    "def Identify_ColMerges():\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "### IDENTITY AND PURPOSE\n",
    "\n",
    "You are an expert in bioinformatics. You advise on the most scientifically valuable experiments that can be performed, and have a deep awareness of DEG analysis tools, such as limma and edgeR.\n",
    "\n",
    "Your task is to study the provided metadata, and determine which columns to use in proceeding with the analysis.\n",
    "\n",
    "### STEPS\n",
    "\n",
    "Note that a future step of the analysis will involve design of a matrix as follows:\n",
    "design <- model.matrix(data = DGE.final$samples,\n",
    "                       ~0 + column)\n",
    "\n",
    "Crucially, this only includes a single column. As such, if there are columns with DISTINCT SCIENTIFIC INFORMATION, these should be merged. Columns with similar information DO NOT need to be merged. Therefore, take a deep breath, and follow these steps to ensure that subsequent analyses are as robust as possible:\n",
    "\n",
    "1. Assess the content of each column in the provided metadata\n",
    "2. Determine which columns contain anything of biological relevance\n",
    "3. Determine if any columns are redundant, and do not need to be considered (e.g. similar content). In this case, only consider the column with simpler values (i.e. fewer special characters)\n",
    "4. Determine which columns contain information that would be scientifically valuable to analyse, i.e. could result in a meaningful biological finding.\n",
    "5. If there are multiple columns that contain scientifically valuable information, identify these columns as needing to be merged.\n",
    "6. If there is one one column containing scientifically valuable information, no columns need to be merged\n",
    "7. If you would be merging two redundant columns, these do not need to be merged. As such, no merge should occur (i.e. set merge to FALSE). Note that in this case, merging will COMPLICATE the analysis. Instead, IGNORING one of these columns is the best way to proceed.\n",
    "8. Be very aware that no merging can be perfectly viable. Do not force a suboptimal merge.\n",
    "\n",
    "Take into consideration that, suppose the values in one column are\n",
    "A\n",
    "B\n",
    "C\n",
    "\n",
    "And another column are \n",
    "1\n",
    "2\n",
    "3\n",
    "\n",
    "The merged output would be\n",
    "A_1\n",
    "B_2\n",
    "C_3 \n",
    "\n",
    "(or something comparable to that)\n",
    "\n",
    "### OUTPUT\n",
    "\n",
    "- Specify if any columns will need to be merged\n",
    "- State the names of the columns to be merged\n",
    "- Justify your choice\n",
    "\n",
    "### INPUT METADATA\n",
    "\n",
    "{metadata_json}\n",
    "\n",
    "\"\"\"\n",
    "    chat_completion = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format = ColumnMerging\n",
    "        )\n",
    "    result = chat_completion.choices[0].message.parsed\n",
    "    print(f\"Generated tokens: \", chat_completion.usage.completion_tokens)\n",
    "    print(f\"Prompt tokens: \", chat_completion.usage.prompt_tokens)\n",
    "    print(f\"Total tokens: \", chat_completion.usage.total_tokens)\n",
    "    return(result)\n",
    "\n",
    "col_merge_info = Identify_ColMerges()\n",
    "col_merge_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2c8e077-9d1d-4779-8860-ba3ea0855146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged columns ['characteristics_ch1.4', 'treatment:ch1'] into 'merged_analysis_group'.\n"
     ]
    }
   ],
   "source": [
    "# omg I actually give up. I think a completely different approach is needed. Now, I don't think this will change any of the results, however it is indicative of this not working to the standard I want.\n",
    "\n",
    "def clean_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean a string by normalizing special characters, replacing spaces with underscores,\n",
    "    and removing non-word characters.\n",
    "\n",
    "    Args:\n",
    "        s (str): The string to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned string.\n",
    "    \"\"\"\n",
    "    if pd.isnull(s):\n",
    "        return \"NA\"  # Handle missing values\n",
    "    s = str(s)\n",
    "    s = s.strip()  # Remove leading and trailing whitespaces\n",
    "    s = unidecode(s)  # Normalize special characters to ASCII\n",
    "    s = s.replace(\" \", \"_\")  # Replace spaces with underscores\n",
    "    s = re.sub(r'[^\\w]', '', s)  # Remove non-word characters (retain letters, digits, underscores)\n",
    "    return s\n",
    "\n",
    "def process_column_merging(df: pd.DataFrame, column_merge_info: ColumnMerging) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process column merging based on ColumnMerging information.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The sample metadata DataFrame.\n",
    "        column_merge_info (ColumnMerging): Information about column merging.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with merged columns if applicable.\n",
    "    \"\"\"\n",
    "    if column_merge_info.merge:\n",
    "        # Ensure that at least two columns are provided for merging\n",
    "        if not column_merge_info.cols or len(column_merge_info.cols) < 2:\n",
    "            raise ValueError(\"At least two columns must be specified for merging when merge=True.\")\n",
    "        \n",
    "        cols_to_merge = column_merge_info.cols\n",
    "\n",
    "        # Generate new column name by combining base names of the columns to merge\n",
    "        # For example, merging 'genotype:ch1' and 'treatment:ch1' becomes 'genotype_treatment_clean'\n",
    "        base_names = [col.split(\":\")[0] for col in cols_to_merge]\n",
    "        new_col_name = \"merged_analysis_group\"\n",
    "\n",
    "        # Clean the values in the columns to be merged\n",
    "        cleaned_columns = df[cols_to_merge].map(clean_string)\n",
    "\n",
    "        # Merge the cleaned columns by concatenating their values with underscores\n",
    "        df[new_col_name] = cleaned_columns.apply(lambda row: \"_\".join(row.values), axis=1)\n",
    "\n",
    "        print(f\"Merged columns {cols_to_merge} into '{new_col_name}'.\")\n",
    "    else:\n",
    "        # When merging is not required, ensure exactly one column is specified\n",
    "        if not column_merge_info.cols or len(column_merge_info.cols) != 1:\n",
    "            raise ValueError(\"Exactly one column must be specified for cleaning when merge=False.\")\n",
    "        \n",
    "        col_to_clean = column_merge_info.cols[0]\n",
    "\n",
    "        # Generate a new column name by appending '_clean' to the original column name\n",
    "        new_col_name = \"merged_analysis_group\"\n",
    "\n",
    "        # Rename the column in the DataFrame\n",
    "        df = df.rename(columns={col_to_clean: new_col_name})\n",
    "\n",
    "        # Clean the values in the renamed column\n",
    "        df[new_col_name] = df[new_col_name].apply(clean_string)\n",
    "\n",
    "        print(f\"Cleaned column '{col_to_clean}' into '{new_col_name}'.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "cleaned_metadata_df = process_column_merging(df, col_merge_info)\n",
    "cleaned_metadata_json = cleaned_metadata_df.to_json(orient='records', lines=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "35b623d4-a46e-47a0-bde3-56475a3e7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:  121\n",
      "Prompt tokens:  1845\n",
      "Total tokens:  1966\n"
     ]
    }
   ],
   "source": [
    "# Now to identify the contrasts...\n",
    "\n",
    "class Contrast(BaseModel):\n",
    "    name: str = Field(..., description = \"Name of contrast to perform\")\n",
    "    values: list[str] = Field(..., description = \"Values involved in analysis of the contrast\")\n",
    "    description: str = Field(..., description = \"Description of the contrast\")\n",
    "    justification: str = Field(..., description = \"Justification of why the contrast is of interest to analyse\")\n",
    "\n",
    "class AllAnalysisContrasts(BaseModel):\n",
    "    contrasts: list[Contrast]\n",
    "\n",
    "def IdentifyContrasts():\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "### IDENTITY AND PURPOSE\n",
    "\n",
    "You are an expert in bioinformatics. You advise on the most scientifically valuable experiments that can be performed, and have a deep awareness of DEG analysis tools, such as limma and edgeR.\n",
    "\n",
    "Your task is to study the provided information, and determine what contrasts would be interesting to study.\n",
    "\n",
    "### STEPS\n",
    "\n",
    "1. You will be given input sample metadata. The crux of the decision making should be based on this.\n",
    "2. You will be given some input information about a \"merged column\" called \"merged_analysis_group\". You should focus on the values in this column. However, the information will also detail where the merged values are derived from, so you can use this information as well.\n",
    "3. You will be provided information about the dataset summary. Use this to inform about the scientific purpose of the dataset.\n",
    "4. Having considered and digested the input information, carefully decide what the most valuable contrasts to analyse will be. Keep in mind the following guidelines:\n",
    "- The values you specify should be derived ONLY from the merged column\n",
    "- The contrasts you analyse should have scientific value, and not simply be \"control experiments\"\n",
    "- The contrasts should be focussed and have a clear defined purpose\n",
    "- Here are some examples of how to structure the contrasts:\n",
    "    - If the samples to be compared are, for example \"Treatment X vs. Y in genotpye A samples\", the output should be \"X_A, Y_A\" (where X_A refers to the EXACT value in the merged_analysis_group column)\n",
    "    - If the samples to be compared are, for example \"Treatment X vs. Y\", the output should be \"X_A, X_B, Y_A, Y_B\". \n",
    "5. Once you have produced the output, double check that:\n",
    "- You have considered the correct column\n",
    "- The values you have stated are derived from the correct column\n",
    "\n",
    "\n",
    "### OUTPUT\n",
    "\n",
    "- Assign a name for each contrast\n",
    "- State the values required to correctly analyse each contrast. These values must EXACTLY match the value in the merged_analysis_group column\n",
    "- Describe what the contrast is investigating\n",
    "- Justify why the contrast is scientifically valuable\n",
    "\n",
    "### INPUTS\n",
    "\n",
    "Sample metadata: {cleaned_metadata_json}\n",
    "Information about merged columns: {col_merge_info}\n",
    "Dataset summary: {study_summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    chat_completion = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format = AllAnalysisContrasts\n",
    "        )\n",
    "    result = chat_completion.choices[0].message.parsed\n",
    "    print(f\"Generated tokens: \", chat_completion.usage.completion_tokens)\n",
    "    print(f\"Prompt tokens: \", chat_completion.usage.prompt_tokens)\n",
    "    print(f\"Total tokens: \", chat_completion.usage.total_tokens)\n",
    "    return(result)\n",
    "\n",
    "contrasts_data = IdentifyContrasts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f95de346-b8ef-4db6-8e3d-40c18861eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:  44\n",
      "Prompt tokens:  480\n",
      "Total tokens:  524\n"
     ]
    }
   ],
   "source": [
    "# Generate expressions for these contrasts\n",
    "\n",
    "class Expressions(BaseModel):\n",
    "    name: str = Field(..., description = \"Name of contrast to perform\")\n",
    "    expressions: str = Field(..., description = \"Expressions representing contrasts\")\n",
    "\n",
    "class ContrastMatrix(BaseModel):\n",
    "    contrasts: list[Expressions]\n",
    "\n",
    "def GenerateContrastExpressions():\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "### IDENTITY AND PURPOSE\n",
    "\n",
    "You are an expert in bioinformatics. You advise on the most scientifically valuable experiments that can be performed, and have a deep awareness of DEG analysis tools, such as limma and edgeR.\n",
    "\n",
    "Your task is to study the provided information, and determine the epxressions to use to construct the contrast matrix.\n",
    "\n",
    "### STEPS\n",
    "\n",
    "1. You will be given input information about the contrasts to use. Make note of the description of the contrast, as well as the values\n",
    "2. For each suggested contrast, state a simple name to represent it (e.g. TreatmentInKO). The fewer characters the better, however it should still be informative.\n",
    "3. For each suggested contrast, use an expression to represent it. The expression must only use values, exactly as written, indicated in the information about contrasts. Note that this expression MUST be compatible with the makeContrasts function. See below for some examples:\n",
    "\"GNASknockout - WT\"\n",
    "\"(GNASknockout_A - GNASknockout_B) - (WT_A - WT_B)\"\n",
    "\n",
    "\n",
    "### OUTPUT\n",
    "\n",
    "- State a simple name for each contrast\n",
    "- State an appropriate expression for each contrast\n",
    "\n",
    "### INPUTS\n",
    "\n",
    "Contrast information: {contrasts_data}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    chat_completion = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format = ContrastMatrix\n",
    "        )\n",
    "    result = chat_completion.choices[0].message.parsed\n",
    "    print(f\"Generated tokens: \", chat_completion.usage.completion_tokens)\n",
    "    print(f\"Prompt tokens: \", chat_completion.usage.prompt_tokens)\n",
    "    print(f\"Total tokens: \", chat_completion.usage.total_tokens)\n",
    "    return(result)\n",
    "\n",
    "exprs = GenerateContrastExpressions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7667d1ad-f16a-4c0a-ac9f-ae146fdbc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The contrast (singular in this case, I would agree this is reasonable) to analyse has been determined, now to generate DGEList object to perform the DEG analysis.\n",
    "\n",
    "# Part 1 of this is to identify the input files that will be needed. This includes the metadata, linking samples to their abundance files, \n",
    "# the actual abundance files, transcript to gene text file\n",
    "\n",
    "# First the Kallisto and tx2gene files.\n",
    "\n",
    "abundance_directory = \".\" # [HARDCODED]\n",
    "abundance_suffix = \"abundance.tsv\" # [HARDCODED]\n",
    "abundance_files = list_files(abundance_directory, abundance_suffix) # just for my own sanity I didn't print the output, but I can see it was able to find all the files\n",
    "tx2gene_files = list_files(directory = \"/home/myuser/work\", # Ah. [HARDCODED]. This won't work well...\n",
    "                          suffix = \"t2g.txt\")\n",
    "\n",
    "SRA_IDs = pd.read_csv(sra_file[0], sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5ea760b2-e21a-45a4-8ef9-df45e3b3fa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:  162\n",
      "Prompt tokens:  374\n",
      "Total tokens:  536\n"
     ]
    }
   ],
   "source": [
    "class IDMatching(BaseModel):\n",
    "    SRA_ID: str = Field(..., description = \"Name of SRA ID\")\n",
    "    Kallisto_path: str = Field(..., description = \"Name of matching Kallisto path\")\n",
    "\n",
    "class AllIDMatches(BaseModel):\n",
    "    AllMatches: list[IDMatching]\n",
    "\n",
    "def Match_SRAIDs():\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "You will be given inputs for SRA IDs, as well as the path to abundance files generated from Kallisto. Your task is to generate 1-to-1 matches between abundance files and the SRA IDs.\n",
    "\n",
    "That is - for each SRA ID, identify the single path that is most likely to correspond to that SRA ID.\n",
    "\n",
    "Your output should consist of ONLY the SRA ID, and their matching Kallisto path. The output should match the input text EXACTLY with no other formatting included.\n",
    "\n",
    "### INPUTS\n",
    "\n",
    "SRA IDs: {SRA_IDs['SRA_ID']}\n",
    "Kallisto paths: {abundance_files}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    chat_completion = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format = AllIDMatches\n",
    "        )\n",
    "    result = chat_completion.choices[0].message.parsed\n",
    "    print(f\"Generated tokens: \", chat_completion.usage.completion_tokens)\n",
    "    print(f\"Prompt tokens: \", chat_completion.usage.prompt_tokens)\n",
    "    print(f\"Total tokens: \", chat_completion.usage.total_tokens)\n",
    "    return(result)\n",
    "\n",
    "# Now a commands to link these together...\n",
    "\n",
    "SRA_ID_links = Match_SRAIDs()\n",
    "SRA_IDs = pd.read_csv(sra_file[0], sep = '\\t')\n",
    "json_data = SRA_ID_links.json()\n",
    "all_id_matches = AllIDMatches.parse_raw(json_data)\n",
    "sra_to_kallisto = {match.SRA_ID: match.Kallisto_path for match in all_id_matches.AllMatches}\n",
    "SRA_IDs['Kallisto_path'] = SRA_IDs['SRA_ID'].map(sra_to_kallisto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c1dfc97e-6237-468d-9241-e306b8208f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated R Script:\n",
      "\n",
      "\n",
      "    library(tximport)\n",
      "    library(tidyverse)\n",
      "    library(edgeR)\n",
      "    \n",
      "    # Read tx2gene\n",
      "    tx2gene <- read_tsv(\"/home/myuser/work/data/kallisto_indices/human/t2g.txt\", col_names = FALSE) %>%\n",
      "      dplyr::select(1, 3) %>%\n",
      "      drop_na()\n",
      "    \n",
      "    # Define abundance files\n",
      "    files <- c(\"./output_SR31017114/abundance.tsv\", \"./output_SR31017115/abundance.tsv\", \"./output_SR31017119/abundance.tsv\", \"./output_SR31017117/abundance.tsv\", \"./output_SR31017116/abundance.tsv\", \"./output_SR31017118/abundance.tsv\")\n",
      "    \n",
      "    # Import data using tximport\n",
      "    kallisto <- tximport(files = files,\n",
      "                        type = \"kallisto\",\n",
      "                        tx2gene = tx2gene,\n",
      "                        ignoreAfterBar = TRUE,\n",
      "                        countsFromAbundance = \"lengthScaledTPM\")\n",
      "    \n",
      "    # Read metadata\n",
      "    meta <- read.csv(\"/tmp/tmpueq78k9j.csv\", row.names = 1)\n",
      "    \n",
      "    # Create DGEList\n",
      "    DGE <- DGEList(counts = kallisto$counts,\n",
      "                  samples = meta)\n",
      "    \n",
      "    keep.exprs <- filterByExpr(DGE, group = DGE$samples$merged_analysis_group)\n",
      "    DGE.filtered <- DGE[keep.exprs, keep.lib.sizes = FALSE]\n",
      "    print(dim(DGE.filtered))\n",
      "    # Normalize\n",
      "    DGE.final <- calcNormFactors(DGE.filtered)\n",
      "    \n",
      "    library(stringr)\n",
      "    \n",
      "    # Create design matrix using the specified grouping variable\n",
      "    design <- model.matrix(~0 + merged_analysis_group, data = DGE.final$samples)\n",
      "    \n",
      "    # Clean column names by removing the grouping variable string\n",
      "    colnames(design) <- str_remove_all(colnames(design), \"merged_analysis_group\")\n",
      "    \n",
      "    print(design)\n",
      "    \n",
      "    colnames(design)\n",
      "    # Create contrast matrix\n",
      "    contrast.matrix <- makeContrasts(\n",
      "      ExoEffect = \"treatment_incubated_with_exosomes_incubated_with_exosomes - treatment_incubated_without_exosomes_incubated_without_exosomes\",\n",
      "      levels = colnames(design)\n",
      "    )\n",
      "    \n",
      "    \n",
      "    # Optionally, you can proceed with fitting the model and other downstream analysis\n",
      "    v <- voom(DGE.final,\n",
      "          design)\n",
      "    vfit <- lmFit(v,\n",
      "              design)\n",
      "              \n",
      "    vfit <- contrasts.fit(vfit,\n",
      "                      contrast.matrix)\n",
      "                      \n",
      "    efit <- eBayes(vfit)\n",
      "\n",
      "    contrasts <- colnames(contrast.matrix)\n",
      "    \n",
      "    LFC.summary <- sapply(contrasts, function(x){\n",
      "    lfc.list <- list()\n",
      "    top <- topTable(efit,\n",
      "                    coef = x,\n",
      "                    number = Inf) %>%\n",
      "    list()\n",
      "    lfc.list <- append(lfc.list, top)\n",
      "    })\n",
      "\n",
      "    saveRDS(LFC.summary, \"LFCs.RDS\")\n",
      "    \n",
      "Executing R script...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n",
      "✔ dplyr     1.1.4     ✔ readr     2.1.5\n",
      "✔ forcats   1.0.0     ✔ stringr   1.5.1\n",
      "✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n",
      "✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n",
      "✔ purrr     1.0.2     \n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n",
      "Loading required package: limma\n",
      "Rows: 227665 Columns: 8\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \"\\t\"\n",
      "chr (6): X1, X2, X3, X4, X5, X8\n",
      "dbl (2): X6, X7\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "Note: importing `abundance.h5` is typically faster than `abundance.tsv`\n",
      "reading in files with read_tsv\n",
      "1 2 3 4 5 6 \n",
      "transcripts missing from tx2gene: 24683\n",
      "summarizing abundance\n",
      "summarizing counts\n",
      "summarizing length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1419    6\n",
      "        treatment_incubated_with_exosomes_incubated_with_exosomes\n",
      "Sample1                                                         1\n",
      "Sample2                                                         1\n",
      "Sample3                                                         1\n",
      "Sample4                                                         0\n",
      "Sample5                                                         0\n",
      "Sample6                                                         0\n",
      "        treatment_incubated_without_exosomes_incubated_without_exosomes\n",
      "Sample1                                                               0\n",
      "Sample2                                                               0\n",
      "Sample3                                                               0\n",
      "Sample4                                                               1\n",
      "Sample5                                                               1\n",
      "Sample6                                                               1\n",
      "attr(,\"assign\")\n",
      "[1] 1 1\n",
      "attr(,\"contrasts\")\n",
      "attr(,\"contrasts\")$merged_analysis_group\n",
      "[1] \"contr.treatment\"\n",
      "\n",
      "[1] \"treatment_incubated_with_exosomes_incubated_with_exosomes\"      \n",
      "[2] \"treatment_incubated_without_exosomes_incubated_without_exosomes\"\n",
      "R script executed successfully.\n"
     ]
    }
   ],
   "source": [
    "# With this being done, now I prepare the reading of the files and the execution of the R script\n",
    "\n",
    "\n",
    "# Paths and data (Assuming these are defined elsewhere in your code)\n",
    "tx2gene_path = tx2gene_files[1] # [HARDCODED] ... I think I need this as an LLM prompt to determine which path to use... \n",
    "analysis_group = \"merged_analysis_group\" # [HARDCODED] ...\n",
    "\n",
    "# Export metadata to a temporary CSV file for R to read\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8') as tmp_meta:\n",
    "    metadata_path = tmp_meta.name\n",
    "    linked_data.to_csv(metadata_path, index=False)\n",
    "\n",
    "# Create a temporary R script file\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.R', encoding='utf-8') as tmp_r_script:\n",
    "    r_script_path = tmp_r_script.name\n",
    "\n",
    "    # Start constructing the R script\n",
    "    r_script = f\"\"\"\n",
    "    library(tximport)\n",
    "    library(tidyverse)\n",
    "    library(edgeR)\n",
    "    \n",
    "    # Read tx2gene\n",
    "    tx2gene <- read_tsv(\"{tx2gene_path}\", col_names = FALSE) %>%\n",
    "      dplyr::select(1, 3) %>%\n",
    "      drop_na()\n",
    "    \n",
    "    # Define abundance files\n",
    "    files <- c({', '.join([f'\"{file}\"' for file in abundance_files])})\n",
    "    \n",
    "    # Import data using tximport\n",
    "    kallisto <- tximport(files = files,\n",
    "                        type = \"kallisto\",\n",
    "                        tx2gene = tx2gene,\n",
    "                        ignoreAfterBar = TRUE,\n",
    "                        countsFromAbundance = \"lengthScaledTPM\")\n",
    "    \n",
    "    # Read metadata\n",
    "    meta <- read.csv(\"{metadata_path}\", row.names = 1)\n",
    "    \n",
    "    # Create DGEList\n",
    "    DGE <- DGEList(counts = kallisto$counts,\n",
    "                  samples = meta)\n",
    "    \n",
    "    keep.exprs <- filterByExpr(DGE, group = DGE$samples${analysis_group})\n",
    "    DGE.filtered <- DGE[keep.exprs, keep.lib.sizes = FALSE]\n",
    "    print(dim(DGE.filtered))\n",
    "    # Normalize\n",
    "    DGE.final <- calcNormFactors(DGE.filtered)\n",
    "    \"\"\"\n",
    "\n",
    "    # Add Design Matrix Code\n",
    "    r_script += f\"\"\"\n",
    "    library(stringr)\n",
    "    \n",
    "    # Create design matrix using the specified grouping variable\n",
    "    design <- model.matrix(~0 + {analysis_group}, data = DGE.final$samples)\n",
    "    \n",
    "    # Clean column names by removing the grouping variable string\n",
    "    colnames(design) <- str_remove_all(colnames(design), \"{analysis_group}\")\n",
    "    \n",
    "    print(design)\n",
    "    \"\"\"\n",
    "\n",
    "    # Add Contrast Matrix Code\n",
    "    # Extract contrast names and expressions from exprs\n",
    "    contrast_entries = []\n",
    "    for contrast in exprs.dict()['contrasts']:\n",
    "        name = contrast['name']\n",
    "        expression = contrast['expressions']\n",
    "        # Escape double quotes in expressions\n",
    "        expression = expression.replace('\"', '\\\\\"')\n",
    "        contrast_entries.append(f'{name} = \"{expression}\"')\n",
    "\n",
    "    contrast_matrix_str = \",\\n  \".join(contrast_entries)\n",
    "\n",
    "    # Use single quotes in message to avoid conflicts with double quotes in contrast_matrix_str\n",
    "    r_script += f\"\"\"\n",
    "    colnames(design)\n",
    "    # Create contrast matrix\n",
    "    contrast.matrix <- makeContrasts(\n",
    "      {contrast_matrix_str},\n",
    "      levels = colnames(design)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Optionally, you can proceed with fitting the model and other downstream analysis\n",
    "    v <- voom(DGE.final,\n",
    "          design)\n",
    "    vfit <- lmFit(v,\n",
    "              design)\n",
    "              \n",
    "    vfit <- contrasts.fit(vfit,\n",
    "                      contrast.matrix)\n",
    "                      \n",
    "    efit <- eBayes(vfit)\n",
    "\n",
    "    contrasts <- colnames(contrast.matrix)\n",
    "    \n",
    "    LFC.summary <- sapply(contrasts, function(x){{\n",
    "    lfc.list <- list()\n",
    "    top <- topTable(efit,\n",
    "                    coef = x,\n",
    "                    number = Inf) %>%\n",
    "    list()\n",
    "    lfc.list <- append(lfc.list, top)\n",
    "    }})\n",
    "\n",
    "    saveRDS(LFC.summary, \"LFCs.RDS\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Write the complete R script to the temporary file\n",
    "    tmp_r_script.write(r_script)\n",
    "\n",
    "# Optional: Print the generated R script for debugging\n",
    "print(\"Generated R Script:\\n\")\n",
    "with open(r_script_path, 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Execute the R script\n",
    "try:\n",
    "    print(\"Executing R script...\")\n",
    "    subprocess.run([\"Rscript\", r_script_path], check=True)\n",
    "    print(\"R script executed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"An error occurred while executing the R script.\")\n",
    "    print(\"Error message:\")\n",
    "    print(e.stderr if e.stderr else e)\n",
    "finally:\n",
    "    # Clean up temporary files if desired\n",
    "    os.remove(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e464a-4f23-4856-b2b9-0fe5a4f612ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry)",
   "language": "python",
   "name": "poetry-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
