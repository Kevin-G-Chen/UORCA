# UORCA - Unified Omics Reference Corpus of Analyses

> Generated by Claude Code on 2025-10-01
> Last updated: 2025-10-01

## Project Overview

UORCA is a fully containerized, AI-powered workflow for automated RNA-seq analysis of public datasets from the Gene Expression Omnibus (GEO). It combines dataset discovery, batch processing, differential expression analysis, and interactive visualization with AI-powered interpretation.

**Type**: Python CLI tool for bioinformatics/RNA-seq analysis
**Primary Language**: Python 3.13+
**Package Manager**: uv

## Quick Start Commands

### Setup
```bash
# Install dependencies
uv sync

# Install in development mode
uv pip install -e .

# Configure environment variables (required)
cp .env.example .env
# Edit .env with your ENTREZ_EMAIL, OPENAI_API_KEY, and optionally ENTREZ_API_KEY
```

### Development
```bash
# The main CLI provides three subcommands:

# 1. Identify relevant datasets
uv run uorca identify -q "cancer stem cell differentiation" -o identification_results

# 2. Run RNA-seq analysis pipeline
uv run uorca run local --input identification_results/ --output_dir ../UORCA_results --max_workers 4
# OR for HPC clusters:
uv run uorca run slurm --input identification_results/ --output_dir ../UORCA_results

# 3. Explore results interactively
uv run uorca explore ../UORCA_results --port 8501
# Opens web browser automatically. For remote access via SSH:
# On HPC: uv run uorca explore ../UORCA_results --port 8501
# On laptop: ssh -L 8000:127.0.0.1:8501 username@hpc_server
# Browser: http://127.0.0.1:8000
```

### Testing
```bash
# Run all tests with coverage
uv run pytest

# Run specific test file
uv run pytest main_workflow/reporting/tests/unit/test_gene_selection.py

# Run specific test markers
uv run pytest -m unit        # Unit tests only
uv run pytest -m integration # Integration tests only
uv run pytest -m "not slow"  # Exclude slow tests

# Run with verbose output
uv run pytest -v

# Run with coverage report
uv run pytest --cov=main_workflow/reporting/core --cov-report=html
# View coverage: open htmlcov/index.html

# Watch mode (requires pytest-watch)
uv run pytest --watch
```

### Type Checking
```bash
# Full type check
uv run pyright

# Check specific files/directories
uv run pyright uorca/
uv run pyright main_workflow/agents/

# Verbose output for debugging
uv run pyright --verbose

# Watch mode
uv run pyright --watch
```

### Code Quality
```bash
# Run full quality check (recommended before committing)
uv run pytest && uv run pyright

# Note: No linting/formatting tools currently configured
# Consider adding ruff or black for future code formatting
```

### Build
```bash
# Install package locally
uv pip install -e .

# Build Docker container
docker build -t uorca:latest .

# Pull Singularity/Apptainer container (for HPC)
singularity pull uorca_0.1.0.sif docker://kevingchen/uorca:0.1.0
# OR
apptainer pull uorca_0.1.0.sif docker://kevingchen/uorca:0.1.0
```

## Code Conventions

### File Organization
- Main package code: `uorca/` (CLI interface, batch processing)
- Core workflow: `main_workflow/` (agents, dataset identification, reporting)
- Tests: `main_workflow/reporting/tests/` organized into `unit/` and `integration/`
- Prompts: `main_workflow/prompts/` (AI agent system prompts)
- Configuration: `pyproject.toml`, `pytest.ini`, `.env`
- Sample data: `sample_inputs/` (example datasets for testing)

### Naming Conventions
- Files and modules: `snake_case` (e.g., `gene_selection.py`, `ai_agent_factory.py`)
- Functions and methods: `snake_case` (e.g., `identify_frequent_degs()`, `fetch_geo_metadata()`)
- Classes: `PascalCase` (e.g., `ExtractionContext`, `DatasetIdentification`)
- Constants: `UPPER_SNAKE_CASE` (e.g., `ENTREZ_EMAIL`, `DEFAULT_THRESHOLD`)
- Test files: `test_*.py` (e.g., `test_gene_selection.py`)
- Test functions: `test_*` (e.g., `test_identify_frequent_degs()`)

### Import Patterns
```python
# Standard library imports first
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Optional

# Third-party imports second
import pandas as pd
import pytest
from pydantic_ai import Agent, RunContext

# Local imports last (relative or absolute)
from shared import ExtractionContext, RNAseqCoreContext
from core.gene_selection import identify_frequent_degs
```

### Error Handling
```python
# Use logging extensively
logger = logging.getLogger(__name__)

try:
    result = risky_operation()
    logger.info("Operation successful")
except SpecificError as e:
    logger.error(f"Operation failed: {e}")
    raise

# For AI agents, return structured error messages
return f"Error: {error_message}. Please check the logs."
```

### Testing Patterns
- Test file naming: `test_<module_name>.py`
- Test organization: Group related tests in test classes or use descriptive function names
- Fixtures: Define in `conftest.py` for shared test data
- Assertion style: Use pytest's native assertions (`assert x == y`)
- Markers: Use `@pytest.mark.unit`, `@pytest.mark.integration`, `@pytest.mark.slow`
- Coverage target: `main_workflow/reporting/core` directory

```python
# Example test structure
def test_function_name_describes_behavior(sample_fixture):
    """Test description explaining what is being tested."""
    result = function_under_test(sample_fixture)

    assert result == expected_value
    assert isinstance(result, ExpectedType)
```

## Development Workflow

### Recommended Development Flow
1. **Understand the task**: Read relevant code and documentation
2. **Plan the implementation**: Break down into smaller steps
3. **Write tests first** (TDD recommended for core functionality)
4. **Implement the feature**: Follow existing code patterns
5. **Run tests frequently**: `uv run pytest` during development
6. **Type check**: `uv run pyright` to catch type errors early
7. **Test manually** if applicable (e.g., CLI commands, web interface)

### Git Workflow
- Main branch: `master`
- Feature branches: Use descriptive names (e.g., `ease-of-use`, `add-feature-x`)
- Commit messages: Clear, concise descriptions of changes
- Before committing: Run tests and type checking

### Before Committing
```bash
# Always run these before committing:
uv run pytest              # Ensure all tests pass
uv run pyright             # Ensure no type errors
```

## Checkpoint Strategy

UORCA uses a checkpoint system to track workflow progress:
- Checkpoints defined in `shared/__init__.py`
- Each major step (extraction, metadata, analysis) has a checkpoint
- Status values: `NOT_STARTED`, `IN_PROGRESS`, `COMPLETED`, `FAILED`
- Checkpoints enable resume/recovery from failures

When implementing changes:
1. Understand which checkpoints are affected
2. Update checkpoint status appropriately
3. Test checkpoint progression with sample datasets

## Common Tasks

### Adding a New CLI Command
1. Define command parser in `uorca/cli.py`
2. Create command handler function (e.g., in `uorca/<command>.py`)
3. Add command logic using existing patterns
4. Test with `uv run uorca <command> --help`
5. Add tests for the command logic

### Adding a New Agent Tool
1. Define tool function with `@agent.tool` decorator
2. Use `@log_tool` for automatic logging
3. Update context with results using `ctx.deps`
4. Return clear success/error messages
5. Test tool independently if possible

### Adding a New Streamlit Tab
1. Create new file in `main_workflow/reporting/streamlit_tabs/`
2. Define tab rendering function
3. Use fragment isolation (`@st.fragment`) for performance
4. Import and integrate in `uorca_explorer.py`
5. Test with `uv run uorca explore <results_dir>`

### Running Tests for Specific Functionality
```bash
# Test gene selection module
uv run pytest main_workflow/reporting/tests/unit/test_gene_selection.py

# Test with coverage for specific module
uv run pytest main_workflow/reporting/tests/unit/test_gene_selection.py --cov=core.gene_selection

# Integration tests only
uv run pytest -m integration
```

### Debugging
- Log files: `logs/` directory (AI tool calls, dataset identification, etc.)
- Streamlit logs: Check terminal output when running `uv run uorca explore`
- Analysis logs: In output directory under `<GSE_ID>/logs/`
- Python debugger: Use `breakpoint()` for interactive debugging
- Verbose mode: Use `-v` flag for identify command

## Gotchas & Known Issues

### Environment Variables Required
**Problem**: UORCA requires `ENTREZ_EMAIL` and `OPENAI_API_KEY` to function.
**Solution**: Copy `.env.example` to `.env` and fill in your credentials. Without these, dataset identification and analysis will fail.

### NCBI Rate Limiting
**Problem**: NCBI API has rate limits (3 requests/second without API key).
**Solution**: Get a free NCBI API key from https://www.ncbi.nlm.nih.gov/account/settings/ and add as `ENTREZ_API_KEY` in `.env`. This increases the limit to 10 requests/second.

### Container vs Local Execution
**Problem**: Docker requires root privileges on HPC clusters, causing security issues.
**Solution**: Use Singularity/Apptainer instead (`singularity pull` or `apptainer pull`). These run without root and integrate with SLURM.

### Test Coverage Target
**Problem**: Coverage is only tracked for `main_workflow/reporting/core`.
**Solution**: This is intentional to focus on core business logic. Expand coverage configuration in `pytest.ini` if needed.

### Hardcoded Version Display
**Problem**: The UORCA Explorer summary tab displays version `1.0` (hardcoded).
**Solution**: Update version in `main_workflow/reporting/streamlit_tabs/uorca_summary_tab.py` if the app version changes.

## Performance Considerations

- **Caching**: Streamlit app uses extensive caching for expensive operations (gene selection, clustering)
- **Fragment isolation**: Use `@st.fragment` to update only parts of the Streamlit UI
- **Batch processing**: Use appropriate `--max_workers` for local execution based on CPU cores
- **SLURM resource management**: Configure in `slurm_config.yaml` for optimal HPC utilization
- **Dataset size**: Large datasets (>100 samples) may require more memory for analysis

## Security Notes

- **Never commit `.env` file**: Contains API keys and credentials
- **API key protection**: Keys should never appear in logs or error messages
- **Container security**: Singularity/Apptainer preferred over Docker for HPC (no root required)
- **Data privacy**: GEO data is public, but be mindful of institutional data policies

## External Dependencies

### Required Services
- **NCBI Entrez API**: For GEO/SRA metadata retrieval (requires `ENTREZ_EMAIL`)
- **OpenAI API**: For AI-powered dataset identification and analysis (requires `OPENAI_API_KEY`)

### Required Tools (for full pipeline)
- **Kallisto**: RNA-seq quantification (download indices with `./download_kallisto_indices.sh`)
- **R**: Differential expression analysis (with edgeR, limma, BiocManager)
- **SRA Toolkit**: FASTQ download (`prefetch`, `fasterq-dump`)

### Environment Variables
```bash
# Required
ENTREZ_EMAIL=your.email@institution.edu           # Your email (NCBI requirement)
OPENAI_API_KEY=sk-proj-your-key-here              # OpenAI API key

# Optional but recommended
ENTREZ_API_KEY=your-ncbi-api-key                  # NCBI API key (10x faster queries)
```

## Escalation Rules

When to stop and ask for help:
- [ ] Changes to AI agent system prompts (in `main_workflow/prompts/`)
- [ ] Modifications to checkpoint system or workflow orchestration
- [ ] Changes to R analysis scripts (`additional_scripts/RNAseq.R`)
- [ ] SLURM configuration or resource allocation changes
- [ ] Database schema or metadata format changes
- [ ] Container configuration changes affecting production deployments
- [ ] Breaking changes to CLI interface or API contracts
- [ ] Security-sensitive code (API key handling, authentication)

## Project-Specific AI Instructions

### Autonomous Operation
Safe to run without approval:
- Running tests (`uv run pytest`)
- Type checking (`uv run pyright`)
- Reading code files
- Searching codebase
- Running CLI commands with test data from `sample_inputs/`

Requires user approval:
- Creating or modifying `.env` file
- Running analysis on real GEO datasets (may incur API costs)
- Modifying system prompts for AI agents
- Making changes to SLURM configuration
- Deploying containers

### Code Analysis Tips
- Start with `AGENTS.md` for navigation guidance
- Core business logic is in `main_workflow/reporting/core/`
- AI agent definitions are in `main_workflow/agents/`
- Streamlit UI code is modular in `streamlit_tabs/`
- Use grep/search for specific functionality (codebase is well-organized)

### Testing Strategy
- Focus tests on `main_workflow/reporting/core/` modules
- Use fixtures in `conftest.py` for common test data
- Integration tests may be slow (use `-m "not slow"` to skip)
- Coverage reports help identify untested code paths

## Resources

- **GitHub Repository**: https://github.com/Kevin-G-Chen/UORCA
- **GEO Database**: https://www.ncbi.nlm.nih.gov/geo/
- **Kallisto Documentation**: https://pachterlab.github.io/kallisto/
- **pydantic-ai Documentation**: https://ai.pydantic.dev/
- **Streamlit Documentation**: https://docs.streamlit.io/

## Key Architecture Decisions

### Why pydantic-ai for agents?
- Type-safe AI interactions with structured outputs
- Built-in support for tools and context passing
- Excellent logging and debugging capabilities
- MCP server integration for advanced tool usage

### Why Streamlit for UI?
- Rapid prototyping of interactive interfaces
- Built-in caching and state management
- Easy deployment (single Python file)
- Great for scientific/data visualization apps

### Why uv for package management?
- Fast, reliable dependency resolution
- Modern Python tooling with great developer experience
- Virtual environment management built-in
- Compatible with standard Python packaging

### Why Singularity/Apptainer for HPC?
- Rootless container execution (security requirement for HPC)
- Native SLURM integration
- Better performance on HPC storage systems
- Converts Docker images seamlessly

## Updates

- 2025-10-01: Initial CLAUDE.md generated by Claude Code
- Track major changes here as the project evolves

---

*This file helps Claude Code maintain context across sessions. Update it when you discover new patterns, conventions, or project-specific knowledge.*
