# UORCA - Unified Omics Reference Corpus of Analyses

> Generated by Claude Code on 2025-10-01
> Last updated: 2025-10-01

## Project Overview

UORCA is a fully containerized, AI-powered workflow for automated RNA-seq analysis of public datasets from the Gene Expression Omnibus (GEO). It combines bioinformatics analysis with AI-driven dataset identification and interpretation.

**Type**: Bioinformatics CLI Tool + Streamlit Web Application
**Primary Language**: Python 3.13+
**Package Manager**: uv
**Container Runtime**: Docker (local) / Singularity/Apptainer (HPC)

## Quick Start Commands

### Setup
```bash
# Clone and install
git clone https://github.com/Kevin-G-Chen/UORCA.git
cd UORCA
uv sync

# Download Kallisto indices (required for RNA-seq)
./download_kallisto_indices.sh human  # Or: human, mouse, dog, monkey, zebrafish

# Setup environment variables
cp .env.example .env
# Edit .env with your API keys (ENTREZ_EMAIL, OPENAI_API_KEY, ENTREZ_API_KEY)
```

### Development
```bash
# Start UORCA Explorer (Streamlit web UI)
uv run uorca explore  # Runs on port 8501

# For development with auto-reload in background:
# Use Ctrl+b in Claude Code to run in background
uv run streamlit run main_workflow/reporting/uorca_explorer.py --server.runOnSave true

# Run specific workflow steps
uv run uorca identify -q "your research question"
uv run uorca run local --input datasets.csv --output results/
```

### Testing
```bash
# Run all tests
uv run pytest

# Run specific test file
uv run pytest main_workflow/reporting/tests/test_task_manager.py

# Run with coverage
uv run pytest --cov=main_workflow/reporting/core --cov-report=html

# Run only unit tests
uv run pytest -m unit

# Run only integration tests
uv run pytest -m integration

# Watch mode (requires pytest-watch)
uv run ptw
```

### Type Checking
```bash
# Full type check
uv run pyright

# Check specific module
uv run pyright main_workflow/reporting/core/

# Watch mode (if available)
uv run pyright --watch
```

### Code Quality
```bash
# Linting (if ruff/black configured)
# Currently: Manual code review recommended

# Format check
# Currently: Follow PEP 8 conventions

# Full quality check before commit
uv run pytest && uv run pyright
```

### Build
```bash
# No build step required for Python package
# For container:
docker pull kevingchen/uorca:0.1.0

# For Singularity/Apptainer (HPC):
singularity pull uorca_0.1.0.sif docker://kevingchen/uorca:0.1.0
```

## Code Conventions

### File Organization
- **Main modules**: `uorca/` - CLI commands (identify, run, explore)
- **Batch processing**: `uorca/batch/` - SLURM and local execution
- **Web interface**: `main_workflow/reporting/` - Streamlit application
- **Streamlit tabs**: `main_workflow/reporting/streamlit_tabs/` - UI components
- **Core utilities**: `main_workflow/reporting/core/` - Shared utilities (TaskManager, validators, formatters)
- **Tests**: `main_workflow/reporting/tests/` - Unit and integration tests
- **Dataset identification**: `main_workflow/dataset_identification/` - GEO search and AI ranking

### Naming Conventions
- **Files**: snake_case.py (e.g., `task_manager.py`, `ai_assistant_tab.py`)
- **Functions**: snake_case (e.g., `submit_task`, `get_task_status`)
- **Classes**: PascalCase (e.g., `TaskManager`, `BatchProcessor`)
- **Constants**: UPPER_SNAKE_CASE (e.g., `ENTREZ_EMAIL`, `MAX_WORKERS`)
- **Enums**: PascalCase with UPPER values (e.g., `TaskStatus.PENDING`)

### Import Patterns
```python
# Standard library first
import os
import sys
from pathlib import Path
from typing import Optional, Dict, Any

# Third-party packages
import streamlit as st
import pandas as pd
from dotenv import load_dotenv

# Local imports
from uorca.batch.local import LocalBatchProcessor
from main_workflow.reporting.core import TaskManager, TaskStatus
```

### Error Handling
```python
# Use specific exceptions with clear messages
try:
    result = process_dataset(geo_id)
except FileNotFoundError as e:
    st.error(f"Dataset files not found: {e}")
    return None
except ValueError as e:
    st.error(f"Invalid dataset format: {e}")
    return None

# For background tasks, capture full traceback
except Exception as e:
    error_msg = f"{type(e).__name__}: {str(e)}\n{traceback.format_exc()}"
    task_manager.update_status(task_id, TaskStatus.FAILED, error=error_msg)
```

### Testing Patterns
- **Test file naming**: `test_*.py` (e.g., `test_task_manager.py`)
- **Test organization**: pytest functions with descriptive names
- **Fixtures**: Use pytest fixtures for setup/teardown (e.g., `temp_db`, `mock_api`)
- **Mocking**: Use `pytest-mock` or `unittest.mock` for API calls
- **Markers**: `@pytest.mark.unit`, `@pytest.mark.integration`, `@pytest.mark.slow`
- **Coverage target**: >90% for core modules, >80% overall

## Development Workflow

### Current Focus: No-CLI UORCA Implementation
The project is currently implementing a GUI-first experience to eliminate command-line requirements. See `docs/plans/no_cli_uorca_implementation.md` and `todos/no-cli-uorca-implementation-todos.md`.

**Implementation Phases**:
1. **Phase 1A**: Background Task System (TaskManager with SQLite) - ‚úÖ COMPLETE
2. **Phase 1B**: Dataset Identification Tab - ‚úÖ COMPLETE (with critical fixes)
3. **Phase 1C**: Pipeline Execution Tab - üöß NEXT
4. **Phase 2A**: Batch Launcher Scripts (Windows/macOS/Linux)
5. **Phase 2B**: Native Desktop App (Optional)
6. **Phase 3**: Polish & Testing

**Lessons Learned from Phase 1B**: See `docs/lessons_learned/dataset_identification_gui_implementation.md` for critical patterns that MUST be applied to Phase 1C.

### Recommended Claude Code Commands
1. **Research**: Use `/research_codebase` to understand existing patterns
2. **Planning**: Use `/create_plan` for complex features
3. **Implementation**: Use `/implement_plan` to execute approved plans
4. **Quick checks**: Use `/check` during development
5. **Validation**: Use `/validate_plan` for comprehensive verification

### Git Workflow
- **Branch naming**: `feature/task-description` or `fix/issue-description`
- **Commit messages**: Descriptive, imperative mood (e.g., "Add TaskManager with SQLite persistence")
- **PR requirements**: All tests pass, type checking clean, manual testing completed

### Before Committing
```bash
# Always run before committing:
uv run pytest                    # All tests must pass
uv run pyright                   # No type errors
# Manual testing for GUI changes
```

## Checkpoint Strategy

When implementing complex changes (especially during `/implement_plan`):
1. **Before each phase**: Create checkpoint with descriptive name (e.g., "Completed Phase 1A - TaskManager")
2. **Test incrementally**: Verify each component works before moving to next
3. **Commit working states**: Create atomic commits for each completed task
4. **Use /rewind if needed**: Rollback to last known good state

Example checkpoint workflow for Phase 1A:
```bash
# After completing TaskManager core
git add main_workflow/reporting/core/task_manager.py
git commit -m "feat: implement TaskManager core class with SQLite persistence"

# After tests pass
git add main_workflow/reporting/tests/test_task_manager.py
git commit -m "test: add unit tests for TaskManager"
```

## Common Tasks

### Adding a New Streamlit Tab
1. Create tab file: `main_workflow/reporting/streamlit_tabs/your_tab.py`
2. Implement `show_your_tab()` function
3. Import in `streamlit_tabs/__init__.py`
4. Add to tab list in `main_workflow/reporting/uorca_explorer.py`
5. Test manually: `uv run uorca explore`

### Testing Background Tasks (TaskManager)
```python
# Use temporary database for testing
import tempfile
from main_workflow.reporting.core import TaskManager

with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as f:
    db_path = f.name

manager = TaskManager(db_path=db_path)
task_id = manager.submit_task("test_1", "test", my_function, {"param": "value"})

# Wait and verify
import time
time.sleep(0.5)
status = manager.get_task_status(task_id)
assert status["status"] == TaskStatus.COMPLETED
```

### Running Pipeline Locally
```bash
# Small test run (2-3 datasets)
uv run uorca run local \
  --input sample_inputs/ \
  --output_dir test_results/ \
  --max_workers 2 \
  --max_storage_gb 50

# Monitor progress
uv run uorca explore test_results/
```

### Debugging Streamlit Issues
```bash
# Run with debug logging
uv run streamlit run main_workflow/reporting/uorca_explorer.py --logger.level=debug

# Clear cache
rm -rf ~/.streamlit/cache

# Check port conflicts
lsof -i :8501
```

## Gotchas & Known Issues

### Issue 1: SQLite Thread Safety
**Problem**: SQLite connections created in one thread cannot be used in another
**Solution**: Always use `check_same_thread=False` in sqlite3.connect() calls
```python
conn = sqlite3.connect(self.db_path, check_same_thread=False)
```

### Issue 2: Streamlit Session State Persistence
**Problem**: Session state is lost on page rerun
**Solution**: Use TaskManager SQLite persistence for long-running tasks, not session_state
```python
# DON'T: Store task status only in session_state
st.session_state.task_status = status

# DO: Store in TaskManager, retrieve on rerun
task_manager = TaskManager()
status = task_manager.get_task_status(task_id)
```

### Issue 3: BrokenPipeError from tqdm in Background Threads ‚ö†Ô∏è CRITICAL
**Problem**: tqdm tries to write to stderr in background threads, causing BrokenPipeError
**When**: Any CLI tool called from TaskManager that uses tqdm progress bars
**Solution**: Multi-layer defense:
```python
# 1. Detect thread and disable tqdm
is_main_thread = threading.current_thread() is threading.main_thread()
for item in tqdm(items, desc="...", disable=not is_main_thread):
    ...

# 2. Replace tqdm.write() with logging
logging.info("Progress message")  # Not tqdm.write()

# 3. Redirect stdout/stderr in GUI wrapper
original_stdout, original_stderr = sys.stdout, sys.stderr
sys.stdout, sys.stderr = io.StringIO(), io.StringIO()
try:
    cli_main()
finally:
    sys.stdout, sys.stderr = original_stdout, original_stderr
```
**Applies to**: Dataset identification, pipeline execution, any background CLI calls
**Reference**: `docs/lessons_learned/dataset_identification_gui_implementation.md`

### Issue 4: Streamlit Caching Causes Stale Task Status
**Problem**: TaskManager returned cached in-memory status instead of fresh database data
**Symptoms**: GUI shows "running" even after task completed in database
**Solution**: TaskManager now queries database first, not cache:
```python
# Fixed in uorca/core/task_manager.py:254
def get_task_status(self, task_id: str):
    # Always check database first for freshness
    conn = sqlite3.connect(self.db_path, check_same_thread=False)
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM tasks WHERE task_id = ?", (task_id,))
    row = cursor.fetchone()
    # ... update cache with fresh data ...
```
**When**: Any Streamlit page rerun that checks task status
**Reference**: Commit fcf1540

### Issue 3: Environment Variables in Docker
**Problem**: .env file not accessible inside Docker containers
**Solution**: Pass environment variables explicitly:
```bash
docker run -e ENTREZ_EMAIL=$ENTREZ_EMAIL -e OPENAI_API_KEY=$OPENAI_API_KEY ...
```

### Issue 4: Python Threading Limitations
**Problem**: Cannot force-kill Python threads
**Solution**: Use cooperative cancellation with progress_callback checks
```python
def task_func(progress_callback=None):
    for i in range(100):
        if progress_callback:
            cancelled = progress_callback(i/100, f"Step {i}")
            if cancelled:  # Check cancellation signal
                return None
```

### ~~Issue 5: Docker Desktop Disk Limits~~ ‚úÖ RESOLVED

**Status**: Fixed as of 2025-10-02

**Previous Problem**: Docker Desktop's virtual disk allocation caused "disk-limit exceeded" errors when processing large RNA-seq datasets locally. fasterq-dump wrote temp files to Docker's virtual disk (50GB limit) rather than the host filesystem.

**Solution**: Temporary files now write to host filesystem via bind-mounted directories (`{output_dir}/{accession}/tmp/`), bypassing Docker's virtual disk entirely. No Docker configuration required. Temp directories are automatically created and cleaned up.

**Impact**:
- ‚úÖ Works with default Docker Desktop settings (50GB)
- ‚úÖ Can process datasets of any size (limited by host disk only)
- ‚úÖ Matches SLURM behavior (both use bind-mounted temps)

**Implementation**: `uorca/batch/local.py:68-70, 104, 113-115, 229-237`

### Issue 6: Large File Handling in Streamlit
**Problem**: Uploading large CSV files can timeout
**Solution**: Stream processing or save to temp directory:
```python
temp_dir = Path.home() / ".uorca" / "temp"
temp_dir.mkdir(parents=True, exist_ok=True)
temp_csv = temp_dir / f"upload_{timestamp}.csv"
temp_csv.write_bytes(uploaded_file.read())
```

## Performance Considerations

- **Database queries**: Keep SQLite queries <50ms by using indexes
- **Streamlit reruns**: Minimize with `@st.cache_data` and `@st.cache_resource`
- **Progress updates**: Limit to 1 update per second to avoid UI lag
- **Memory usage**: Monitor with `psutil` during long pipeline runs
- **Docker storage**: Set `container_tmpfs_gb` appropriately for dataset size

## Security Notes

- **API Keys**: Never commit .env file, always use .env.example template
- **Credentials**: Store in environment variables, not hardcoded
- **Docker**: Use read-only mounts where possible
- **Singularity/Apptainer**: Preferred for HPC (no root privileges required)
- **Input validation**: Always validate user-provided paths and parameters

## External Dependencies

### Required Services
- **NCBI Entrez**: Public biological database API (requires email, API key optional)
- **OpenAI API**: AI-powered dataset analysis (requires API key)
- **Docker/Singularity**: Container runtime for bioinformatics tools
- **GEO/SRA**: Source databases for RNA-seq data (public, no auth needed)

### Environment Variables
```bash
# Required
ENTREZ_EMAIL=          # Your email for NCBI API (required by NCBI guidelines)
OPENAI_API_KEY=        # OpenAI API key for AI features (required)

# Optional
ENTREZ_API_KEY=        # NCBI API key for 10x faster queries (free, highly recommended)
```

## Escalation Rules

When to stop and ask for help:
- [ ] Changes to bioinformatics algorithms (R scripts, Kallisto parameters)
- [ ] Database schema changes (TaskManager SQLite schema)
- [ ] Docker container modifications (kevingchen/uorca:0.1.0)
- [ ] SLURM job submission templates
- [ ] Changes affecting CLI backwards compatibility
- [ ] Security-sensitive code (API key handling, container security)

## Project-Specific AI Instructions

### Autonomous Operation
**Safe to run**:
- Unit tests: `uv run pytest -m unit`
- Type checking: `uv run pyright`
- Streamlit dev server: `uv run uorca explore` (in background with Ctrl+b)
- Small test pipelines: `uv run uorca run local` with 1-2 datasets

**Requires approval**:
- Large pipeline runs (>5 datasets)
- Docker container builds
- SLURM job submissions
- Changes to production data directories
- Git force pushes

**Background tasks during development**:
- Streamlit server: `uv run uorca explore` (Ctrl+b)
- Test watcher: `uv run ptw` (if installed)
- Type checker watch: `uv run pyright --watch` (if available)

### Extended Thinking
Use **extended thinking** for:
- Complex bioinformatics algorithm debugging
- Multi-phase implementation planning (e.g., GUI integration)
- Performance optimization decisions
- Architectural design choices
- Integration between CLI, GUI, and batch systems

Use **regular thinking** for:
- Straightforward bug fixes
- Adding new test cases
- Documentation updates
- Simple UI tweaks

### Memory Tool Usage
Cache research findings in:
- `memory/uorca/research/` - Deep dives into modules and patterns
- `memory/uorca/plans/` - Implementation plans and strategies
- `memory/uorca/architecture/` - System design decisions
- `memory/uorca/debugging/` - Common issues and solutions

## Resources

- **Main Repository**: https://github.com/Kevin-G-Chen/UORCA
- **Implementation Plan**: `docs/plans/no_cli_uorca_implementation.md`
- **TODO List**: `todos/no-cli-uorca-implementation-todos.md`
- **Streamlit Docs**: https://docs.streamlit.io
- **OpenAI API**: https://platform.openai.com/docs
- **NCBI Entrez**: https://www.ncbi.nlm.nih.gov/books/NBK25501/

## Testing Strategy

### Unit Tests
- **Location**: `main_workflow/reporting/tests/unit/`
- **Pattern**: Test individual functions and classes in isolation
- **Mocking**: Mock external APIs, file I/O, database operations
- **Speed**: <5 seconds total runtime
- **Coverage**: >90% for core modules

### Integration Tests
- **Location**: `main_workflow/reporting/tests/integration/`
- **Pattern**: Test complete workflows with mocked external services
- **Examples**: Full dataset identification, pipeline execution with test data
- **Speed**: <30 seconds total runtime
- **Coverage**: All critical user paths

### Manual Testing
- **GUI changes**: Always test in browser before committing
- **Background tasks**: Verify progress updates and cancellation
- **Cross-platform**: Test launchers on Windows/macOS/Linux when applicable
- **Long-running**: Validate multi-hour pipeline runs don't crash

## Updates

- 2025-10-01: Initial CLAUDE.md generated by Claude Code
- 2025-10-01: Added no-CLI implementation context and TaskManager patterns

---

*This file helps Claude Code maintain context across sessions. Update it when you discover new patterns, conventions, or project-specific knowledge.*
